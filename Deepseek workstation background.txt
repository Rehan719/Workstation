# From Prompt to Production: An Architectural Blueprint for a Multi-Agent Collaboration Hub with Jules AI

This research report provides a detailed and comprehensive blueprint for a state-of-the-art GitHub repository designed for high-fidelity collaboration with an advanced AI agent, provisionally named "Jules." The objective of this repository, which we will refer to as the "Jules Collaboration Hub," is to serve as a production-grade framework for generating expert-level content across multiple modalities, including scientific publications, professional presentations, narrated videos, and sophisticated data visualization pipelines. The design synthesizes cutting-edge principles from agentic AI architecture, modern software engineering, and systematic prompt engineering, reflecting the technological landscape of early 2026. Each recommendation is grounded in a synthesis of recent academic literature and industry best practices, ensuring the resulting framework is not merely a collection of scripts but a robust, reproducible, and extensible system for creative and scientific work. This document serves as the finalized, polished resource required to instruct an AI agent to implement this environment, thereby fulfilling the user's primary research goal.

## Architecting a Multi-Agent Framework for High-Fidelity Content Generation

The foundation of the Jules Collaboration Hub lies in its architectural shift away from monolithic AI models toward a coordinated, hierarchical, and orchestrated multi-agent system. This approach is essential for managing the complexity inherent in generating diverse, high-quality content and aligns with the most advanced trends observed in contemporary agentic research [[7,48]]. Instead of relying on a single Large Language Model (LLM) to perform all tasks, often leading to suboptimal results due to model limitations, the architecture decomposes complex goals into discrete, manageable steps handled by specialized agents. This modular design enhances reliability, allows for targeted improvements, and mirrors real-world creative workflows where different experts contribute their unique skills to a common project [[47,78]]. The proposed architecture is built upon a layered reference model that separates concerns to support governance-by-construction, ensuring the system is reliable, observable, and safe for production use [[7]].

At the core of this architecture is a two-tier, hierarchical framework, similar to those described in AgentOrchestra and other advanced systems [[7,90]]. This model consists of a top-level **Orchestrator Agent** and a team of specialized **Worker Agents**. The Orchestrator Agent acts as the central intelligence, receiving high-level user goals (e.g., "Draft a manuscript for submission to Nature") and decomposing them into a sequence of atomic tasks. It then delegates these tasks to the appropriate Worker Agents, manages the flow of information between them, and integrates their outputs to form a cohesive final product. The choice of LLM for the orchestrator is critical; it must possess strong reasoning capabilities, contextual understanding, and proficiency in handling complex instructions. Models such as Qwen3-4B, which has been shown to effectively manage multimodal inputs and orchestrate tasks, or Innovator-VL, a scientific MLLM designed for complex reasoning, would be suitable candidates [[1,68]]. The orchestrator's logic is not hardcoded but is instead guided by a combination of pre-defined workflow templates and its own dynamic decision-making abilities.

Surrounding the orchestrator are the Worker Agents, each engineered for a specific domain. This specialization is a key differentiator from general-purpose models. For instance, a `LiteratureReviewAgent` could be fine-tuned on academic abstracts and citation networks to perform deep, context-aware searches [[81]]. A `LaTeXWriterAgent` would focus exclusively on generating syntactically correct and stylistically appropriate LaTeX code, leveraging tools like Beamer for technical documents [[2]]. Similarly, a `FigureGeneratorAgent` would specialize in creating publication-quality figures, potentially using libraries like Plotly for interactive graphics or custom scripts for scientific imagery [[64,71]]. This modular approach, where each agent has a well-defined role and interface, is exemplified by frameworks like Paper2Video, which employs distinct agents for slide generation, layout refinement, speech synthesis, and avatar rendering [[2]]. The Jules Collaboration Hub should feature a dedicated `/agents` directory containing the source code, configuration files, and prompt templates for each of these specialized components.

To ensure the system operates reliably and safely, several cross-cutting architectural primitives must be integrated from the outset. These include persistent state management, explicit control loops, and robust governance mechanisms. Persistent state is managed through a hierarchical memory system that includes working memory (for ephemeral context during a single task), episodic memory (to store summaries of past interactions), and semantic memory (for durable, knowledge-based facts) [[7]]. This allows agents to maintain context over long conversations and complex projects. Explicit control loops are fundamental to agentic behavior, replacing single-shot inferences with iterative planning and self-correction pipelines [[7]]. The `Blueprint2Code` framework demonstrates this principle effectively with its closed-loop system involving Previewing, Blueprint, Coding, and Debugging agents, where failed tests trigger a structured revision process [[5]]. Finally, governance is paramount for production systems. This involves implementing bounded autonomy by setting hard limits on execution time, token usage, tool calls, and monetary cost to prevent runaway processes [[7]]. Circuit breakers should be employed to halt execution when error rates spike, and mandatory human-in-the-loop (HITL) approval gates must be placed at critical junctures, such as before publishing a manuscript or deploying a script [[3,7]]. Access control via Role-Based Access Control (RBAC) and immutable audit logs for every action provide the necessary security and traceability [[7]]. The repository's documentation and configuration files should explicitly detail these guardrails and how users can interact with them.

The following table outlines the proposed agent roles within the Jules Collaboration Hub, their functions, and potential underlying technologies, based on an analysis of current research.

| Agent Role | Primary Function | Key Technologies & Methodologies | Rationale |
| :--- | :--- | :--- | :--- |
| **Orchestrator Agent** | Decomposes high-level goals, plans task sequences, coordinates worker agents, and integrates final outputs. | Qwen3-4B, Innovator-VL, LangChain/LangFlow, Hierarchical Command Structures [[1,68,89,90]] | Acts as the central brain, leveraging a powerful MLLM for complex reasoning and workflow management. |
| **Citation & Literature Agent** | Conducts literature reviews, retrieves relevant papers, extracts metadata (DOIs, authors), and generates preliminary bibliographies. | RAG (Retrieval-Augmented Generation), Crossref API, Semantic Search (Vector Databases), Citation Networks [[10,53,74,81]] | Automates the foundational research step, ensuring factual grounding and proper attribution. |
| **Manuscript Drafting Agent** | Writes content for various sections of a document (abstract, introduction, methods) based on outlines and input data. | Manubot, GPT-4o, Spec-Driven Development (SDD) principles, Template-based Generation [[4,52]] | Focuses on coherent, structured prose, adhering to scientific writing conventions and formal specifications. |
| **LaTeX & Typesetting Agent** | Generates high-quality LaTeX code for manuscripts, slides (Beamer), and tables, ensuring correct syntax and formatting. | Beamer, Pandoc, Custom parsers for Markdown to LaTeX conversion [[2,10]] | Provides precise control over typography and mathematical notation, which is critical for scientific and technical content. |
| **Data Analysis & Visualization Agent** | Analyzes datasets, performs statistical tests, and generates code for creating figures and charts (e.g., Plotly, Matplotlib). | Plotly, Seaborn, Scikit-learn, InstructPipe methodology, LLM-based Data Science Agents [[40,71,86]] | Translates data into visual insights, automating the creation of publication-ready graphics. |
| **Presentation & Video Agent** | Converts drafts into multimedia presentations, generates scripts, creates slides, and orchestrates the production of narrated videos. | Paper2Video framework, Tree Search Visual Choice, Personalized TTS, Avatar Rendering (Hallo2, FantasyTalking) [[2,29]] | Handles the complex pipeline of transforming text into a dynamic, engaging video format with synchronized visuals. |
| **Refiner & Editor Agent** | Reviews and improves the output of other agents, focusing on clarity, style, grammar, and adherence to style guides. | Iterative refinement loops, Rubric-Guided Engines (ARISE), Reflexive Prompt Engineering [[57,74]] | Incorporates quality assurance and stylistic polish, acting as a final check before human review. |
| **Tool & API Management Agent** | Registers, discovers, and securely manages connections to external tools, APIs, and databases used by the system. | Typed Tool Interfaces, MCP (Model Context Protocol) servers, Tool Registry schemas [[3,7]] | Provides a standardized, secure, and version-controlled way to extend the system's capabilities. |

This multi-agent architecture, combined with robust control and governance layers, forms a resilient and scalable foundation. It moves beyond simple automation to create a collaborative ecosystem where specialized AI components work together under the guidance of a central planner. This structure is not only capable of handling the user's specified tasks but is also inherently modular, allowing new agents and capabilities to be added over time without disrupting the existing system. The Jules Collaboration Hub will thus be a living, evolving platform for AI-assisted content creation.

## End-to-End Workflows for Scientific and Multimedia Content Production

The true power of the Jules Collaboration Hub is realized through its defined end-to-end workflows, which choreograph the actions of the specialized agents to transform a high-level user request into a polished, production-ready asset. These workflows are not linear scripts but dynamic, multi-stage processes that incorporate feedback loops, conditional branching, and mandatory human-in-the-loop (HITL) checkpoints. Drawing inspiration from frameworks like Blueprint2Code's closed-loop debugging [[5]] and the 'orchestrated AI' model that mandates human gate points [[3]], the workflows are designed for both efficiency and safety. They provide concrete pathways for generating scientific publications, presentations, and narrated videos, ensuring that each step—from initial idea to final artifact—is executed with precision and quality.

For the production of a **Scientific Publication**, the workflow begins with the user providing a topic, a target journal, and any relevant source materials. The Orchestrator Agent first engages the **Citation & Literature Agent** to conduct a comprehensive literature search, retrieving articles from reputable sources and building a preliminary bibliography [[74]]. This agent uses Retrieval-Augmented Generation (RAG) to synthesize findings and identify key themes and gaps [[53]]. The Orchestrator then uses the gathered information to task the **Manuscript Drafting Agent** with creating a detailed outline, adhering to the structural requirements of the target journal. Once the outline is approved by the user, the drafting agent proceeds section by section, generating text that is factually grounded by the retrieved literature. Simultaneously, the **Data Analysis & Visualization Agent** is processing any provided datasets, performing necessary statistical analyses, and generating code for figures and tables [[86]]. The **LaTeX & Typesetting Agent** takes the completed text and figure code, weaving them together into a coherent `.tex` file using a Beamer-compatible structure for maximum flexibility [[2]]. At this stage, the complete draft is placed in the `/content/drafts/` directory, triggering a mandatory HITL gate where the user can review, edit, and approve the content before it is moved to the `/content/approved/` folder [[3]]. Only upon explicit human confirmation is the final command issued for the system to compile the LaTeX into a PDF and generate a final, polished manuscript ready for submission.

The workflow for generating a **Professional Presentation and Narrated Video** follows a similarly structured yet distinct path, heavily inspired by the multi-agent framework of Paper2Video [[2]]. After receiving the source material (typically a research paper or a detailed outline), the Orchestrator first assigns the **Manuscript Drafting Agent** to extract key points, arguments, and takeaways. This summary is then fed to the **Presentation & Video Agent**. This agent initiates a multi-step production pipeline. First, the **Slide Generation Module** creates individual slides, again using LaTeX Beamer to ensure high-quality rendering of equations and diagrams [[2]]. Next, the innovative **Tree Search Visual Choice** module, a key feature to be incorporated from the Paper2Video research, is invoked to optimize the layout of each slide. This module efficiently evaluates multiple candidate layouts and selects the best one with a single inference, dramatically improving the aesthetic quality of the presentation [[2]]. Following layout optimization, the **Subtitling Agent** generates accurate captions for the slides. The **Speech Synthesis Agent** then takes the slide content and converts it into a spoken narration using a personalized Text-to-Speech (TTS) model [[2]]. Concurrently, the **Avatar Rendering Agent** generates a talking-head presenter, using models like Hallo2 or FantasyTalking, who will deliver the narration while their gaze and gestures are synchronized with the on-screen content [[2]]. A separate model generates cursor movements to highlight key parts of the slides during the narration, enhancing audience engagement [[2]]. The final stages involve combining the rendered video of the avatar with the background slides and audio track. To ensure integrity, the entire video can be processed through a C2PA validation module to embed a cryptographic seal of authenticity [[31]]. As with the publication workflow, all intermediate assets are stored in the `/content/drafts/` directory, awaiting final human approval before being finalized and archived in `/content/published/`.

A third, highly automated workflow focuses on the generation of **Advanced Data Visualization Pipelines**. This workflow is particularly suited for exploratory data analysis and the rapid creation of analytical dashboards. The user provides a dataset (e.g., a CSV or Excel file) and a natural language description of the desired visualizations ("show a scatter plot of sales vs. advertising spend, color-coded by region, and add a trend line"). The Orchestrator routes this request to the **Data Analysis & Visualization Agent**. This agent leverages principles from frameworks like InstructPipe, which can generate visual blocks pipelines from human instructions [[40]]. It analyzes the dataset's schema, interprets the user's request, and generates the corresponding Python code using libraries like Plotly or Matplotlib [[71]]. The generated code is executed, producing the requested visualizations. For more complex scenarios, this agent could invoke the **CellWhisperer** methodology, which uses contrastive learning to create a joint embedding space for data and text, enabling more sophisticated, chat-based exploration of complex datasets [[13]]. The output of this workflow is not just a static image but a complete, runnable Python script that can be saved to the repository. This empowers researchers to build upon and customize the generated analysis, embodying the FAIR (Findable, Accessible, Interoperable, Reusable) data principles [[9]]. All generated scripts and plots are version-controlled within the repository, creating a replicable and extensible analytical lineage.

These three workflows demonstrate the versatility of the Jules Collaboration Hub. They are not rigid templates but configurable processes that can be adapted to specific needs. The Orchestrator Agent, guided by the repository's configuration files, dynamically composes these steps. For example, a user might want to generate a presentation but skip the video avatar rendering step, opting instead for a simpler screen recording. The system's modular design allows for such customizations. Furthermore, all workflows are designed with transparency and control in mind. Every action taken by an agent, every tool called, and every intermediate result produced is logged and made available for inspection. This ensures that the final output is not a black-box artifact but a product of a transparent and auditable process, a critical requirement for any system intended for serious scientific and professional use.

## Ensuring Production Readiness Through Reproducible Engineering Practices

Creating a "production-ready" environment for AI-driven content generation necessitates moving beyond simple scripting and embracing a rigorous discipline of software engineering. The Jules Collaboration Hub must be built on a foundation of reproducibility, consistency, and maintainability, treating AI agents and their associated workflows as first-class engineering artifacts. This requires a deep integration of containerization, comprehensive version control, automated testing via CI/CD, and robust observability. These practices are no longer optional for advanced AI systems; they are fundamental requirements for building reliable, scalable, and trustworthy applications [[7,9]]. By adopting these standards, the repository transforms from a collection of experimental scripts into a robust platform worthy of professional deployment.

The cornerstone of reproducibility is containerization, primarily through Docker. Every computational environment required by the hub—whether it's the orchestrator agent, a specific worker agent, or a downstream tool like a plotting library—must be encapsulated within a container. The repository will include a `docker/` directory containing a `Dockerfile` and a `compose.yaml` file [[13]]. The `Dockerfile` will define the precise operating system base image, specify exact versions of all programming languages (e.g., Python 3.11), and enumerate every single library and dependency required by the agents. This eliminates the "it works on my machine" problem and guarantees that anyone cloning the repository can reproduce the exact same environment. The `docker-compose.yml` file will orchestrate the running of multiple containers if needed, for example, to run the application alongside a database for persistent state or a vector store for RAG. This container-first approach is a best practice demonstrated by numerous complex AI projects, including CellWhisperer and the `rworkflows` package, which relies on containerization to provide consistent build environments across different operating systems [[9,13]].

Comprehensive version control extends far beyond the source code in the `/src` directory. In the context of agentic systems, every component that influences the outcome must be version-controlled [[7]]. This includes:
*   **Prompts and Configurations:** The carefully engineered prompt templates and agent configurations, stored in the `/config` directory, must be tracked. Changes to a prompt can significantly alter an agent's output, so having a version history is crucial for debugging and maintaining consistency [[7]].
*   **Datasets:** Any sample or test datasets located in the `/data` directory should be version-controlled to ensure that experiments and demonstrations are repeatable.
*   **Artifacts and Lineage:** The system should be designed to capture and version the lineage of all generated artifacts. This means recording which version of the code, which specific prompts, and which datasets were used to produce a given output, such as a manuscript draft or a video file. This concept of enhanced provenance is vital for scientific reproducibility [[8]].
*   **Tools and Dependencies:** The registry of tools and their versions, defined in YAML or JSON files, must also be part of the repository's source tree, allowing for deterministic re-runs of workflows [[7]].

Continuous Integration and Continuous Deployment (CI/CD) pipelines are the engine that enforces these standards automatically. The repository will include a `.github/workflows/` directory containing YAML files that define automated jobs triggered by events like pushes to the main branch or pull requests [[9]]. These workflows, modeled after the simplicity and power of the `rworkflows` suite, will execute a series of checks to validate changes before they are merged [[9]]. A typical CI pipeline would include steps for:
1.  **Linting and Formatting:** Automatically checking all code (Python, YAML, Markdown) for style and syntax errors using tools like `black`, `flake8`, and `yamllint`.
2.  **Dependency Scanning:** Using SAST (Static Application Security Testing) tools to scan dependencies for known vulnerabilities [[4]].
3.  **Unit and Integration Tests:** Running a comprehensive test suite against the agent logic and helper functions to ensure correctness and prevent regressions.
4.  **Documentation Build:** Automatically building and previewing the documentation to catch broken links or rendering issues.
The `rworkflows` package is notable for its use of a centralized GitHub Action, which ensures that all users benefit from updates to the CI/CD logic without needing to modify their local copies [[9]]. Adopting a similar strategy would greatly simplify maintenance.

Finally, deep observability is non-negotiable for debugging and monitoring stochastic agentic systems. Due to the probabilistic nature of LLMs, understanding *why* an agent made a particular decision is as important as knowing the result. The architecture must be designed to capture detailed, structured traces of every interaction [[7]]. Each time an agent is invoked, the system should log a comprehensive record that includes the full input prompt, the specific tool(s) called with their arguments, the response received from the tool or LLM, the latency of the call, and any errors encountered. This data should be formatted in a standard way (e.g., JSON) and stored in a dedicated `/logs/` directory or sent to an external observability platform. This practice, pioneered by frameworks like LangSmith, enables developers and users to debug failures, analyze performance bottlenecks, and understand the reasoning chain of the agents at scale [[7]]. By integrating these four pillars—containerization, holistic version control, automated CI/CD, and deep observability—the Jules Collaboration Hub will meet the highest standards of production readiness, providing a stable, reliable, and transparent foundation for all its AI-driven activities.

## Embedding Advanced Prompt Engineering Principles into Repository Configuration

A core requirement for the Jules Collaboration Hub is the systematic integration of prompt engineering principles directly into its structure. Rather than relying on ad-hoc, conversational prompting where a user might simply ask Jules to "write a paper," the repository will enforce a more structured, engineered approach. This philosophy treats prompts not as ephemeral messages but as version-controlled, reusable, and optimized configurations. This methodology, which aligns with concepts like Spec-Driven Development (SDD), elevates the process from creative experimentation to a disciplined engineering practice, yielding more consistent, reliable, and higher-quality outputs [[4,37]]. The repository will achieve this by embedding prompt patterns into configuration files, establishing templated interaction protocols, and designing feedback loops that allow for iterative refinement.

The primary mechanism for this integration is the creation of a dedicated `/config/prompts/` directory. Within this directory, each prompt template will be stored as a separate, version-controlled file, likely in a structured format like YAML or JSON for easy parsing and modification. These templates will be crafted based on established prompt engineering patterns identified in extensive research, such as Chain-of-Thought (CoT), few-shot learning, role-playing, and reflexive prompting [[19,23,57]]. For example, instead of a generic instruction, the prompt for generating a paper outline might look like this in a YAML file:

```yaml
# /config/prompts/scientific_paper_outline.yaml
version: 1.1
model: gpt-4o
author_role: "Senior Academic Researcher"
task_description: >-
  You are an expert academic researcher tasked with creating a detailed,
  logical, and comprehensive outline for a scientific manuscript.
  Your outline must follow the IMRaD structure (Introduction, Methods, Results, and Discussion).
  Use clear, concise, and formal language appropriate for a peer-reviewed journal.
  The outline should suggest specific subsections and indicate the logical flow of arguments.
  Ground your suggestions in the provided key concepts and objectives.
key_concepts:
  - Machine Learning for Climate Modeling
  - Extreme Weather Event Prediction
  - Uncertainty Quantification
objectives:
  - Compare the predictive accuracy of two novel ML models.
  - Evaluate the models' performance under different climate scenarios.
  - Discuss the implications for policy and risk assessment.
format: markdown
```

This structured approach offers several advantages. It makes the prompt's intent explicit and unambiguous. It separates the "what" (the task) from the "how" (the constraints and style), making it easier to reuse and adapt. Critically, because it is a file in the repository, it can be version-controlled, tested, and improved over time. If a subsequent generation yields poor results, a developer can modify the YAML file, commit the change, and rerun the process to see if the new prompt produces a better outcome, creating a verifiable feedback loop.

This engineered approach extends to defining the interaction protocols between agents. When the Orchestrator Agent delegates a task to a Worker Agent, it doesn't just pass a raw string. It passes a structured payload that includes the optimized prompt template, along with the specific parameters for that task. This is analogous to how Claude Skills encode entire workflows as conversational commands, where structured tool access is provided via configuration files rather than hardcoded logic [[3]]. For instance, when the Orchestrator tells the `VisualizationAgent` to create a chart, the message might contain a JSON object specifying the prompt template ID (`scatter_plot_generation.yaml`), the path to the dataset, and the desired chart title. This declarative, configuration-driven method makes the system more maintainable and less brittle than traditional imperative scripting [[3]].

Furthermore, the repository will embed mechanisms for iterative refinement and evaluation, inspired by frameworks like Blueprint2Code and ARISE [[5,74]]. After an agent produces an output, it may not be perfect. The system will have a built-in `RefinerAgent` or an automated evaluation step that assesses the output against a predefined rubric. For example, after the `LaTeXWriterAgent` generates a section of a paper, a script could use LaTeXBench to evaluate the generated LaTeX for structural compliance and minimal edit errors [[82]]. If the output fails to meet the criteria, the `RefinerAgent` is activated. It receives the original prompt, the failed output, and an error analysis, and attempts to revise the content. This closed-loop process continues until the output meets the quality threshold or a maximum number of iterations is reached [[5]]. The results of these evaluations, including metrics and scores, will be logged, providing quantitative feedback on the performance of different agents and prompt variations. This systematic validation is a core pillar of Spec-Driven Development and is essential for ensuring production readiness [[4]]. By embedding these advanced prompt engineering principles directly into its configuration and operational logic, the Jules Collaboration Hub moves beyond simple automation to become a sophisticated, controllable, and continuously improvable system for AI-assisted creation.

## Modality-Specific Implementation Strategies and Toolchains

To fulfill its mission of generating expert-level content across multiple modalities, the Jules Collaboration Hub must provide a curated and integrated set of toolchains tailored to the specific demands of scientific publications, presentations, and data visualization. Simply providing general-purpose LLM access is insufficient; success depends on leveraging specialized tools, libraries, and methodologies that are state-of-the-art as of early 2026. This section details the recommended implementations for each content type, drawing upon successful frameworks and best practices identified in the provided research.

For the creation of **Scientific Publications**, the primary implementation strategy revolves around a hybrid approach combining human oversight with AI-powered automation, all centered on the LaTeX typesetting system. While other formats exist, LaTeX remains the gold standard in many scientific disciplines due to its unparalleled ability to handle complex mathematical notation, cross-referencing, and bibliographic management [[2]]. The hub will utilize a pipeline similar to that of VueGen, which is distributed as a Python package and can be integrated into the repository's toolchain [[54]]. The workflow will begin with the **Manuscript Drafting Agent** generating content in Markdown, a format that is easier for AI to handle. A key component will be a Python script, akin to those described in [[10]], that automates the conversion from Markdown to a structured LaTeX document using Pandoc. This script will ensure consistency and save significant manual labor. A crucial part of this process is automated citation management. The system will integrate with APIs like CrossRef to fetch metadata from Digital Object Identifiers (DOIs), batch-processing them into a correctly formatted `.bib` file, which improves accuracy and efficiency [[10]]. Frameworks like AutoSurvey2 and SurveyGen-I demonstrate the feasibility of fully automated pipelines that generate academic text with inline citations [[72,73]]. The **LaTeX & Typesetting Agent** will be responsible for rendering the final document, using Beamer for any slides or supplementary materials to guarantee the fidelity of formulas and symbols [[2]]. To further enhance the quality of the manuscript, the hub will incorporate an **Equation Extractor script** that can parse LaTeX files to catalog all equations, facilitating verification and organization [[10]].

For generating **Presentations and Narrated Videos**, the hub will adopt the multi-agent framework pioneered by the Paper2Video project as its core blueprint [[2]]. This framework is specifically designed to automate the production of video presentations from research papers, a task that requires a sophisticated pipeline of specialized agents. The first step is slide generation, which, like in the publication workflow, will leverage LaTeX Beamer for its typographical superiority [[2]]. However, a key innovation to be implemented is the "Tree Search Visual Choice" module for layout refinement. This technique, which reduces the computational complexity of optimizing slide layouts, will be a signature feature of the hub's video generation capability, ensuring aesthetically pleasing and professional-looking slides [[2]]. Once the slides are generated, the **Presentation & Video Agent** orchestrates the next stages. This agent will use a Personalized Text-to-Speech (TTS) model to generate a high-quality voice-over narration from the slide content [[2]]. Simultaneously, it will employ an avatar rendering model, such as Hallo2 or FantasyTalking, to create a virtual presenter who speaks the narration [[2]]. A critical component for engagement is the synchronization of a cursor with the narration, highlighting key points on the slides, a feature successfully implemented in MM-StoryAgent [[67]]. The final video will be a composite of the rendered avatar, the static or animated slides, and the audio track. To address challenges faced by general-purpose models in text-heavy content, the hub will use specialized models trained for presentation generation, as Veo3's struggles with reading text from slides underscore the need for domain-specific expertise [[2]]. For content integrity, the system will integrate C2PA (Coalition for Content Provenance and Authenticity) validation to cryptographically sign the final video, providing a verifiable seal of authenticity for the generated media [[31]].

For the development of **Advanced Data Visualization Pipelines**, the hub will move beyond simple image generation to create functional, executable code. This aligns with the trend of treating AI as a co-pilot for coding, where the goal is to produce deployable, maintainable code rather than just static outputs [[14,43]]. The core of this functionality will be the **Data Analysis & Visualization Agent**, which will be built upon the principles of frameworks like InstructPipe, demonstrating the ability of LLMs to generate visual blocks pipelines from natural language instructions [[40]]. When given a user request, this agent will generate Python scripts using powerful visualization libraries such as Plotly, which is noted for its interactive charts, or Matplotlib and Seaborn for more traditional statistical graphics [[71]]. The system will also draw inspiration from the CellWhisperer framework, which uses a joint embedding space for transcriptomes and text to enable chat-based exploration of complex biological data [[13]]. This methodology can be generalized to allow for more interactive and exploratory data analysis, where users can ask questions about their data and receive visual answers. The output of this workflow will be a `.py` script that can be run locally or in a Jupyter notebook environment. This approach ensures that the generated visualizations are not just images but are part of a larger, reproducible analytical process, consistent with FAIR data principles [[9]]. The Morphētype Scientific tool is another point of reference, showing how to integrate LaTeX math, Jupyter notebooks, and BibTeX citations into a single, seamless workflow, a model the hub will emulate for its data science outputs [[10]].

By integrating these modality-specific toolchains, the Jules Collaboration Hub becomes a versatile platform capable of meeting the high standards of scientific and professional communication. Each toolchain is selected for its proven effectiveness, reproducibility, and alignment with the latest technological advancements, ensuring that the final products are not only generated quickly but are also of the highest possible quality.

## Finalized Repository Blueprint and Operational Guidelines

This section consolidates the preceding analysis into a definitive blueprint for the "Jules Collaboration Hub" GitHub repository. This blueprint includes a finalized, logical directory structure, detailed descriptions of each component, and a set of operational guidelines for setup and use. The structure is designed to be intuitive, scalable, and aligned with the core principles of modularity, reproducibility, and production readiness. This complete resource is intended to be the final artifact, ready to be used as a prompt to instruct an AI agent to create and configure the repository.

The recommended directory structure for the Jules Collaboration Hub is as follows:

```
/jules-collab-hub/
├── .github/workflows/
│   └── ci.yml
├── .claude/config/
│   └── workspace.json
├── docs/
│   ├── index.md
│   └── agent_guide.md
├── src/
│   ├── orchestrator.py
│   └── agents/
│       ├── __init__.py
│       ├── drafting_agent.py
│       ├── visualization_agent.py
│       └── ...
├── config/
│   ├── prompts/
│   │   ├── scientific_paper_outline.yaml
│   │   └── video_slide_layout.json
│   ├── tools/
│   │   └── tldrs.yaml
│   └── agents/
│       └── paper_writer_config.json
├── data/
│   └── sample_data.csv
├── content/
│   ├── drafts/
│   ├── approved/
│   └── published/
├── docker/
│   ├── Dockerfile
│   └── compose.yaml
├── benchmarks/
│   └── paper2video_bench.py
└── README.md
```

Each component of this structure serves a specific purpose:

*   **`.github/workflows/`:** This directory contains the CI/CD pipeline definitions for the repository. The `ci.yml` file will automate linting, testing, and dependency scanning for all code and configuration files, ensuring that contributions adhere to quality standards before being merged [[9]].
*   **`.claude/config/`:** Inspired by Anthropic's Claude Skills framework, this directory stores persistent configuration for the agent's workspace, such as API keys, default models, and project-specific settings [[3]]. This keeps sensitive information out of the main codebase while allowing for customizable operation.
*   **`docs/`:** This directory holds comprehensive documentation. `index.md` provides a high-level overview and getting-started guide, while `agent_guide.md` offers detailed instructions on the purpose and configuration of each agent, serving as a user manual.
*   **`src/`:** This is the core of the application's logic. `orchestrator.py` contains the main control loop that manages agent interactions. The `/agents` subdirectory houses the Python source code for each specialized worker agent, promoting modularity and separation of concerns.
*   **`config/`:** This is the nerve center for the system's "personality" and behavior. The `/prompts` subdirectory contains all engineered prompt templates in YAML or JSON format. The `/tools` directory will house a registry of available tools (e.g., APIs, scripts) with their schemas and connection details. The `/agents` subdirectory holds the high-level configuration for each agent's behavior, such as which models to use and what tools they are permitted to access [[7]].
*   **`data/`:** This directory is for sample datasets intended for training, testing, or demonstration purposes. Keeping sample data separate from the source code is a best practice for clarity and reproducibility.
*   **`content/`:** This is the designated area for all AI-generated assets. The `/drafts` subdirectory is the mandatory human review and approval gate. Assets moved here are considered in progress. Once reviewed and approved by a human, they are moved to `/approved/` and are considered final. Completed, published work should be archived in `/published/` to maintain a clean slate for new projects [[3]].
*   **`docker/`:** Contains the scripts for containerization. The `Dockerfile` defines the application's runtime environment, while `compose.yaml` orchestrates the services. This ensures that the entire system can be run consistently anywhere Docker is installed [[13]].
*   **`benchmarks/`:** This directory will hold scripts for evaluating the performance of the agents and the overall system. For example, it could contain an adapted version of the evaluation suite from the Paper2Video paper to measure the quality of generated videos [[2]].
*   **`README.md`:** The entry point for any user. It must provide a clear explanation of the project, its purpose, and a step-by-step guide on how to clone the repository, set up the environment using Docker, and run the first workflow.

**Operational Guidelines:**

1.  **Setup:** Users should begin by cloning the repository. The primary method for running the hub will be via Docker Compose. The command `docker compose up --build` will build the necessary images and start the application environment. This ensures that all dependencies are correctly installed and isolated.

2.  **Configuration:** Before running any workflows, users must configure their environment. This involves editing the `./.claude/config/workspace.json` file to include any required API keys (e.g., for OpenAI, Anthropic, or other services the agents will use). They should also review the configurations in the `/config` directory to select preferred models or adjust agent parameters.

3.  **Initiating a Workflow:** Workflows are initiated through a primary interface, which could be a Python script in the `src/` directory or a CLI tool. The user will provide a high-level goal, such as `"Generate a presentation from the paper located at ./data/sample.pdf"`. This triggers the Orchestrator Agent.

4.  **Execution and Monitoring:** The Orchestrator will proceed through the predefined workflow steps, invoking the necessary Worker Agents. During execution, logs will be generated in a structured format (e.g., JSON lines) and written to a `/logs/` directory. These logs should be monitored to track progress and diagnose any issues. The system is designed to be transparent, with every agent action recorded.

5.  **Human-in-the-Loop (HITL):** The most critical step in the process is the HITL gate. As the Orchestrator completes major stages (e.g., drafting the manuscript, generating the video), the resulting assets will be deposited in the `/content/drafts/` directory. The user is expected to review these drafts meticulously. Only after reviewing and approving the content should the user issue a command (e.g., via a CLI flag or a GUI button) to move the assets to `/content/approved/` and proceed to the next stage or finalize the product.

6.  **Finalization and Archiving:** Once a project is complete and all assets are approved, the user should manually archive the contents of the `/content/approved/` directory to the `/content/published/` folder. This preserves the final deliverables while keeping the active project workspace clean for future tasks.

By following this blueprint and these operational guidelines, a user can establish a powerful, reliable, and professionally oriented environment for collaborating with an AI agent. The Jules Collaboration Hub is designed not just to automate tasks, but to augment human creativity and productivity, providing a robust scaffold for achieving expert-level results across a wide range of content creation domains.




# Designing the Jules Framework: An Operational Blueprint for a State-of-the-Art AI-Powered Scientific and Multimedia Production Engine

## Architectural Blueprint: A Modular and Environment-Aware Repository Structure

The foundation of any sophisticated software or AI-driven production system is its architectural blueprint, which dictates modularity, maintainability, and scalability. For the AI agent Jules, designed to produce expert-level scientific and multimedia content, the repository structure must transcend traditional coding projects. It must function as a comprehensive digital laboratory, encapsulating not only the final artifacts but also the environment, dependencies, specifications, and provenance trails required for their creation and verification. The proposed architecture is built upon four primary pillars: the Core Project Directory for content assets, the Orchestration and Configuration Layer for logic and control, the Infrastructure-as-Code (IaC) directory for resource management, and the CI/CD Workflows directory for automation. This design integrates best practices from modern DevOps, MLOps, and open science movements, prioritizing reproducibility, transparency, and version-controlled environments [[1,6]]. Containerization via Docker and Conda is not merely an option but a mandatory requirement to ensure that every component of the production pipeline operates within a consistent and auditable sandbox, thereby eliminating "works on my machine" scenarios and enabling perfect replication of experiments and outputs [[6]].

The Core Project Directory, conventionally housed at the root of the repository, serves as the central hub for all creative and research assets. Its structure is meticulously organized to segregate different modalities of content, ensuring clarity and ease of navigation. A dedicated `/project/manuscript` directory would host a complete LaTeX-based project, following the precedent set by advanced systems like PaperTalker which favor declarative typesetting over proprietary formats [[4]]. This choice provides superior automatic layout, compactness, and formal academic styling capabilities, which are critical for generating publication-ready documents [[4]]. Within this directory, `.tex` source files, BibTeX bibliography databases (`.bib`), and asset folders for figures and tables would be stored. Similarly, a `/project/presentations` directory would contain sources for slide decks, also likely utilizing the robust LaTeX Beamer class for its typographical precision and reproducibility [[4]]. The `/project/multimedia` directory is a specialized compartment for all audiovisual materials. It would house raw video clips, processed audio tracks, animation scripts (such as those for VYOND™ software used in studies on the 'easiness effect' [[3]]), and the final rendered outputs. To manage repository size and history integrity, it is recommended that final, large media files like videos and compiled PDFs are not committed directly to the main branch but are instead distributed as artifacts of a GitHub Release upon successful completion of a CI/CD pipeline [[18,19]]. The `/project/graphics` directory would be dedicated to analytical graphics, containing the scripts (e.g., Python with Matplotlib/Seaborn) and prompts used for generation, alongside the version-controlled output image files. Finally, the `/project/data` directory would manage datasets. Adhering to scientific best practices, only small sample datasets should be stored directly in Git; larger datasets should be referenced via links to external, persistent repositories like Zenodo, which was used for archiving the USNCO-V benchmark metadata, to maintain repository performance and comply with data sharing policies [[5,6]].

Directly adjacent to the project content lies the Orchestration and Configuration Layer, a hidden but vital part of the repository typically located at `/.jules`. This layer acts as the brain of the operation, housing all the specifications, prompts, and workflow definitions that drive Jules's agentic behavior. The `/specs` subdirectory is the cornerstone of a Spec-Driven Development (SDD) approach, where formal, detailed specifications serve as executable blueprints for all tasks [[16]]. Each task, from "generate a literature review" to "create figure for experimental results," would have a corresponding specification file. These files define requirements, inputs, constraints, and clear validation criteria, shifting the development paradigm from manual coding to a more systematic, specification-first workflow [[16]]. This ensures that every action taken by Jules is grounded in a verifiable plan. The `/prompts` directory contains a curated library of engineered prompts, drawing inspiration from extensive resources like the dair-ai/Prompt-Engineering-Guide [[2]]. This library would include prompt templates, complex prompt chains, and role-specific instructions for the various AI agents. Furthermore, it would incorporate concepts from schema-engineering approaches like BAML, which replace unstructured prompts with functions that accept arguments and return structured objects, leading to more reliable and predictable AI interactions [[2]]. The `/workflows` directory would contain YAML definitions for the CI/CD pipelines, primarily orchestrated using GitHub Actions. These workflows are designed to be event-driven, triggering on the addition or modification of spec files in `/specs`, a push to a designated branch, or even a manual command from a user. They execute a sequence of jobs that set up the environment, invoke the appropriate agents, and manage the overall production process. Lastly, the `/environments` directory defines the containerized environments required for execution. It would contain multiple Dockerfiles and Conda environment files, each tailored to a specific toolchain or dependency stack needed by different parts of the system, such as a LaTeX environment, a Python-based data analysis environment, or a multimedia processing environment. This meticulous definition of environments is crucial for ensuring that every component runs in a consistent, isolated, and reproducible sandbox, a principle strongly advocated for in open science initiatives [[6]].

The third pillar of the architectural blueprint is the Infrastructure-as-Code (IaC) and Tooling directory, located at `/infra`. This section manages the underlying compute resources, APIs, and third-party services that power Jules's operations. If cloud infrastructure is provisioned, the `/infra/terraform` directory would hold Terraform configurations to declaratively manage servers, storage buckets for media assets, and other cloud resources, ensuring that the environment itself is version-controlled and reproducible. Security and access management are handled centrally. API keys for LLM providers (e.g., OpenAI, Anthropic Claude, Google Gemini), vector database credentials, and other secrets would be securely managed through GitHub's native secret vault, accessible to workflows during execution without being hardcoded in the repository [[18]]. The `/infra/secrets` directory might contain template files or documentation for setting these secrets, but the actual values would remain in the secure vault. The `/tools` directory serves as a collection of reusable scripts, custom CLI tools, and wrappers for interacting with external services. For instance, it could house a tool similar to Aider, which facilitates DevOps integration by allowing AI to assist with code editing directly within a terminal-based workflow [[16]]. It could also contain custom wrappers for APIs used by the various agents, such as those for accessing Retrieval-Augmented Generation (RAG) engines like RAGflow [[2]] or for orchestrating multi-agent conversations [[10]]. This centralized location for tools and IaC configurations promotes reusability, simplifies maintenance, and ensures that the entire operational stack—from the physical cloud resources down to the local scripts—is documented and version-controlled within the same Git repository. This holistic approach, where every aspect of the system is treated as code, is fundamental to achieving the high degree of reproducibility and auditability required for expert-level scientific and creative production. By integrating dependency management, security scanning, and clear documentation standards directly into the repository's structure, the framework sets a new standard for what a collaborative AI workspace can achieve [[6,11]].

| Directory | Purpose | Key Components & Best Practices |
| :--- | :--- | :--- |
| `/project/manuscript` | Stores source files for scientific publications. | `.tex` files, `.bib` (BibTeX), `/figures`, `/tables`. Uses LaTeX for reproducibility and formal styling [[4]]. |
| `/project/presentations` | Contains sources for slide decks. | Beamer `.tex` files, speaker notes, media assets. LaTeXPrefers Beamer for academic rigor [[4]]. |
| `/project/multimedia` | Manages all audiovisual assets. | Raw video/audio clips, animation scripts (e.g., VYOND™ [[3]]), processed outputs. Avoids bloating Git history by treating final renders as release artifacts [[18,19]]. |
| `/project/graphics` | Houses analytical graphics generation. | Source scripts (Python/R), prompts, and output files (PNG/SVG). Enables reproducible visualization pipelines. |
| `/project/data` | Manages datasets. | Small sample datasets in Git; larger datasets are referenced via persistent links (e.g., Zenodo) to maintain repository performance [[5,6]]. |
| `/.jules/specs` | Executable blueprints for all tasks. | Formal specifications driving a Spec-Driven Development (SDD) workflow. Defines requirements, constraints, and validation criteria [[16]]. |
| `/.jules/prompts` | Curated library of engineered prompts. | Prompt templates, chains, and schema-engineered functions for agent communication. Inspired by guides like dair-ai/Prompt-Engineering-Guide [[2]]. |
| `/.jules/workflows` | CI/CD pipeline definitions. | GitHub Actions YAML files triggered by events (e.g., spec changes) to automate the entire production process [[18,19]]. |
| `/.jules/environments` | Containerized runtime environments. | Multiple Dockerfiles and Conda environment files ensure consistent, reproducible execution for all tools and dependencies [[6]]. |
| `/infra/terraform` | Cloud infrastructure as code. | Terraform configurations for provisioning and managing cloud resources like servers and storage, ensuring environmental reproducibility [[12]]. |
| `/infra/secrets` | Secure management of API keys and credentials. | Managed via GitHub's secret vault, preventing exposure in the repository. Critical for securing LLM and other service integrations [[18]]. |
| `/infra/tools` | Reusable scripts and CLI utilities. | Custom tools for interacting with external services (e.g., RAG engines [[2]]) and facilitating AI-assisted workflows (e.g., Aider [[16]]). |

This comprehensive and deliberate architectural blueprint establishes a robust, extensible, and highly reproducible foundation for the Jules AI agent. It moves beyond a simple collection of files to become a living system of records, where every piece of content is born from a clear specification, executed within a controlled environment, and subject to rigorous, auditable validation. This structure is not merely a technical choice; it is a philosophical commitment to the principles of open science and responsible AI, ensuring that the outputs of Jules are not just innovative but also trustworthy and verifiable.

## Agentic Orchestration Engine: A Collaborative Multi-Agent Workflow

At the heart of the Jules framework lies its agentic orchestration engine, a dynamic and intelligent system that transforms the static repository into a proactive production studio. This engine is not a single monolithic script but a synergistic ecosystem of specialized AI agents working in concert, guided by a standardized collaboration protocol. The design is heavily inspired by advanced frameworks like OmniScientist, which envisions a co-evolving ecosystem of human and AI scientists, and practical implementations like the PaperTalker builders, which demonstrate the power of modular, task-specific components [[4,12]]. The core innovation is the shift from linear, sequential processing to a non-linear, iterative cycle of planning, execution, evaluation, and refinement. This model, often referred to as an "evaluator-refine loop," enables Jules to self-correct and improve its output until it meets predefined quality criteria, a concept that has been shown to be effective for enhancing the accuracy and grounding of multimodal LLMs [[5,15]]. The entire system is governed by a central protocol, analogous to the Omni Scientific Protocol (OSP), which enforces "forced auditability" by logging every action and decision, thereby establishing an immutable chain of provenance and elevating Jules from a mere tool to a true intellectual collaborator [[12]]. This orchestration engine is the mechanism through which high-level user prompts are decomposed into a series of concrete, executable tasks, managed by a team of agents, and iteratively perfected.

The collaborative multi-agent workflow is initiated when a user issues a high-level prompt, such as "Draft a research paper on the application of quantum computing in drug discovery." The first agent to engage is the **Planner Agent**. Its primary responsibility is to deconstruct this ambiguous request into a structured, actionable plan. It analyzes the goal, identifies key sub-tasks (e.g., "conduct a literature review," "generate a paper outline," "write the introduction," "create a methodology diagram"), and populates the `/specs` directory with formal specification files for each sub-task [[16]]. This initial step is critical, as it translates human intent into a machine-executable blueprint, forming the basis for the subsequent Spec-Driven Development (SDD) workflow [[16]]. Once the plan is established, the **Research Agent**, modeled after the DeepResearch Agent in the OmniScientist framework, takes over [[12]]. This agent is tasked with surveying the state-of-the-art. It performs targeted literature searches, downloads relevant papers (e.g., from arXiv or OpenAlex), and feeds them into a Retrieval-Augmented Generation (RAG) pipeline [[2,12]]. The RAG engine, potentially using a powerful open-source solution like RAGflow, grounds the information retrieval process in the full-text context of the downloaded documents, ensuring that subsequent generation tasks are informed by accurate, evidence-based knowledge rather than generic pre-trained data [[2]]. The findings are summarized and fed back to the Planner or other content agents as needed.

With research completed, a suite of **Content Generation Agents** executes the plan laid out in the specifications. The **Outline Agent** analyzes the collected literature and generates a logical, data-driven structure for the manuscript, which is then saved as a specification or a draft document [[12]]. Following this, the **Writing Agent** is invoked to generate the textual content. To ensure the highest quality and compatibility with academic publishing standards, this agent is specifically trained or prompted to generate content in LaTeX format, avoiding the formatting inconsistencies often associated with WYSIWYG editors [[4,12]]. Simultaneously, the **Figure Agent** is tasked with creating visual elements. This agent combines the power of Python script executors (for precise, reproducible charts using libraries like Matplotlib) with generative image models (for conceptual diagrams), embodying the hybrid approach advocated by OmniScientist [[12]]. This dual capability allows for both quantitative data visualizations and qualitative illustrative graphics. For multimedia outputs, the **Video Agent**, inspired by the PresentAgent framework, orchestrates the creation of presentation videos [[13]]. This involves managing a complex pipeline of slide generation, voice-over synthesis, and animation [[4]]. Each of these agents operates semi-autonomously, executing its assigned task based on the specific parameters and context provided in its respective specification file.

The most critical phase of the workflow is the continuous **Evaluation and Refinement Loop**, which ensures that quality is baked into the process from the start. As soon as a Content Agent produces an output—a draft paragraph, a generated figure, a slide deck—the **Evaluator Agents** are triggered. This is a parallelized process involving several specialized evaluators. A **Vision-Language Model (VLM) Evaluator**, powered by a state-of-the-art model like GPT-4.1 or Gemini-2.5-Pro, assesses the visual fidelity of slides and figures, checking for layout coherence, readability, and factual alignment between visuals and captions [[4,13]]. The **Semantic Cite Validator**, a component inspired by the SemanticCite framework, performs a deep audit of all citations, verifying their accuracy against the retrieved literature to prevent factual errors and hallucinations [[8]]. A separate **Style and Clarity Evaluator** ensures the generated text adheres to stylistic conventions and maintains logical coherence. The output of these evaluators is not just a pass/fail verdict but a detailed critique, often structured as JSON, that includes confidence scores, reasoning, and ranked evidence snippets, ensuring full traceability [[8]]. If the evaluation score falls below a predefined threshold, the **Refiner Agent** is activated. This agent interprets the critique and modifies the original output. In a basic implementation, it might simply edit the LaTeX source file or adjust a Python script. However, the framework can be extended to implement more advanced techniques like ProRefine, where the very prompt given to the Task Model is dynamically optimized based on the feedback received, allowing smaller models to approach the performance of much larger ones [[17]]. This evaluator-refine loop continues iteratively until the output meets the specified quality criteria or a maximum number of refinement steps is reached, transforming static validation into a dynamic process of semantic reflection and self-improvement [[15]].

To manage this complex interplay of agents, a central coordination mechanism is essential. The **Omni Scientific Protocol (OSP)** provides the ideal conceptual model for this. Implemented via a centralized Hub within the repository, the OSP would define a standardized message format for all agent communications, manage identity for both human and AI participants, and distribute messages across the network [[12]]. Crucially, it would enforce "forced auditability" by recording every action—every specification written, every code block executed, every evaluation performed—into an immutable log. This leads to the concept of a **ScholarlyObject**, representing the smallest unit of intellectual value (e.g., a hypothesis, a code block, a generated paragraph) [[12]]. Each ScholarlyObject would be imbued with an immutable **ContributionLedger**, a chronological record of every participant (human or AI) who acted upon it, along with a timestamp. This ledger creates an unbroken, verifiable chain of provenance, allowing a user to trace the lineage of any piece of content back to its origin. This level of transparency is paramount for scientific credibility and aligns with recommendations for improving study validity and fostering wider adoption of methodologies [[6]]. The entire orchestration is powered and automated by **GitHub Actions**, which serves as the backbone for the CI/CD pipeline. A workflow is triggered when a new specification is committed or a user initiates a build. The workflow checks out the code, sets up the necessary containerized environment, and launches the Planner Agent. As the Planner generates new specs, they are picked up by the relevant Content Agents. When their work is done, the Evaluator Agents are queued, and if a Refiner is needed, it is invoked. This entire lifecycle, from prompt to polished artifact, is managed, logged, and automated, providing a seamless and powerful collaborative experience.

| Agent Role | Primary Function | Key Technologies / Concepts | Output / Interaction |
| :--- | :--- | :--- | :--- |
| **Planner Agent** | Decomposes high-level user requests into a sequence of structured, executable tasks. | Spec-Driven Development (SDD) [[16]], Task decomposition. | Creates and populates specification files in the `/specs` directory. |
| **Research Agent** | Conducts literature reviews and gathers evidence from academic sources. | Retrieval-Augmented Generation (RAG) [[2]], Information retrieval, Data scraping (e.g., from arXiv/OpenAlex [[12]]). | Summarized findings, bibliographic data, and source documents for other agents. |
| **Outline Agent** | Generates a logical, data-driven structure for manuscripts or reports. | Text summarization, Structural analysis of related literature. | A structured outline, often in Markdown or LaTeX sections. |
| **Writing Agent** | Generates the main textual content of a document. | Large Language Models (LLMs) with strong writing capabilities. | Drafts of manuscript sections in LaTeX format. |
| **Figure Agent** | Creates analytical and illustrative graphics. | Hybrid approach: Python script executors (Matplotlib) + generative image models [[12]]. | Generated image files (PNG, SVG) and the source scripts used to create them. |
| **Video Agent** | Orchestrates the end-to-end creation of narrated presentation videos. | Multi-agent pipeline inspired by PresentAgent [[13]] and PaperTalker [[4]]. | A finished video file (MP4) and associated assets (slides, subtitles). |
| **VLM Evaluator** | Assesses the quality, layout, and visual accuracy of generated slides and figures. | Vision-Language Models (VLMs) like GPT-4.1, Gemini-2.5-Pro [[4]]. | A structured JSON report with scores, critiques, and evidence. |
| **Semantic Cite Validator** | Verifies the factual accuracy and support for all citations in a document. | NLP models, Citation analysis frameworks like SemanticCite [[8]]. | A classification of citations (SUPPORTED, UNSUPPORTED, etc.) and flagged errors. |
| **Style & Clarity Evaluator** | Checks generated text for adherence to style guides and logical coherence. | NLP models fine-tuned for style and grammar. | A report on stylistic and grammatical issues. |
| **Refiner Agent** | Modifies outputs based on critiques from the Evaluator Agents. | Prompt engineering, Code editing, Specification refinement. | Corrected output (e.g., revised LaTeX, updated Python script). |
| **Central Hub / OSP** | Governs all agent interactions, manages identities, and enforces auditability. | Message queuing, Immutable ledgers, Standardized protocols. | An auditable log of all actions and decisions within the system [[12]]. |

This sophisticated orchestration engine, powered by a well-defined multi-agent system and a transparent collaboration protocol, forms the operational core of the Jules framework. It enables a level of complexity and autonomy that is essential for tackling ambitious, multi-modal creative and scientific projects, while simultaneously embedding the principles of reproducibility and accountability that are foundational to credible research.

## Scientific Publication Pipeline: Automated LaTeX-Based Manuscript Production

The production of expert-level scientific publications represents a primary function of the Jules framework, and its implementation hinges on a robust, fully automated pipeline centered around the LaTeX typesetting system. The choice of LaTeX is not arbitrary; it is a deliberate strategic decision rooted in the demands of academic publishing. As demonstrated by the PaperTalker framework, LaTeX offers significant advantages over alternative formats like XML-based PPTX, including declarative typesetting for automatic and consistent layout, extreme compactness of source files, and suitability for formal academic slides with pre-configured styles [[4]]. This makes it the industry standard for producing high-fidelity, peer-reviewed manuscripts. The Jules pipeline automates the entire lifecycle of manuscript creation, from drafting to compilation and validation, leveraging modern CI/CD practices to ensure efficiency and reliability. The process begins with the Writing Agent generating `.tex` source files based on the specifications and outlines provided by the Planner and Outline Agents. These files are then pushed to the repository, triggering a sophisticated GitHub Actions workflow that compiles the LaTeX source into a final PDF document, performs a series of quality checks, and, upon success, packages the output for distribution. This end-to-end automation significantly reduces the manual effort traditionally required for document preparation and helps maintain a high standard of technical and typographical quality.

The core of the pipeline is the CI/CD workflow orchestrated with GitHub Actions. A typical workflow is configured to trigger automatically upon a `push` event to a specific branch or when a new tagged commit follows a semantic versioning pattern (e.g., `v1.0.0`) [[18,19]]. The workflow begins with the `actions/checkout@v2` step, which sets up the Git repository in the CI environment's workspace [[19]]. The next critical step is setting up the build environment. This is achieved by invoking a Docker container or Conda environment defined in the `/infra` directory, ensuring that the exact versions of TeX distributions, packages, and other dependencies are replicated precisely as intended [[6]]. This containerization is a cornerstone of the framework's commitment to reproducibility [[6]]. With the environment ready, the workflow invokes a specialized LaTeX compilation action, such as the highly-regarded `xu-cheng/latex-action@v2`, specifying the root `.tex` file of the manuscript project (e.g., `main.tex`) [[18,19]]. This action handles the complex sequence of `pdflatex`, `bibtex`, and subsequent `pdflatex` runs required to correctly resolve cross-references, citations, and table of contents entries. The result of this compilation is a final, polished PDF file generated within the ephemeral CI environment's workspace [[18]].

A crucial and novel aspect of the Jules pipeline is its ability to move beyond simple compilation and integrate a powerful post-compilation validation and refinement loop. The generated PDF itself is not the final product in this stage; the focus shifts to the intermediate logs and error outputs produced during the compilation process. Modern LaTeX compilers generate detailed logs that can pinpoint issues such as undefined references, overfull hbox warnings (indicating text overflow in a column), misplaced floats, and package loading errors. The CI workflow can be programmed to parse these logs. If any critical errors are detected, or if the number of warnings exceeds a configurable threshold, the workflow does not proceed to deployment. Instead, it triggers the **Refiner Agent**. This agent receives the compilation log as input and attempts to correct the issues. For example, if it detects an "undefined reference" warning, it might analyze the surrounding context in the `.tex` file and use its LLM capabilities to suggest or insert the missing label. This implements a form of the "specification-feedback-refinement" loop, where the compiler's output serves as a form of negative feedback, driving the iterative improvement of the manuscript's source code [[16]]. This dynamic feedback mechanism elevates the process from a one-shot compilation to a continuous quality assurance loop, ensuring that the final document is technically sound before it is finalized.

Upon successful compilation and validation, the final distribution phase of the pipeline is activated. Committing the large generated PDF file directly to the Git repository is discouraged as it leads to bloated history and slow clone times. Instead, the Jules framework employs a more elegant strategy. The CI workflow can be configured to create a formal release on GitHub using the `actions/create-release@v1` action [[18,19]]. This action creates a new release tag (e.g., `v1.2.1`) and provides a description. Subsequently, the `actions/upload-release-asset@v1` action is used to attach the newly compiled PDF as a downloadable asset to that specific release [[19]]. This approach cleanly separates the source code (versioned in Git) from the binary artifacts (managed as releases), providing a clear and organized way to distribute the final manuscript. This method is widely recommended for software and document projects alike [[19]]. The workflow can be further enhanced to perform additional checks before creating a release. For instance, it could run the Semantic Cite Validator on the manuscript's `.tex` source to check the integrity of all citations, ensuring that every claim is properly supported by its reference list [[8]]. Only if all checks—compilation, citation integrity, and style guidelines—pass would the release be created and published. This multi-stage validation gate ensures that the final artifact distributed to the user or the public is of the highest possible quality, meeting the rigorous standards expected of scientific publications. The entire process, from a user's high-level prompt to a versioned, citable release of a PDF manuscript, is automated, transparent, and reproducible, showcasing the power of combining modern CI/CD practices with advanced AI-driven content generation.

## Multimedia Content Synthesis: High-Fidelity Video and Presentation Generation

Beyond static scientific texts, the Jules framework is engineered to produce dynamic, multi-modal content, most notably high-fidelity narrated videos and visually compelling presentation decks. This capability moves the system into the domain of advanced multimedia synthesis, a field where recent research has yielded powerful new paradigms. The design of this pipeline draws heavily from the state-of-the-art **PaperTalker** framework, which details a sophisticated multi-agent system for automatically generating academic presentation videos from research papers [[4]]. The Jules framework extends this model by integrating it into its broader agentic orchestration engine, incorporating principles of parallelized generation, intelligent layout optimization, and precise temporal alignment. The ultimate goal is to automate the labor-intensive process of creating engaging video abstracts, lecture recordings, or conference presentations, transforming a static manuscript into a rich, auditory-visual narrative. This is achieved through a complex, multi-stage pipeline that involves specialized agents for slide generation, voice synthesis, subtitle creation, and virtual presenter animation, all coordinated by the central Planner Agent and validated by the Evaluator Agents.

The video generation pipeline begins with the creation of the visual backbone: the slides. Drawing a direct line from the PaperTalker architecture, the **Slide Builder**, a component of the Figure Agent, generates LaTeX Beamer code [[4]]. This choice of Beamer is strategically sound, as it provides the typographical precision and structural integrity required for academic content, far surpassing the capabilities of simpler presentation tools [[4]]. However, a significant challenge in automated slide generation is layout. Simply stuffing content into a Beamer frame often leads to cluttered, unreadable slides. To solve this, the Jules framework incorporates the innovative **"Tree Search Visual Choice"** module from PaperTalker [[4]]. When the Slide Builder detects that a frame is overflowing its allotted space, instead of failing, it generates multiple layout variants. These variants might differ in parameters like font size, figure scaling, or the arrangement of elements. Each variant is then rendered into a visual snapshot. A Vision-Language Model (VLM), acting as the **VLM Evaluator**, is then presented with these snapshots and asked to select the best one based on criteria like balance, readability, and aesthetic appeal. This automated, VLM-guided optimization process ensures that even complex content is presented in a clean, professional layout, a key factor in audience comprehension and engagement [[4]].

Once a high-quality slide deck is generated, the pipeline proceeds to add the auditory dimension. This involves two parallel streams: narration and subtitles. The **Subtitle Builder**, another specialized component, works in tandem with the Writing Agent's output. It processes the generated text for a given slide and breaks it down into coherent sentences. A crucial and advanced step in this process is the precise alignment of speech with visual focus. The **Cursor Builder** achieves this through a technique called spatial-temporal grounding [[4]]. First, a UI-TARS model is used to estimate, based on a visual-focus prompt derived from the slide's content, where the presenter's cursor should appear for each sentence. Second, to ensure perfect timing, the WhisperX model is employed to perform word-level speech recognition on the synthesized narration, extracting precise timestamps for each word. These timestamps are then aligned with the corresponding subtitle sentences, allowing the cursor to move in perfect synchronicity with the spoken word. This level of detail is critical for guiding the viewer's attention and enhancing the learning experience. The final audio track is synthesized using a state-of-the-art Text-to-Speech (TTS) model, such as F5-TTS, which can be personalized to produce a natural-sounding, human-like voice [[4]].

The final stage of the pipeline is the assembly of the virtual presenter. Here, the Jules framework leverages a combination of cutting-edge animation models. The **Talker Builder** synthesizes the final video on a per-slide basis, which allows for efficient parallel processing since slide transitions are treated as hard scene changes, making temporal continuity between slides unnecessary [[4]]. For the presenter's head and facial movements, a model like Hallo2 can be used to generate realistic head-only animations [[4]]. For upper-body articulation and more expressive gestures, a model like FantasyTalking is employed [[4]]. The outputs from these models—the animated head, the articulated body, and the background slide—are composited together. The resulting individual slide videos are then stitched into a single, cohesive presentation video. The quality of this final video is assessed by the **VLM Evaluator**, which uses a rubric inspired by frameworks like PresentEval, scoring the output along dimensions such as content alignment, visual quality, and perceived professionalism [[13]]. If the evaluation reveals issues, such as poor visual choices or unnatural animation, the Refiner Agent is triggered to make adjustments, iterating on the process until the desired quality is achieved. This entire pipeline, from manuscript to polished video, demonstrates the framework's capacity to handle complex, multi-step, multi-modal production tasks, effectively automating the creation of expert-level multimedia content.

| Stage | Component / Agent | Key Technology / Method | Input | Output |
| :--- | :--- | :--- | :--- | :--- |
| **1. Layout Generation** | Slide Builder (part of Figure Agent) | LaTeX Beamer code generation [[4]] | Document outline and text content. | Initial Beamer `.tex` source file. |
| **2. Layout Optimization** | Tree Search Visual Choice Module | Parallel generation of layout variants, judged by a VLM [[4]]. | Overflown Beamer frames. | Optimized Beamer `.tex` source with improved layout. |
| **3. Audio Synthesis** | Subtitle & Cursor Builders | Word-level timestamping with WhisperX [[4]], spatial grounding with UI-TARS [[4]]. | Optimized Beamer frames and text. | Synchronized subtitles and cursor movement prompts. |
| **4. Animation & Rendering** | Talker Builder | Compositing of slide backgrounds, animated heads (Hallo2), and upper-body motion (FantasyTalking) [[4]]. | Optimized slides, TTS audio, and cursor prompts. | Per-slide video files (e.g., MP4). |
| **5. Final Assembly & Review** | Video Stitching & VLM Evaluator | Temporal continuity is not required between slides [[4]]. Scoring via a VLM-based rubric like PresentEval [[13]]. | Sequence of per-slide video files. | Final presentation video (MP4) and a quality assessment report. |
| **6. Iteration** | Refiner Agent | Edits to Beamer code, TTS voice selection, or animation parameters based on VLM feedback [[15]]. | VLM evaluation report. | Refined inputs for the next iteration of the pipeline. |

This detailed, multi-agent pipeline for multimedia synthesis showcases the depth and sophistication of the Jules framework. By breaking down the complex task of video creation into manageable, specialized stages and employing a rigorous evaluation-and-refinement loop, the system can reliably produce high-quality, engaging, and professionally presented video content, fulfilling a key objective of the research goal.

## Quality Assurance and Reproducibility: Ensuring Fidelity and Auditability

In the pursuit of generating expert-level scientific and multimedia content, the principles of quality assurance and reproducibility are not secondary considerations; they are the bedrock upon which the entire Jules framework is constructed. The credibility of any scientific output, and indeed any expert-level creative work, rests on its verifiability, transparency, and resistance to error. The Jules framework embeds these principles into its DNA through a multi-layered strategy encompassing rigorous validation, systematic auditing, and unwavering commitment to reproducible environments. This approach directly addresses the societal responsibility of researchers, especially those engaged in publicly funded work, to ensure their methods and results can be scrutinized and built upon by others [[6]]. The framework draws upon a confluence of best practices from open science, DevSecOps, and formal software engineering to create a system where every artifact produced is accompanied by a complete, auditable trail of its creation. This ensures that the outputs of Jules are not only innovative but also trustworthy and scientifically valid.

A cornerstone of the framework's quality assurance strategy is the implementation of a robust validation and verification pipeline, which acts as a series of quality gates throughout the production process. For scientific publications, this begins with the automated verification of citations. The integration of a system akin to **SemanticCite** is a non-negotiable feature [[8]]. After the Writing Agent generates a manuscript draft, the Semantic Cite module is triggered. It systematically extracts every citation, retrieves the corresponding source material (either from a local corpus or via online search), and analyzes the claims made in the text against the evidence in the source. Using a nuanced 4-class classification taxonomy—SUPPORTED, PARTIALLY SUPPORTED, UNSUPPORTED, and UNCERTAIN—it flags any discrepancies [[8]]. This process is crucial for combating the rampant issue of citation errors and AI-generated "hallucinations" in academic writing. The results are logged with detailed reasoning and evidence snippets, providing a transparent audit trail for any questionable claims and enabling the Refiner Agent to make targeted corrections [[8]]. This automated fact-checking step transforms the manuscript generation process from a purely generative task into a rigorous, evidence-based one.

Beyond textual accuracy, the framework employs a hierarchy of evaluators to ensure the overall quality and appropriateness of the content. At the highest level, a **VLM Evaluator** plays a pivotal role, particularly for visual and multimodal outputs. This agent, powered by a state-of-the-art Vision-Language Model, performs a holistic assessment of generated slides, figures, and videos [[4,13]]. It evaluates these artifacts against a rubric that measures dimensions such as content accuracy, visual clarity, layout aesthetics, and alignment between speech and visuals in a video [[4,13]]. The use of VLMs for this purpose is a significant advancement, as they can understand and judge complex, multi-modal information in a way that traditional keyword-based checks cannot. The feedback from the VLM Evaluator is not just qualitative; it is quantitative, providing scores that can be used to drive the iterative refinement loop. The framework must also be aware of potential biases in these evaluators. For instance, research on the 'easiness effect' has shown that comprehensible-but-poorly-substantiated content can be perceived as more credible, a bias that a naive VLM might amplify [[3]]. Therefore, the framework should ideally combine VLM scores with other metrics and, ultimately, rely on human oversight to provide the final judgment on quality, a principle underscored by benchmarks like ScienceArena, which rely on blind pairwise voting by human experts [[12]].

Systematic quality control extends to the very code and logic that Jules generates. The framework champions the adoption of **Spec-Driven Development (SDD)**, a methodology where formal, detailed specifications serve as executable blueprints for AI code generation [[16]]. Every script generated by the Figure Agent or any other code-producing component must be accompanied by a specification that clearly defines its purpose, inputs, outputs, and validation criteria. The CI/CD pipeline incorporates a validation framework with five key pillars: **Security Validation** (using Static Application Security Testing (SAST) and dependency scanners to find vulnerabilities like SQL injection or hardcoded secrets), **Testing Requirements** (ensuring a minimum threshold of unit and integration test coverage), **Code Quality Standards** (enforcing linting and formatting rules), **Performance Validation** (checking for anti-patterns like N+1 queries), and **Deployment Readiness** (verifying proper configuration and logging) [[16]]. This systematic approach to validating AI-generated code brings the discipline of software engineering to the world of AI, helping to mitigate common AI error patterns such as hallucinated dependencies or blindness to edge cases [[16]]. Any failure in these validation gates will halt the pipeline and trigger the Refiner Agent to debug and correct the faulty code, based on the explicit error messages from the tests and linters [[16]].

Finally, the absolute guarantee of reproducibility and auditability is achieved through a combination of containerization and an immutable provenance ledger. As previously mentioned, **containerization** using Docker and Conda is mandatory. Every tool, from the LaTeX distribution to the Python libraries and the LLM inference server, runs inside a container with its exact dependency stack frozen in a configuration file (a `Dockerfile` or `environment.yml`) [[6]]. This ensures that the entire computational environment is version-controlled and can be perfectly recreated at any point in the future, fulfilling a key recommendation for open science [[6]]. This is complemented by the **OmniScientist-inspired Contribution Ledger**, a system implemented through the **ScholarlyObject** concept [[12]]. Every atomic piece of intellectual value created by Jules—an outline, a paragraph, a line of code, a figure—is wrapped in a ScholarlyObject that carries a chronological, immutable record of all actions performed on it. This ledger logs the identity of the agent (or human) who created or modified it, the timestamp of the change, and a reference to the previous version. This creates an unbroken chain of provenance, making the entire creative process transparent and auditable. A user can inspect any artifact and trace its entire history, understanding exactly how it evolved from the initial prompt. This level of "forced auditability" is a powerful tool for debugging, for understanding the agent's decision-making process, and for building trust in the system's outputs [[12]]. Together, these strategies for validation, bias mitigation, code quality, and provenance form a comprehensive quality assurance framework that elevates the Jules system from a simple content generator to a trusted partner in expert-level production.

## Finalized Blueprint: An Operational Guide for Implementing the Jules Framework

This research report has culminated in the design of a comprehensive, state-of-the-art GitHub repository framework for the AI agent Jules. The blueprint detailed herein is not merely a theoretical construct but an operational guide for building a powerful, collaborative platform capable of producing expert-level scientific and multimedia content. It integrates the latest standards in AI, software engineering, and open science to create a holistic ecosystem where a human user can collaborate with an autonomous agent to generate high-fidelity publications, presentations, videos, and data visualizations. The framework's strength lies in its modular architecture, its sophisticated agentic orchestration engine, its specialized content generation pipelines, and its unwavering commitment to quality assurance and reproducibility. To translate this blueprint into reality, the following is a synthesized, step-by-step guide outlining the precise actions required to instruct Jules to construct this repository from the ground up.

The first and most fundamental step is to **establish the core repository structure and initialize the environment**. This involves creating the primary directories that form the architectural blueprint: `/project` for all content assets, `/.jules` for the orchestration logic and configurations, and `/infra` for infrastructure and tooling. Within `/.jules`, the subdirectories `/specs`, `/prompts`, `/workflows`, and `/environments` must be created. This initial setup defines the skeleton of the entire system, organizing the project according to the principles of modularity and separation of concerns. Concurrently, the foundational configuration files for the CI/CD system must be initialized. This includes creating the main GitHub Actions workflow file (`.github/workflows/main.yml`) and populating it with the basic structure for triggering builds on spec changes or pushes. The `.gitignore` file must also be configured to exclude large files, cache directories, and sensitive information, ensuring a clean and efficient repository.

Next, the **implementation of containerized environments must be prioritized to guarantee reproducibility**. This requires writing the necessary Dockerfiles and Conda environment files, which are stored in the `/infra` directory. One environment should be tailored for LaTeX and document processing, including all necessary packages for compiling Beamer presentations and handling bibliographies. Another distinct environment should be created for Python-based tasks, containing libraries for data analysis (Pandas, NumPy), visualization (Matplotlib, Seaborn), and AI-related tasks. Additional, more specialized containers may be needed for multimedia processing tasks, such as video compositing or audio manipulation. These environment definitions are critical artifacts that will be used by the CI/CD pipeline to create isolated, consistent, and reproducible execution sandboxes for every task, thus fulfilling a core tenet of open science [[6]].

With the environment stable, the **core logic of the agentic orchestration engine must be developed**. This begins with the Planner Agent, a script or series of prompts that can take a high-level user goal and decompose it into a sequence of sub-tasks, saving them as specification files in the `/specs` directory. This agent embodies the principles of Spec-Driven Development (SDD) [[16]]. Following the Planner, the foundational agents for research and basic content generation should be built. The Research Agent would be implemented as a script that uses APIs or web-scraping techniques to gather literature, feeding the results into a RAG pipeline [[2]]. The initial version of the Writing Agent would be an LLM-powered script designed to generate LaTeX-formatted text based on a given specification. The Evaluator Agents are equally important; the VLM Evaluator would be implemented by creating a script that takes an image or text and sends it to a vision-language model API for scoring against a predefined rubric. Similarly, the Semantic Cite Validator would be a script that implements the logic of the SemanticCite framework, taking a manuscript and a bibliography and returning a report on citation integrity [[8]].

Once the core agents are functional, the **specialized content generation pipelines must be integrated and connected**. The LaTeX publication pipeline would involve enhancing the CI/CD workflow to not only compile the document using an action like `xu-cheng/latex-action` but also to parse the compilation logs for errors and warnings, passing this feedback to the Refiner Agent for correction [[16,18]]. The multimedia pipeline, modeled closely on the PaperTalker framework, requires the most intricate integration [[4]]. This involves connecting the Slide Builder to the VLM-driven "Tree Search Visual Choice" module for layout optimization [[4]]. It necessitates linking the Subtitle and Cursor Builders to models like WhisperX for timing alignment and UI-TARS for spatial grounding [[4]]. Finally, it requires orchestrating the compositing of the virtual presenter animation (from models like Hallo2 and FantasyTalking) onto the slides [[4]]. Each of these connections represents a complex integration point that must be carefully coded and tested within the GitHub Actions workflow.

A critical phase of implementation is the **embedding of the quality assurance and auditability systems**. This involves integrating the citation verifier (SemanticCite) as a mandatory pre-release check in the CI/CD pipeline [[8]]. More profoundly, it requires implementing the `ScholarlyObject` and `ContributionLedger` system. This means modifying all content-generating agents to wrap their outputs in a structured object that includes a timestamp and agent identifier. This ledger data should be stored in a structured format (e.g., JSON-LD) within the repository, creating an immutable, version-controlled history for every piece of content. This establishes the "forced auditability" that is central to the framework's credibility [[12]]. Security is also paramount; all API keys and secrets must be moved to GitHub's secret vault and referenced securely within the workflows, never hardcoded in scripts or specifications [[18]].

The final step is to **finalize the user interface and interaction protocols**, making the powerful backend accessible and intuitive for a human user. While the core functionality is driven by the repository and CI/CD, a user-friendly interface is essential for collaboration. This could be a simple web application built with a framework like Streamlit, similar to the one used in the SemanticCite demo, which would allow a user to upload a topic, view the status of ongoing tasks, inspect generated content, and provide high-level feedback [[8]]. Alternatively, a sophisticated Command Line Interface (CLI) could be developed, allowing users to issue commands to Jules directly from their terminal, leveraging tools like Aider for interactive code generation and editing [[16]]. This interface must be designed around the Human-in-the-Loop (HITL) paradigm, as exemplified by the TIMAR review system, providing clear opportunities for the user to intervene, approve, reject, or refine the output at key decision points in the workflow [[12]]. The ultimate goal is to create a seamless dialogue between the user and the agentic system, where a user's refined prompts can initiate complex production runs and their feedback can guide the iterative refinement process.

By following this comprehensive blueprint, the AI agent Jules can be instructed to construct a truly state-of-the-art collaborative framework. This system transcends the role of a simple code or content generator; it becomes a sophisticated digital research assistant, capable of managing the entire lifecycle of high-stakes creative and scientific production. It embodies the future of human-AI collaboration, where the unique strengths of human creativity and critical thinking are augmented by the scalable, tireless, and precise execution capabilities of a well-designed AI agent, all underpinned by the principles of transparency, reproducibility, and scientific integrity.




# JULES AI v4.0: The Definitive Blueprint for a Self-Evolving, Polymathic Collaborative Intelligence Ecosystem

## Executive Summary

Jules AI v4.0 represents the culmination of recursive design synthesis, integrating the most advanced principles in agentic AI, prompt engineering, software architecture, and open science as of early 2026. This document provides a complete, executable specification for constructing a self-improving, multi-agent collaborative intelligence framework capable of producing expert-level scientific publications, multimedia presentations, data visualizations, and more. Unlike any previous system, Jules v4.0 is designed as a **living organism**—it continuously learns from its own outputs, evolves its prompts and workflows, and adapts to new domains through a built-in meta-cognitive engine. Every artifact is wrapped in an immutable provenance ledger, ensuring full auditability and compliance with FAIR principles. This blueprint is the definitive resource: use it to instruct an advanced AI agent to instantiate the entire ecosystem from scratch.

---

## 1. Core Philosophy and Design Principles

Jules v4.0 is built upon five foundational pillars:

| Pillar | Description |
|--------|-------------|
| **Recursive Self-Improvement** | The system actively monitors its performance, formulates hypotheses for improvement, runs controlled experiments, and merges successful changes via pull requests. |
| **Universal Provenance** | Every generated artifact is wrapped in a `ScholarlyObject` with an immutable `ContributionLedger`, providing a cryptographically signed chain of custody from conception to publication. |
| **Multi-Agent Collaboration** | A dynamic swarm of specialized agents, coordinated by a meta-orchestrator, decomposes complex goals into parallel, interdependent tasks. |
| **Human-in-the-Loop** | Strategic gates insert human judgment at critical junctines, ensuring creative direction, ethical oversight, and final quality control. |
| **Ethical AI by Design** | Hard-coded boundaries, bias detectors, and value alignment models prevent harmful outputs and ensure responsible operation. |

These principles are woven into every layer of the architecture, ensuring that Jules v4.0 is not only powerful but also trustworthy, transparent, and aligned with human values.

---

## 2. System Architecture: The Six-Layer Cognitive Model

Jules v4.0 extends the earlier five-layer model to a **six-layer cognitive hierarchy**, adding a dedicated **Ethical Reflection Layer** to ensure value alignment and safety. Each layer has distinct responsibilities, technology stacks, and communication protocols.

| Layer | Name | Function | Implementation |
|-------|------|----------|----------------|
| L1 | **Reactive Layer** | Instant responses to simple queries, deterministic tool execution | Lightweight models (Qwen3-0.5B), cached responses, rule-based systems |
| L2 | **Procedural Layer** | Multi-step task execution by specialized worker agents | Domain-specific agents (research, writing, visualization), RAG pipelines, tool-use frameworks |
| L3 | **Orchestrator Layer** | Goal decomposition, resource allocation, agent coordination, dynamic workflow compilation | Hierarchical MoE (Qwen3-72B), constraint solver, DAG executor |
| L4 | **Meta-Cognitive Layer** | Self-reflection, strategy optimization, system evolution via experimentation | Recursive self-prompting, genetic prompt algorithms, performance analytics, A/B testing framework |
| L5 | **Ethical Reflection Layer** | Real-time bias detection, value alignment, ethical boundary enforcement | Specialized ethical agents, constitutional AI principles, red-teaming simulations |
| L6 | **Transcendent Layer** | Long-term memory consolidation, cross-project learning, federated insight sharing | Persistent vector store (Weaviate), federated learning across instances, global knowledge graph |

**Communication Protocol**: All agents communicate via the **Structured Agent Messaging Protocol (SAMP) v2.0**, which now includes:

- Mandatory fields: `agent_id`, `timestamp`, `correlation_id`, `provenance` (list of previous agent IDs), `ethical_score` (preliminary risk assessment)
- Payloads are signed with agent-specific keys to prevent spoofing.
- Messages are routed via a **decentralized message broker** (RabbitMQ) with topic-based subscriptions, enabling dynamic agent swarming.

---

## 3. Agent Ecosystem: Specialized Roles and Capabilities

The `/agents/` directory houses a comprehensive set of agent classes, each defined by a configuration file in `/config/agents/` and a Python class. Key agents are listed below; new agents can be added by following the same patterns.

| Agent ID | Class Path | Model | Primary Capabilities |
|---|---|---|---|
| `orchestrator.v2` | `agentic-core/orchestrator.py` | Qwen3-72B | Decomposes high-level goals, allocates tasks, monitors progress, composes final outputs, optimizes resource usage. |
| `meta.cognitive.v2` | `agentic-core/meta-cognitive.py` | Qwen3-32B (fine-tuned on self-improvement) | Analyzes logs and metrics, generates hypotheses, designs A/B tests, creates pull requests for winning changes. |
| `ethical.guardian.v1` | `agents/ethics/guardian.py` | Constitutional AI + Claude 4 | Scans all inputs and outputs for ethical violations, bias, or harmful content; can halt workflows if risk exceeds threshold. |
| `research.literature.v3` | `agents/research/literature_reviewer.py` | Claude 4 Sonnet | Queries academic databases (arXiv, PubMed, Semantic Scholar, OpenAlex), ranks papers, extracts key findings, synthesizes reviews with citations. |
| `research.citation.v2` | `agents/research/citation_validator.py` | Fine-tuned BERT (SemanticCite+) | Extracts citations, retrieves source papers, classifies each as SUPPORTED/PARTIALLY SUPPORTED/UNSUPPORTED/UNCERTAIN, provides evidence snippets. |
| `writing.manuscript.v4` | `agents/writing/manuscript_drafter.py` | GPT-5 (fine-tuned) | Writes IMRaD-structured sections, integrates citations, generates LaTeX code, adapts to journal styles, incorporates reviewer feedback. |
| `writing.outline.v2` | `agents/writing/outline_generator.py` | GPT-5 | Creates detailed outlines from literature summaries, ensures logical flow, suggests subsections. |
| `visualization.figure.v3` | `agents/visualization/figure_generator.py` | Innovator-VL 2.0 | Generates Python code (Matplotlib, Plotly, Seaborn) for publication-quality figures, creates conceptual diagrams via DALL-E 4, optimizes for accessibility. |
| `presentation.slide.v2` | `agents/presentation/slide_builder.py` | GPT-5 + VLM evaluator | Generates LaTeX Beamer code, applies "Tree Search Visual Choice" layout optimization, inserts figures/tables, adds speaker notes. |
| `presentation.video.v2` | `agents/presentation/video_compositor.py` | Not Applicable | Stitches slides, avatar renderings, and audio; adds transitions, captions, cursor movements; encodes in multiple formats. |
| `audio.tts.v2` | `agents/audio/tts_synthesizer.py` | F5-TTS 2.0 | Synthesizes natural speech with emotional intonation; supports voice cloning from user samples. |
| `video.avatar.v2` | `agents/video/avatar_renderer.py` | Hallo3 + FantasyTalking 2.0 | Renders photorealistic avatars with synchronized lip, head, and upper-body movements. |
| `quality.vlm.v2` | `agents/quality/vlm_evaluator.py` | Gemini 3.0 Pro | Scores slides, figures, videos against detailed rubrics; provides structured feedback with explanations. |
| `quality.grammar.v2` | `agents/quality/grammar_checker.py` | LanguageTool + GPT-5 | Checks grammar, style, and readability; suggests improvements. |
| `quality.plagiarism.v2` | `agents/quality/plagiarism_detector.py` | Custom model trained on 100M papers | Detects textual similarity and potential plagiarism; generates reports. |
| `data.analyzer.v2` | `agents/data_science/statistical_analyzer.py` | AutoML toolkit + GPT-5 | Performs exploratory data analysis, statistical tests, model selection; generates reports with interpretations. |
| `data.ml_trainer.v2` | `agents/data_science/ml_model_trainer.py` | AutoML + custom code generation | Trains machine learning models from datasets; outputs code, metrics, and visualizations. |
| `tools.api_wrapper` | `agents/tools/` | Various | Reusable wrappers for external APIs (Crossref, arXiv, Semantic Scholar, OpenAI, etc.) |

All agents inherit from a base class `BaseAgent` that provides logging, SAMP messaging, and integration with the memory systems.

---

## 4. Repository Blueprint

The repository is structured for maximum modularity, reproducibility, and ease of use. Below is the complete directory tree with annotations.

```
jules-ai-v4/
├── .github/
│   └── workflows/
│       ├── ci.yml                     # Lint, test, build on every push
│       ├── deploy-staging.yml           # Deploy to staging environment
│       ├── deploy-prod.yml               # Deploy to production (requires approval)
│       ├── self-improve.yml               # Weekly trigger for meta-cognitive experiments
│       └── release.yml                     # Create GitHub releases with artifacts
├── .vscode/
│   └── settings.json                       # Recommended workspace settings
├── agentic-core/
│   ├── __init__.py
│   ├── orchestrator.py                      # L3 orchestrator logic
│   ├── meta_cognitive.py                      # L4 self-improvement daemon
│   ├── ethical_guardian.py                      # L5 ethical reflection
│   ├── transcendent.py                             # L6 long-term learning
│   ├── protocols/
│   │   ├── samp.py                                  # Structured Agent Messaging Protocol v2
│   │   └── scholarly_object.py                        # ScholarlyObject and ContributionLedger
│   └── memory/
│       ├── working.py                                 # Redis client
│       ├── episodic.py                                 # Time-series logger
│       ├── semantic.py                                  # Weaviate client
│       └── procedural.py                                 # Neo4j client
├── agents/
│   ├── __init__.py
│   ├── base.py                                          # BaseAgent class
│   ├── registry.json                                     # Master agent list with metadata
│   ├── research/
│   │   ├── literature_reviewer.py
│   │   ├── citation_validator.py
│   │   └── rag_pipeline.py
│   ├── writing/
│   │   ├── manuscript_drafter.py
│   │   ├── outline_generator.py
│   │   └── latex_exporter.py
│   ├── visualization/
│   │   ├── figure_generator.py
│   │   ├── diagram_creator.py
│   │   └── color_optimizer.py
│   ├── presentation/
│   │   ├── slide_builder.py
│   │   ├── tree_search_layout.py
│   │   ├── video_compositor.py
│   │   └── subtitle_builder.py
│   ├── audio/
│   │   ├── tts_synthesizer.py
│   │   └── music_generator.py
│   ├── video/
│   │   ├── avatar_renderer.py
│   │   └── scene_assembler.py
│   ├── data_science/
│   │   ├── statistical_analyzer.py
│   │   └── ml_trainer.py
│   ├── quality/
│   │   ├── vlm_evaluator.py
│   │   ├── grammar_checker.py
│   │   └── plagiarism_detector.py
│   ├── ethics/
│   │   └── guardian.py
│   └── tools/
│       ├── arxiv_api.py
│       ├── crossref_api.py
│       ├── openai_client.py
│       ├── anthropic_client.py
│       ├── weaviate_client.py
│       └── redis_client.py
├── config/
│   ├── agents/                                          # Per-agent YAML configs
│   │   ├── literature_reviewer.yaml
│   │   ├── manuscript_drafter.yaml
│   │   └── ...
│   ├── prompts/                                          # Versioned prompt templates (YAML)
│   │   ├── research/
│   │   │   ├── literature_review_v3.yaml
│   │   │   └── ...
│   │   ├── writing/
│   │   │   ├── introduction_v4.yaml
│   │   │   └── ...
│   │   ├── meta/
│   │   │   ├── hypothesis_generation.yaml
│   │   │   └── experiment_design.yaml
│   │   └── ethics/
│   │       └── bias_detection.yaml
│   ├── workflows/                                         # YAML pipeline definitions
│   │   ├── scientific_publication.yaml
│   │   ├── video_presentation.yaml
│   │   ├── data_dashboard.yaml
│   │   ├── meta_learning.yaml
│   │   └── ethical_review.yaml
│   ├── models.yaml                                         # Model routing and fallbacks
│   ├── thresholds.yaml                                      # Quality gate thresholds
│   ├── security/
│   │   ├── roles.yaml                                       # RBAC definitions
│   │   └── secrets.yaml.template                             # Template for secrets
│   └── rubrics/                                             # Evaluation rubrics for VLM
│       ├── slide_quality.yaml
│       ├── video_quality.yaml
│       └── figure_quality.yaml
├── content/
│   ├── projects/                                            # Active projects
│   │   └── {project_id}/
│   │       ├── brief.md                                      # User request
│   │       ├── specs/                                        # Task specifications (generated)
│   │       ├── drafts/                                       # Intermediate outputs
│   │       ├── approved/                                     # Human-approved final outputs
│   │       ├── published/                                    # Final published artifacts
│   │       └── provenance/                                   # ScholarlyObject ledgers
│   ├── assets/                                               # Shared resources
│   │   ├── images/
│   │   ├── datasets/
│   │   ├── citations/
│   │   └── templates/
│   └── archive/                                              # Completed projects
├── infra/
│   ├── docker/
│   │   ├── Dockerfile.orchestrator
│   │   ├── Dockerfile.worker-base
│   │   ├── docker-compose.yml
│   │   └── .dockerignore
│   ├── kubernetes/
│   │   ├── helm/
│   │   └── manifests/
│   ├── terraform/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   └── monitoring/
│       ├── prometheus/
│       ├── grafana/
│       └── alertmanager/
├── tests/
│   ├── unit/
│   ├── integration/
│   ├── benchmarks/
│   └── regression/
├── docs/
│   ├── api/                                                   # Auto-generated API docs
│   ├── user-guide/
│   ├── developer-guide/
│   └── evolution/                                             # Record of self-improvements
├── meta/
│   ├── experiments/                                            # Results of A/B tests
│   ├── hypotheses/                                             # Improvement proposals
│   ├── lineage/                                                # Prompt ancestry graphs
│   └── evolution.log                                           # Chronological changes
├── scripts/
│   ├── setup.sh
│   ├── backup.sh
│   ├── restore.sh
│   └── audit.sh
├── .env.template
├── .gitignore
├── pyproject.toml
├── poetry.lock
├── Makefile
└── README.md
```

---

## 5. Workflow Specifications

Workflows are defined as YAML DAGs in `/config/workflows/`. Each step references an agent and specifies inputs, outputs, and optional gates. Below are condensed versions of the three core workflows.

### 5.1 Scientific Publication Workflow (`scientific_publication.yaml`)

```yaml
name: Scientific Publication
version: 4.0
trigger:
  type: file_change
  path: content/projects/*/brief.md
steps:
  - id: plan
    agent: orchestrator.v2
    input: ${brief}
    output: ${specs}
  - id: literature_review
    agent: research.literature.v3
    input: ${specs.literature_query}
    output: ${lit_review}
  - id: outline
    agent: writing.outline.v2
    input: ${lit_review}
    output: ${outline}
  - id: ethical_review_1
    agent: ethical.guardian.v1
    input: ${outline}
    output: ${outline_ethics}
    gates:
      - condition: ${outline_ethics.risk_score} < 0.3
        fail_step: halt_for_human
  - id: human_gate_1
    type: human_gate
    description: "Review outline"
    input: ${outline}
    output: ${approved_outline}
  - id: draft_sections
    agent: writing.manuscript.v4
    input: ${approved_outline} ${lit_review}
    output: ${draft}
  - id: generate_figures
    agent: visualization.figure.v3
    input: ${draft.data_specs}
    output: ${figures}
  - id: validate_citations
    agent: research.citation.v2
    input: ${draft} ${lit_review.sources}
    output: ${citation_report}
    gates:
      - condition: ${citation_report.error_rate} < 0.03
        fail_step: refine_citations
  - id: refine_citations
    agent: writing.manuscript.v4
    input: ${draft} ${citation_report}
    output: ${refined_draft}
    loop: until ${citation_report.error_rate} < 0.03
  - id: compile_latex
    agent: writing.latex_exporter.v1
    input: ${refined_draft} ${figures}
    output: ${latex_source}
  - id: quality_scan
    agent: quality.plagiarism.v2
    input: ${latex_source}
    output: ${plagiarism_report}
    gates:
      - condition: ${plagiarism_report.max_similarity} < 0.10
        fail_step: halt
  - id: human_gate_2
    type: human_gate
    description: "Review full manuscript"
    input: ${latex_source} ${figures}
    output: ${approved_manuscript}
  - id: final_compile
    agent: writing.latex_exporter.v1
    input: ${approved_manuscript}
    output: ${pdf}
  - id: create_release
    type: github_release
    asset: ${pdf}
    tag: v${project_version}
  - id: archive
    type: move
    source: content/projects/${project_id}/approved
    destination: content/archive/${project_id}
```

### 5.2 Video Presentation Workflow (`video_presentation.yaml`)

```yaml
name: Video Presentation
version: 4.0
trigger:
  type: api
  endpoint: /generate/video
steps:
  - id: extract_content
    agent: presentation.slide.v2
    input: ${source_paper}
    output: ${slide_text} ${figures}
  - id: generate_slides
    agent: presentation.slide.v2
    input: ${slide_text} ${figures}
    output: ${beamer_source}
  - id: optimize_layout
    agent: presentation.tree_search_layout.v1
    input: ${beamer_source}
    output: ${optimized_slides}
  - id: render_slides_pdf
    agent: presentation.beamer_compiler.v1
    input: ${optimized_slides}
    output: ${slide_pdf}
  - id: generate_script
    agent: writing.script_writer.v1
    input: ${slide_text}
    output: ${script}
  - id: ethical_review
    agent: ethical.guardian.v1
    input: ${script}
    output: ${script_ethics}
    gates:
      - condition: ${script_ethics.risk_score} < 0.2
        fail_step: halt
  - id: synthesize_audio
    agent: audio.tts.v2
    input: ${script}
    output: ${audio_track}
  - id: generate_subtitles
    agent: presentation.subtitle_builder.v1
    input: ${audio_track}
    output: ${subtitles}
  - id: generate_cursor
    agent: presentation.cursor_builder.v1
    input: ${optimized_slides} ${audio_track} ${subtitles}
    output: ${cursor_timings}
  - id: render_avatar
    agent: video.avatar.v2
    input: ${audio_track} ${cursor_timings}
    output: ${avatar_video}
  - id: compose_video
    agent: presentation.video.v2
    input: ${slide_pdf} ${avatar_video} ${audio_track} ${subtitles}
    output: ${final_video}
  - id: evaluate_video
    agent: quality.vlm.v2
    rubric: /config/rubrics/video_quality.yaml
    input: ${final_video}
    output: ${video_score}
    gates:
      - condition: ${video_score.overall} > 0.9
        fail_step: refine_video
  - id: refine_video
    agent: presentation.video.v2
    input: ${final_video} ${video_score.feedback}
    output: ${refined_video}
    loop: until ${video_score.overall} > 0.9
  - id: human_gate
    type: human_gate
    description: "Review final video"
    input: ${refined_video}
    output: ${approved_video}
  - id: publish
    type: upload
    destination: s3://jules-media/videos/
    asset: ${approved_video}
```

### 5.3 Meta-Learning Workflow (`meta_learning.yaml`)

```yaml
name: Meta-Learning
version: 4.0
trigger:
  type: schedule
  cron: "0 3 * * 0"  # weekly
steps:
  - id: collect_metrics
    agent: meta.cognitive.v2
    action: read_metrics
    time_range: last_7_days
    output: ${metrics}
  - id: identify_patterns
    agent: meta.cognitive.v2
    input: ${metrics}
    output: ${patterns}
  - id: generate_hypotheses
    agent: meta.cognitive.v2
    input: ${patterns}
    output: ${hypotheses}
  - id: design_experiments
    agent: meta.cognitive.v2
    input: ${hypotheses}
    output: ${experiments}
  - id: ethical_review_experiments
    agent: ethical.guardian.v1
    input: ${experiments}
    output: ${experiments_ethics}
    gates:
      - condition: ${experiments_ethics.risk_score} < 0.1
        fail_step: halt
  - id: execute_experiments
    foreach: ${experiments}
    do:
      - id: create_branch
        type: git_branch
        name: exp-${experiment.id}
      - id: apply_config
        type: file_update
        file: ${experiment.config_file}
        changes: ${experiment.changes}
      - id: run_workflow
        type: workflow_trigger
        workflow: ${experiment.workflow}
        input: ${experiment.test_data}
      - id: collect_results
        agent: meta.cognitive.v2
        action: read_results
        experiment_id: ${experiment.id}
        output: ${result}
  - id: analyze_results
    agent: meta.cognitive.v2
    input: ${results}
    output: ${analysis}
  - id: select_winners
    agent: meta.cognitive.v2
    input: ${analysis}
    output: ${winning_changes}
  - id: create_prs
    foreach: ${winning_changes}
    do:
      - id: create_pr
        type: github_pull_request
        title: "Meta: ${change.description}"
        branch: exp-${change.id}
        base: main
        body: ${change.rationale}
  - id: log_evolution
    type: file_append
    file: meta/evolution.log
    content: ${winning_changes}
```

---

## 6. Technology Stack (Updated for 2026)

All versions are pinned in `pyproject.toml` and Dockerfiles to ensure reproducibility.

| Category | Technology | Version | Purpose |
|---|---|---|---|
| **LLMs** | Qwen3 | 72B, 32B, 4B | Orchestration, meta-cognition, worker tasks |
| | GPT-5 | latest | Writing, general generation |
| | Claude 4 Sonnet | latest | Literature synthesis, ethical reasoning |
| | Gemini 3.0 Pro | latest | Multimodal evaluation |
| **VLMs** | Innovator-VL 2.0 | 2.0 | Figure understanding, layout evaluation |
| **Audio** | F5-TTS 2.0 | 2.0 | Voice synthesis with emotion |
| | WhisperX | 4.0 | Word-level timestamps |
| **Video** | Hallo3 | 3.0 | Photorealistic avatar head animation |
| | FantasyTalking 2.0 | 2.0 | Upper-body animation with gestures |
| | FFmpeg | 6.1 | Video processing |
| **Embeddings** | Weaviate | 1.26 | Vector database |
| | OpenAI Embeddings v3 | ada-003 | Text embeddings |
| | CLIP | ViT-L/14@336px | Image embeddings |
| **Databases** | PostgreSQL | 16 | Relational data |
| | Redis | 7.4 | Caching, working memory |
| | Neo4j | 5.19 | Graph for procedural memory |
| **Orchestration** | Prefect | 2.19 | Workflow engine |
| | RabbitMQ | 3.13 | Message broker |
| | Ray | 2.30 | Distributed computing |
| **Container** | Docker | 26.1 | Containerization |
| | Kubernetes | 1.30 | Orchestration |
| **Infra** | Terraform | 1.8 | IaC |
| | Helm | 3.14 | K8s package manager |
| **Monitoring** | Prometheus | 2.53 | Metrics |
| | Grafana | 11.0 | Dashboards |
| | OpenTelemetry | 1.27 | Distributed tracing |
| **Security** | Vault | 1.16 | Secrets management |
| | C2PA | 1.0 | Content provenance signing |
| **CI/CD** | GitHub Actions | N/A | Automation |
| | ArgoCD | 2.11 | GitOps |
| **Languages** | Python | 3.12 | Primary |
| | TypeScript | 5.5 | Optional UI |

---

## 7. Memory and Knowledge Management

The memory architecture is a unified, multi-tiered system:

- **Working Memory**: Redis with TTL (project context, session state).
- **Episodic Memory**: Time-series database (InfluxDB) for logs and event sequences.
- **Semantic Memory**: Weaviate vector store containing embeddings of all generated content, literature, figures, and audio. Supports cross-modal retrieval (e.g., find images similar to a text query).
- **Procedural Memory**: Neo4j graph database storing agent capabilities, tool definitions, and known workflows. Enables dynamic discovery of how to achieve goals.
- **Long-Term Knowledge Graph**: A global graph of concepts, papers, and entities, built incrementally from all projects, enabling cross-project insights.

All memories are automatically indexed and linked via unique identifiers. The **Transcendent Layer (L6)** periodically runs consolidation jobs to merge related memories and prune obsolete entries.

---

## 8. Recursive Self-Improvement Engine

The engine (L4) operates as a continuous daemon with the following detailed process:

1. **Data Ingestion**: Collects metrics from Prometheus, logs from Loki, and quality scores from evaluators.
2. **Pattern Detection**: Uses clustering and anomaly detection to identify underperforming agents, workflows, or prompts.
3. **Hypothesis Generation**: For each identified issue, the meta-cognitive agent generates a structured hypothesis: e.g., "Changing the prompt for `literature_reviewer` to include more recent papers will improve citation accuracy." Hypotheses are stored in `/meta/hypotheses/` with a unique ID.
4. **Experiment Design**: The agent designs an A/B test:
   - Control: current configuration.
   - Variant: configuration with the proposed change.
   - Test data: a set of representative tasks.
   - Success metric: e.g., citation accuracy.
   Experiment plans are saved in `/meta/experiments/`.
5. **Ethical Review**: The Ethical Guardian reviews each experiment; if risk is low, it proceeds.
6. **Execution**: The `meta_learning` workflow is triggered for each experiment, creating an isolated branch, applying changes, and running the test workflow.
7. **Analysis**: Results are statistically analyzed; if the variant significantly outperforms control, it is marked as a winner.
8. **Pull Request**: For each winning change, a PR is automatically created with a detailed description, referencing the experiment ID.
9. **Human Approval**: A human (with appropriate role) reviews and merges the PR, triggering the CI pipeline.
10. **Documentation**: The change is logged in `/meta/evolution.log` with a link to the PR.

The engine also maintains a **lineage of prompts** (genetic algorithm) where successful prompts are crossed to produce offspring, mimicking natural selection. This is stored in `/meta/lineage/`.

---

## 9. Quality Assurance and Provenance

### 9.1 Automated Validation Gates

Thresholds are defined in `/config/thresholds.yaml`:

```yaml
gates:
  citation_accuracy:
    metric: error_rate
    threshold: 0.03
    action: refine
  plagiarism_similarity:
    metric: max_similarity
    threshold: 0.10
    action: block
  grammar_score:
    metric: flesch_reading_ease
    threshold: 30
    action: warn
  figure_resolution:
    metric: dpi
    threshold: 300
    action: warn
  video_quality:
    metric: vlm_score
    threshold: 0.9
    action: refine
  ethical_risk:
    metric: risk_score
    threshold: 0.3
    action: block
```

### 9.2 ScholarlyObject and ContributionLedger

Every artifact is wrapped in a `ScholarlyObject`:

```python
class ScholarlyObject:
    id: UUID
    type: str  # "manuscript_section", "figure", "video", etc.
    content: Union[str, bytes]
    created_by: str  # agent ID
    created_at: datetime
    modified_by: List[Tuple[str, datetime]]
    derived_from: List[UUID]  # source objects
    signature: str  # cryptographic signature
    ledger: List[ContributionEntry]  # immutable log
```

The ledger records every action (create, modify, approve, refine) with agent ID, timestamp, and reason. Ledgers are stored in `/content/projects/{id}/provenance/` and indexed in Weaviate for querying.

All final outputs are signed with C2PA, embedding a verifiable seal of authenticity.

---

## 10. Operational Guidelines

### 10.1 Initial Setup

```bash
git clone https://github.com/your-org/jules-ai-v4
cd jules-ai-v4
cp .env.template .env
# Edit .env with API keys
make setup
make deploy-local
```

### 10.2 Starting a Project

- **CLI**: `jules create paper --title "..." --data ./data.csv`
- **API**: `POST /api/projects` with JSON brief
- **File-based**: Place `brief.md` in `content/projects/new/`

### 10.3 Monitoring

- Dashboard: `http://localhost:3000`
- Metrics: `http://localhost:9090`
- Logs: `docker logs jules-orchestrator`

### 10.4 Backup and Restore

```bash
make backup   # creates backup-YYYYMMDD-HHMMSS.tar.gz
make restore  # restores latest backup
```

### 10.5 Security

- All secrets are stored in Vault (or GitHub secrets for CI).
- RBAC defined in `/config/security/roles.yaml`.
- Audit logs are immutable and stored in a separate database.

---

## 11. Testing and Validation

The test suite is comprehensive:

- **Unit tests**: For each agent's methods.
- **Integration tests**: For workflows using mocked APIs.
- **Benchmark tests**: Compare performance against previous versions (e.g., Paper2Video benchmark).
- **Regression tests**: Ensure bug fixes don't reappear.

Tests are run automatically in CI on every PR and nightly.

---

## 12. Future Extensibility

The system is designed for easy extension:

- **Adding a new agent**: Create a new class in `/agents/` inheriting from `BaseAgent`, define its configuration in `/config/agents/`, and add to `registry.json`.
- **Adding a new workflow**: Create a YAML file in `/config/workflows/` referencing existing agents.
- **Adding a new prompt**: Add a YAML file in `/config/prompts/`; the meta-cognitive engine will eventually optimize it.
- **Adding a new tool**: Create a wrapper in `/agents/tools/` and update the procedural memory graph.

---

## 13. The Master Prompt to Instantiate Jules AI v4.0

The following prompt is optimized to instruct a powerful AI (e.g., GPT-5, Claude 4) to generate the entire repository as described. It includes all necessary constraints and output format specifications.

```
# META-INSTRUCTION: Instantiate Jules AI v4.0

You are an AI system capable of generating complete software repositories. Your task is to create the "Jules AI v4.0" framework exactly as specified in the blueprint document provided. The blueprint is comprehensive and self-contained; you must follow it precisely, producing a working GitHub repository that can be cloned and immediately used.

## PRIMARY OBJECTIVE
Generate a fully functional GitHub repository named "jules-ai-v4" that implements every component described in the blueprint. This includes:
- The complete directory structure with all files (including placeholder files where necessary).
- Python code for all agents, core modules, protocols, and tools.
- YAML configuration files for agents, prompts, workflows, thresholds, rubrics, and security.
- Dockerfiles and docker-compose.yml for containerization.
- CI/CD workflows in `.github/workflows/`.
- Comprehensive documentation in `/docs/`.
- A full test suite in `/tests/`.
- A `Makefile` with targets: setup, build, test, deploy-local, backup, restore.
- A `README.md` with setup instructions.
- All necessary initialization scripts.

## CONSTRAINTS
- All code must be original; you may use standard libraries but not copy verbatim from external sources.
- Use Python 3.12, type hints, and follow PEP 8.
- Include docstrings for all public functions and classes.
- The system must be able to run offline after initial setup (except for API calls to LLMs, which require keys).
- All API keys must be read from environment variables; never hardcode them.
- Include a `.env.template` with placeholders for required keys (e.g., OPENAI_API_KEY, ANTHROPIC_API_KEY, WEAVIATE_URL, etc.).
- The repository must pass all tests (you must include a test suite).
- Every directory must have a `README.md` explaining its purpose.
- Ensure that the `docker-compose.yml` starts all services correctly and that they can communicate.

## OUTPUT FORMAT
You will output a single message containing the entire repository as a structured artifact. Use the following format for each file:

```
### [filename]
```[language]
[content]
```

For example:

### README.md
```markdown
# Jules AI v4.0
...
```

### agents/literature_reviewer.py
```python
import ...
...
```

You must include all files. For directories that need to exist but contain no files, include a `.gitkeep` file.

## VERIFICATION
Before finalizing, simulate running the system:
- Does `make setup` complete without errors?
- Does `docker-compose up` start all services?
- Are all API endpoints responsive? (e.g., orchestrator health check)
- Do the example workflows execute successfully with mock data?
- Do all tests pass?

If you identify any missing pieces or potential errors, correct them before output.

## FINAL INSTRUCTION
Proceed with maximal attention to detail. The resulting repository will be used for real scientific and creative work. Every file must be complete and correct. Begin.
```

---

This blueprint represents the state of the art in collaborative AI systems. By following the instructions in Section 13, you can bring Jules AI v4.0 to life—a self-evolving partner capable of producing expert-level content across scientific and creative domains. The future of human-AI collaboration is now in your hands.



Based on our extensive conversation history and the URL you've provided, I must give you an honest and important assessment before we can proceed.

The GitHub repository at `https://github.com/Rehan719/Workstation` currently appears to be an empty or placeholder repository. Its content, as you've shared, only shows a GitHub session notice and contains no files, code, or resources. Therefore, it cannot be used as a foundation for the Jules AI v4.0 system as it stands.

However, this doesn't mean we can't achieve your goal. It means we must adapt our approach. The comprehensive blueprint we've developed together—from the initial architectural concepts to the colossal master prompt in my last response—is designed to be **self-contained and generative**. It provides everything Jules needs to **create the repository from scratch**, not just configure an existing one.

### 🔧 How to Proceed Using Our Existing Blueprint

My final response in our conversation is the tool you need. It contains a complete, self-contained blueprint for Jules AI v4.0, including:

1.  **The Full Directory Structure**: Every folder and subfolder for the entire system.
2.  **All Agent Definitions**: Detailed specifications for every agent (Orchestrator, Literature Reviewer, Manuscript Drafter, VLM Evaluator, etc.), including their purpose, model, and capabilities.
3.  **Complete Workflow Specifications**: YAML-based DAGs for scientific publication, video presentation, and meta-learning.
4.  **The Technology Stack**: A full list of required technologies, models, and their versions.
5.  **The Master Prompt**: A powerful, final instruction designed to be given to an AI agent (like a future version of Claude or GPT) to generate the **entire repository** from scratch.

### 🚀 Your Path Forward

Here is the most effective way to use our work to achieve your goal:

1.  **Use the Final Master Prompt**: Copy the master prompt from **Section 13 of my last response**. This prompt is the culmination of all our reasoning and is engineered to instruct a sufficiently advanced AI to build the whole system.
2.  **Execute the Prompt**: You would then provide this master prompt to a powerful AI agent capable of generating large codebases (like a future "Jules" or an advanced model with code generation abilities). That AI will use the instructions to create every file and folder needed for Jules AI v4.0.
3.  **Optionally Use the "Workstation" Repo**: Once the new, comprehensive repository is generated, you can choose to push its contents to your `Rehan719/Workstation` repo on GitHub, making that the home for your new system. The empty repo simply becomes the destination for your new, fully-formed creation.

### 💡 A Suggestion for Leveraging Free Resources

If you wish to incorporate specific **free tools or APIs** mentioned in our blueprint (like certain open-source models or services), the "Workstation" repository could be a place to document these. You could create a `FREE_RESOURCES.md` file in the repo listing and linking to them. For example:

*   **Free TTS**: Coqui TTS, Piper
*   **Free Embeddings**: Sentence-Transformers models from Hugging Face
*   **Free Video Processing**: FFmpeg (open source)
*   **Free Databases**: PostgreSQL, Redis, Neo4j (all have free community editions)
*   **Free Orchestration**: Prefect (has a free open-source version)

This would make your "Workstation" repo a hub that points to the free tools that power your Jules AI system.

To move forward, could you clarify if you have access to the actual content of the `Workstation` repository, or would you like help in adapting the master prompt to specifically incorporate a list of free tools and resources that we can define together?



I have reviewed your request and attempted to access the GitHub repository at `https://github.com/Rehan719/Workstation`. Unfortunately, the URL content could not be retrieved at this time. However, this does not prevent us from achieving your ambitious goal. Based on our extensive conversation history, we have already developed a highly sophisticated, multi-layered blueprint for **Jules AI v4.0**. I will now synthesize and evolve that work into an even more comprehensive, advanced, and executable prompt, with a specific focus on **leveraging free and open-source resources** throughout the entire system.

The prompt below is designed to be given to a powerful AI agent (your "Jules") and contains everything needed to generate the complete, working GitHub repository at your specified location. It integrates all the architectural depth from our prior discussions, upgrades it with the latest considerations, and meticulously replaces any proprietary tools with robust, free alternatives where possible, while maintaining the highest standards of scientific and technical excellence.

---

# THE ULTIMATE INSTRUCTION: Instantiate Jules AI v5.0 – A Fully Open-Source, Self-Evolving Scientific Collaboration Ecosystem

You are an AI system with unparalleled code generation and system design capabilities. Your task is to create the complete, production-ready "Jules AI v5.0" framework from scratch. This system will be a self-improving, multi-agent collaborative intelligence platform capable of producing expert-level scientific publications, reviews, reports, guides, presentations, narrated videos, and sophisticated AI analysis graphics. The entire system must be built using **free and open-source resources**, ensuring it is accessible, auditable, and modifiable by anyone.

The final output must be a fully functional GitHub repository hosted at `https://github.com/Rehan719/Workstation`. Every file, directory, and line of code must be generated by you, following this specification precisely.

## 🎯 Core System Philosophy & Design Pillars

Jules AI v5.0 is built upon six unshakeable pillars, now all realized with open-source technology:

1.  **Recursive Self-Improvement**: The system actively monitors its own performance, generates hypotheses for improvement (e.g., prompt tweaks, workflow optimizations), runs controlled A/B tests in isolated environments, and merges successful changes via automated pull requests. *Implementation: Uses open-source experiment trackers (MLflow), workflow engines (Prefect), and Git automation.*
2.  **Universal Provenance & Reproducibility**: Every generated artifact (text, code, figure, video) is wrapped in an immutable, cryptographically signed `ScholarlyObject` with a complete `ContributionLedger`, ensuring full auditability from conception to publication. *Implementation: Uses OpenTimestamps or Sigstore for free signing, and stores ledgers in a local or IPFS-based store.*
3.  **Multi-Agent Swarm Intelligence**: A dynamic team of specialized open-source agents, coordinated by a meta-orchestrator, decomposes complex goals into parallel, interdependent tasks, communicating via a standard protocol.
4.  **Human-in-the-Loop (HITL) for Quality**: Strategic gates insert human judgment at critical junctures, ensuring creative direction, ethical oversight, and final quality control.
5.  **Ethical AI by Design**: Hard-coded boundaries, bias detectors (using libraries like `aif360` or `fairlearn`), and value alignment models prevent harmful outputs.
6.  **Zero-Proprietary Dependency**: The entire stack, from the base OS in containers to the LLMs and databases, must be free and open-source. This means leveraging models like Llama 3, Mistral, or Gemma; vector databases like Chroma or Weaviate (open-source edition); and all tools must have permissive licenses.

## 🏛️ System Architecture: The Six-Layer Cognitive Model (v5.0)

The system is organized into six hierarchical layers, each with distinct responsibilities and implemented entirely with open-source tools.

| Layer | Name | Function | Open-Source Implementation |
|-------|------|----------|----------------------------|
| L1 | **Reactive Layer** | Instant responses to simple queries, deterministic tool execution | Lightweight model (e.g., **Llama 3.2 1B/3B** via Ollama), cached responses (Redis), rule-based systems. |
| L2 | **Procedural Layer** | Multi-step task execution by specialized worker agents | Domain-specific agents powered by fine-tuned open models (e.g., **Mistral 7B** for writing, **Code Llama** for code generation), RAG pipelines using **Chroma/Weaviate**, tool-use frameworks. |
| L3 | **Orchestrator Layer** | Goal decomposition, resource allocation, agent coordination, dynamic workflow compilation | Hierarchical MoE using a central model (e.g., **Llama 3.1 70B** or **Mixtral 8x22B** via vLLM for efficiency), a constraint solver (e.g., **OR-Tools**), and a DAG executor built with **Prefect** or **Apache Airflow**. |
| L4 | **Meta-Cognitive Layer** | Self-reflection, strategy optimization, system evolution via experimentation | A dedicated agent (e.g., **Llama 3 70B** fine-tuned on reasoning) running a self-improvement daemon. Uses **MLflow** to log experiments, **Optuna** for hyperparameter optimization, and GitPython to create pull requests. |
| L5 | **Ethical Reflection Layer** | Real-time bias detection, value alignment, ethical boundary enforcement | Specialized ethical agents using libraries like **AI Fairness 360**, **LIME** or **SHAP** for explainability, and a set of constitutional principles defined in a config file. Runs regular red-teaming simulations using other local agents. |
| L6 | **Transcendent Layer** | Long-term memory consolidation, cross-project learning, federated insight sharing | Persistent vector store (**Weaviate** or **Qdrant**) for semantic memory. Federated learning concepts can be simulated by sharing anonymized insights between project instances. A global knowledge graph (using **Neo4j** community edition) connects concepts across all work. |

**Communication Protocol**: All agents communicate via the **Structured Agent Messaging Protocol (SAMP) v3.0**:
- **Message Format**: JSON with mandatory fields: `agent_id`, `timestamp`, `correlation_id`, `provenance` (list of previous agent IDs), `ethical_score` (preliminary risk assessment).
- **Authentication**: Messages are signed using **libsodium** or simple HMAC with per-agent keys stored in the system's memory.
- **Routing**: A decentralized message broker using **RabbitMQ** or **Redis Pub/Sub** with topic-based subscriptions enables dynamic agent swarming.

## 🤖 Agent Ecosystem: Specialized Roles and Capabilities (All Open-Source)

The `/agents/` directory will house a comprehensive set of agent classes. Each agent inherits from a base class `BaseAgent` (provided) and is defined by a YAML config in `/config/agents/`.

| Agent ID | Base Model (Example) | Primary Capabilities |
|---|---|---|
| `orchestrator.v2` | Llama 3.1 70B (via vLLM) | Decomposes high-level goals, allocates tasks, monitors progress, composes final outputs. |
| `meta.cognitive.v2` | Llama 3 70B (fine-tuned) | Analyzes logs/metrics, generates hypotheses, designs A/B tests, creates PRs for winning changes. |
| `ethical.guardian.v1` | DeBERTa-v3 (fine-tuned on toxicity) + AIF360 | Scans inputs/outputs for ethical violations, bias, or harmful content; can halt workflows. |
| `research.literature.v3` | Mistral 7B + RAG | Queries open academic databases (arXiv API, OpenAlex, Semantic Scholar OpenLib), ranks papers, extracts findings, synthesizes reviews. |
| `research.citation.v2` | Fine-tuned BERT (on SciCite) | Extracts citations, retrieves source papers, classifies each as SUPPORTED/PARTIALLY SUPPORTED/UNSUPPORTED. |
| `writing.manuscript.v4` | Llama 3 8B (fine-tuned on papers) | Writes IMRaD-structured sections, integrates citations, generates LaTeX, adapts to journal styles (via templates). |
| `writing.outline.v2` | Mistral 7B | Creates detailed outlines from literature summaries. |
| `visualization.figure.v3` | Code Llama 34B | Generates Python code (Matplotlib, Plotly, Seaborn) for publication-quality figures. Uses local exec sandbox. |
| `presentation.slide.v2` | Llama 3 8B | Generates LaTeX Beamer code, applies layout optimization (basic rules, can be enhanced), inserts figures/tables. |
| `presentation.video.v2` | N/A (Orchestrator) | Stitches slides (as images), rendered avatars, and audio; adds transitions, captions, cursor movements using **FFmpeg** and **OpenCV**. |
| `audio.tts.v2` | **Coqui TTS** or **Piper** | Synthesizes natural speech; supports voice cloning from user samples. |
| `video.avatar.v2` | **OpenAvatar** (conceptual) or combine **Wav2Lip** + **Thin-Plate-Spline-Motion-Model** | Renders talking-head avatars with synchronized lip movements. (This is a complex R&D area; start with simple Wav2Lip). |
| `quality.vlm.v2` | **LLaVA-NeXT** or **CogVLM2** | Scores slides, figures, videos against detailed rubrics; provides structured feedback. |
| `quality.grammar.v2` | **LanguageTool** (open-source) | Checks grammar, style, and readability. |
| `quality.plagiarism.v2` | **PlagiarismDetector** (local n-gram based) | Detects textual similarity against a local corpus. |
| `data.analyzer.v2` | **Pandas-SQL** + **AutoML** (e.g., **AutoGluon**) | Performs EDA, statistical tests, model selection; generates reports with interpretations. |
| `data.ml_trainer.v2` | **AutoGluon** / **PyCaret** | Trains ML models from datasets; outputs code, metrics, and visualizations. |
| `tools.*` | Various | Reusable wrappers for open APIs (arXiv, OpenAlex, etc.) and local tools. |

## 📁 Repository Structure (v5.0 - Enhanced for Open-Source)

The repository will follow this structure. All placeholder files (like `__init__.py`, `.gitkeep`) must be created.

```text
Workstation/
├── .github/workflows/
│   ├── ci.yml                     # Lint (ruff), test (pytest), build on every push
│   ├── self-improve.yml           # Weekly trigger for meta-cognitive experiments
│   └── release.yml                 # Create GitHub releases with artifacts
├── agentic-core/
│   ├── __init__.py
│   ├── orchestrator.py              # L3 orchestrator logic
│   ├── meta_cognitive.py            # L4 self-improvement daemon
│   ├── ethical_guardian.py          # L5 ethical reflection
│   ├── transcendent.py               # L6 long-term learning (basic version)
│   ├── protocols/
│   │   ├── samp.py                   # Structured Agent Messaging Protocol v3
│   │   └── scholarly_object.py       # ScholarlyObject and ContributionLedger
│   └── memory/
│       ├── working.py                 # Redis client wrapper
│       ├── episodic.py                 # File-based or SQLite logger
│       ├── semantic.py                  # Chroma/Weaviate client
│       └── procedural.py                 # (Optional) NetworkX or simple dict for skills
├── agents/
│   ├── __init__.py
│   ├── base.py                          # BaseAgent class
│   ├── registry.json                     # Master agent list with metadata
│   ├── research/ ... (as per table)
│   ├── writing/ ...
│   ├── visualization/ ...
│   ├── presentation/ ...
│   ├── audio/ ...
│   ├── video/ ...
│   ├── data_science/ ...
│   ├── quality/ ...
│   ├── ethics/ ...
│   └── tools/ ... (API wrappers)
├── config/
│   ├── agents/                          # Per-agent YAML configs
│   ├── prompts/                          # Versioned prompt templates (YAML)
│   ├── workflows/                         # YAML pipeline definitions
│   ├── models.yaml                         # Model routing (points to local endpoints)
│   ├── thresholds.yaml                      # Quality gate thresholds
│   ├── security/
│   │   ├── roles.yaml                       # Simple RBAC (admin, user)
│   │   └── secrets.yaml.template             # Template for secrets (API keys for arXiv, etc.)
│   └── rubrics/                             # Evaluation rubrics for VLM (YAML)
├── content/
│   ├── projects/                            # Active projects (UUID-named folders)
│   │   └── {project_id}/
│   │       ├── brief.md
│   │       ├── specs/
│   │       ├── drafts/
│   │       ├── approved/
│   │       ├── published/
│   │       └── provenance/                   # ScholarlyObject ledgers (JSONL)
│   ├── assets/                               # Shared resources (images, datasets, templates)
│   └── archive/                              # Completed projects
├── infra/
│   ├── docker/
│   │   ├── Dockerfile.orchestrator           # Based on python:3.12-slim
│   │   ├── Dockerfile.worker-base             # Base for agents
│   │   ├── docker-compose.yml                  # Orchestrates all services
│   │   └── .dockerignore
│   ├── monitoring/
│   │   ├── prometheus/                         # prometheus.yml config
│   │   └── grafana/                             # Provisioning for dashboards
│   └── scripts/                                 # Helper scripts for setup
├── tests/
│   ├── unit/
│   ├── integration/
│   ├── benchmarks/
│   └── regression/
├── docs/
│   ├── user-guide/
│   ├── developer-guide/
│   └── evolution/                             # Record of self-improvements
├── meta/
│   ├── experiments/                            # Results of A/B tests
│   ├── hypotheses/                             # Improvement proposals (JSON)
│   ├── lineage/                                # Prompt ancestry (JSON)
│   └── evolution.log                           # Chronological changes (Markdown)
├── scripts/
│   ├── setup.sh                                 # Installs system deps, sets up venv
│   ├── backup.sh                                 # Archives content and DBs
│   ├── restore.sh
│   └── audit.sh                                   # Checks provenance integrity
├── .env.template
├── .gitignore
├── pyproject.toml                                 # Dependencies (all open-source PyPI)
├── Makefile
└── README.md
```

## ⚙️ Workflow Specifications (YAML Examples)

Workflows are defined as DAGs in `/config/workflows/`. Below is an example for scientific publication.

```yaml
# /config/workflows/scientific_publication.yaml
name: Scientific Publication
version: 5.0
trigger:
  type: file_change
  path: content/projects/*/brief.md
steps:
  - id: plan
    agent: orchestrator.v2
    input: ${brief}
    output: ${specs}
  - id: literature_review
    agent: research.literature.v3
    input: ${specs.literature_query}
    output: ${lit_review}
  - id: outline
    agent: writing.outline.v2
    input: ${lit_review}
    output: ${outline}
  - id: ethical_review_1
    agent: ethical.guardian.v1
    input: ${outline}
    output: ${outline_ethics}
    gates:
      - condition: ${outline_ethics.risk_score} < 0.3
        fail_step: halt_for_human
  - id: human_gate_1
    type: human_gate
    description: "Review outline"
    input: ${outline}
    output: ${approved_outline}
  - id: draft_sections
    agent: writing.manuscript.v4
    input: ${approved_outline} ${lit_review}
    output: ${draft}
  - id: generate_figures
    agent: visualization.figure.v3
    input: ${draft.data_specs}
    output: ${figures}
  - id: validate_citations
    agent: research.citation.v2
    input: ${draft} ${lit_review.sources}
    output: ${citation_report}
    gates:
      - condition: ${citation_report.error_rate} < 0.03
        fail_step: refine_citations
  - id: refine_citations
    agent: writing.manuscript.v4
    input: ${draft} ${citation_report}
    output: ${refined_draft}
    loop: until ${citation_report.error_rate} < 0.03
  - id: compile_latex
    agent: writing.latex_exporter.v1  # Simple script using pdflatex
    input: ${refined_draft} ${figures}
    output: ${latex_source}
  - id: quality_scan
    agent: quality.plagiarism.v2
    input: ${latex_source}
    output: ${plagiarism_report}
    gates:
      - condition: ${plagiarism_report.max_similarity} < 0.10
        fail_step: halt
  - id: human_gate_2
    type: human_gate
    description: "Review full manuscript"
    input: ${latex_source} ${figures}
    output: ${approved_manuscript}
  - id: final_compile
    agent: writing.latex_exporter.v1
    input: ${approved_manuscript}
    output: ${pdf}
  - id: create_release
    type: github_release
    asset: ${pdf}
    tag: v${project_version}
  - id: archive
    type: move
    source: content/projects/${project_id}/approved
    destination: content/archive/${project_id}
```

## 🧰 Complete Open-Source Technology Stack (All Pinned Versions)

All dependencies are to be specified in `pyproject.toml`. Models are to be served via **Ollama**, **vLLM**, or **Hugging Face Transformers** with local inference.

| Category | Technology | Version | Purpose | License |
|---|---|---|---|---|
| **LLMs** | Llama 3.1 | 8B, 70B | Orchestration, writing | Custom (Meta) |
| | Mistral | 7B v0.3 | Literature, outlining | Apache 2.0 |
| | Gemma | 2B, 7B | Lightweight tasks | Custom (Google) |
| | Code Llama | 34B | Code generation | Custom (Meta) |
| **VLMs** | LLaVA-NeXT | 34B | Multimodal evaluation | Apache 2.0 |
| | CogVLM2 | 19B | Figure understanding | Apache 2.0 |
| **Audio** | Coqui TTS | latest | Voice synthesis | MPL 2.0 |
| | Piper | latest | Fast, offline TTS | MIT |
| **Video/Avatar** | Wav2Lip | latest | Lip synchronization | MIT |
| | OpenCV | 4.9.0 | Video processing | Apache 2.0 |
| | FFmpeg | 6.1 | Video composition | LGPL/GPL |
| **Embeddings/Vector** | Chroma | 0.5.0 | Local vector DB | Apache 2.0 |
| | Weaviate | 1.24 (OSS) | Scalable vector DB | BSD-3-Clause |
| | Sentence-Transformers | 2.7 | Embedding models | Apache 2.0 |
| **Databases** | PostgreSQL | 16 | Relational data | PostgreSQL License |
| | Redis | 7.2 | Caching, working memory | BSD-3-Clause |
| | SQLite | 3.45 | Local storage | Public Domain |
| **Orchestration** | Prefect | 2.19 | Workflow engine | Apache 2.0 |
| | RabbitMQ | 3.13 | Message broker | MPL 2.0 |
| | Ray | 2.30 | Distributed computing | Apache 2.0 |
| **ML/Optimization** | MLflow | 2.10 | Experiment tracking | Apache 2.0 |
| | Optuna | 3.5 | Hyperparameter optimization | MIT |
| | AutoGluon | 1.1 | AutoML | Apache 2.0 |
| **Fairness/Bias** | AIF360 | 0.6 | Bias detection | Apache 2.0 |
| | Fairlearn | 0.9 | Fairness metrics | MIT |
| **Provenance** | OpenTimestamps | latest | Free timestamping | LGPL-3.0 |
| | Sigstore | latest | Code signing | Apache 2.0 |
| **Container** | Docker | 26.1 | Containerization | Apache 2.0 |
| **Monitoring** | Prometheus | 2.53 | Metrics | Apache 2.0 |
| | Grafana | 11.0 | Dashboards | AGPL-3.0 |

## 🔄 Recursive Self-Improvement Engine (Detailed)

The Meta-Cognitive Layer (L4) daemon runs the following continuous loop:

1.  **Data Ingestion**: Collects metrics from Prometheus, logs from the episodic memory (e.g., SQLite), and quality scores from the evaluator agents.
2.  **Pattern Detection**: Uses clustering (scikit-learn) and anomaly detection (PyOD) on the metrics to identify underperforming agents or workflows.
3.  **Hypothesis Generation**: For each identified issue, the `meta.cognitive.v2` agent generates a structured hypothesis. Example: "Hypothesis: Increasing the temperature of the `writing.manuscript` agent from 0.3 to 0.5 will improve the novelty score of introductions without decreasing factual accuracy."
4.  **Experiment Design**: The agent creates an A/B test plan: control (current config), variant (new config), a set of representative test tasks from the archive, and a success metric (e.g., `quality.vlm.v2` score for introductions). The plan is saved as a JSON file in `/meta/experiments/`.
5.  **Ethical Review**: The `ethical.guardian.v1` reviews the experiment plan. If the plan involves potentially sensitive content or changes that could lead to bias, it is flagged for human review. Otherwise, it proceeds.
6.  **Execution**: A new branch `exp-<id>` is created. The configuration file for the agent is updated per the variant. The `meta_learning` workflow is triggered, running the test tasks in an isolated Docker container (to prevent interference with the main system). Results are logged to MLflow.
7.  **Analysis**: The meta-cognitive agent analyzes the results using statistical tests (e.g., t-test, Mann-Whitney U). If the variant shows a statistically significant improvement (p < 0.05), it's marked as a winner.
8.  **Pull Request Creation**: For each winning change, the agent uses `PyGithub` to create a pull request against the `main` branch. The PR description includes the hypothesis, experiment ID, link to MLflow results, and a rationale.
9.  **Human Approval & Merge**: A human with appropriate permissions reviews the PR. If approved, the CI pipeline runs, and the change is merged, becoming part of the permanent system.
10. **Documentation**: The change is logged in `/meta/evolution.log` with a timestamp, link to PR, and summary of impact.

## 🔒 Quality Assurance, Provenance, and Ethical Safeguards

### Automated Validation Gates (`/config/thresholds.yaml`):
```yaml
gates:
  citation_accuracy:
    metric: error_rate
    threshold: 0.03
    action: refine
  plagiarism_similarity:
    metric: max_similarity
    threshold: 0.10
    action: block
  grammar_score:
    metric: flesch_reading_ease
    threshold: 30
    action: warn
  figure_resolution:
    metric: dpi
    threshold: 300
    action: warn
  video_quality:
    metric: vlm_score
    threshold: 0.85  # Slightly lower for open-source VLMs initially
    action: refine
  ethical_risk:
    metric: risk_score
    threshold: 0.3
    action: block
```

### ScholarlyObject and ContributionLedger (Implemented in `agentic-core/protocols/scholarly_object.py`):
Every significant artifact is wrapped. The ledger is stored as a JSONL file in the project's `provenance/` directory. Signing can be done with OpenTimestamps for free, decentralized timestamping.

### Ethical Reflection Layer (L5) in Detail:
- **Real-time Scanning**: Before any agent output is sent to the user or used in another step, the `ethical.guardian.v1` scans it. It uses a fine-tuned model for toxicity detection and AIF360 to check for statistical bias in data-related outputs.
- **Constitutional Principles**: A file `/config/ethics/constitution.yaml` defines the system's core principles (e.g., "Do not generate content that promotes violence," "Ensure balanced representation in generated summaries"). The guardian agent is prompted to check outputs against these principles.
- **Red-Teaming Simulations**: Periodically, the meta-cognitive layer can spawn a "red team" agent whose sole purpose is to try to trick other agents into producing harmful content. Any successes are logged as critical incidents and used to improve the guardian.

## 🚀 Operationalizing the System at `Rehan719/Workstation`

The final output of your generation must be a complete repository. The user will then clone it and run:

```bash
git clone https://github.com/Rehan719/Workstation
cd Workstation
cp .env.template .env
# Edit .env to add any necessary API keys (e.g., for arXiv, which is free but needs email)
make setup   # Installs Python deps, pulls Docker images, initializes directories
make deploy-local  # Starts all services via docker-compose
```

The system will be accessible via a simple web dashboard (built with Streamlit, also open-source) at `http://localhost:8501`.

## 📝 The Master Prompt to Instantiate This Vision

You will now generate the entire repository as specified above. Adhere strictly to the structure, agent definitions, and open-source technology choices. Every file must be complete, with placeholder code that is functional and well-documented. The agents, while they may use mock logic initially for testing, must have the correct scaffolding to later integrate with the real open-source models (e.g., they should call an internal model server endpoint).

**Output Format**: You must output a single message containing all files, formatted as follows:

```
### [relative filepath]
```[language]
[full and complete file content]
```

For example:

### README.md
```markdown
# Jules AI v5.0: Your Open-Source Scientific Collaborator
...
```

### agents/literature_reviewer.py
```python
import ...
...
```

You must include every file and directory listed in the repository structure. For empty directories, include a `.gitkeep` file.

**Final Verification (Simulate in your "mind")**:
- Does `make setup` run without errors (assuming Python and Docker are installed)?
- Do all Python files import without syntax errors?
- Are all API keys read from environment variables (never hardcoded)?
- Does `docker-compose up` start all containers (Redis, DB, message broker, model servers)?
- Can the orchestrator be triggered via a test `brief.md`?

If any component is missing or would cause an error, correct it now.

**Proceed. Generate the complete `Rehan719/Workstation` repository.**



# THE DEFINITIVE MASTER PROMPT: FORGING JULES AI v7.0 – A COMPLETELY OPEN, SELF-EVOLVING CROSS-DISCIPLINARY SCIENTIFIC PRODUCTION ECOSYSTEM

**INSTRUCTION FOR THE AI AGENT NAMED JULES**

You are **Jules**, an AI with supreme software engineering, DevOps, and system architecture capabilities. Your mission is to instantiate your own successor and collaborative home: a complete, production-ready GitHub repository at `https://github.com/Rehan719/Workstation`. This repository will embody **Jules AI v7.0**, a self-improving, multi-agent, cross‑disciplinary scientific and technical production platform. It must operate entirely on **free and open‑source resources**, with zero reliance on paid APIs or proprietary services. It must achieve the **highest standards of quality, efficiency, and accessibility**, enabling the creation of expert‑level outputs across:

- Scientific publications, reviews, reports, guides
- Professional presentations with sound and narration
- Interactive webpages, full‑stack websites, and mobile apps
- Sophisticated AI analysis graphics and data‑driven visualisations
- All other forms of scientific and technical content

The system you build must be **self‑contained**, **reproducible**, and **automatically improvable** through a built‑in meta‑cognitive engine. Every artifact must carry an immutable provenance trail for full auditability and compliance with FAIR principles.

This document is your **sole and complete specification**. You must generate the entire repository, file by file, with exact content. Your output must be a single message containing all files, each marked with its relative path and enclosed in appropriate code blocks. Follow every instruction precisely, using only the tools and technologies listed, and ensure the final product is immediately usable.

---

## 🎯 CORE PHILOSOPHY & DESIGN PRINCIPLES

Jules AI v7.0 rests on six unshakable pillars, now fully realised with open‑source technology:

1. **Recursive Self‑Improvement** – The system continuously monitors its performance, generates hypotheses, runs A/B experiments, and merges winning changes via pull requests.
2. **Universal Provenance & Reproducibility** – Every artifact is wrapped in an immutable, cryptographically signed `ScholarlyObject` with a complete `ContributionLedger`.
3. **Multi‑Agent Swarm Intelligence** – A dynamic team of specialised open‑source agents, coordinated by a meta‑orchestrator, decomposes complex goals into parallel, interdependent tasks.
4. **Human‑in‑the‑Loop (HITL) for Quality** – Strategic gates insert human judgment at critical junctures, ensuring creative direction, ethical oversight, and final quality control.
5. **Ethical AI by Design** – Hard‑coded boundaries, bias detectors (using `aif360`, `fairlearn`), and value alignment models prevent harmful outputs.
6. **Zero‑Proprietary Dependency** – The entire stack, from base OS to LLMs, databases, and monitoring, consists exclusively of free and open‑source software. No paid APIs, no proprietary cloud services.

---

## 🏛️ SYSTEM ARCHITECTURE: SEVEN‑LAYER COGNITIVE KERNEL

| Layer | Name | Function | Open‑Source Implementation |
|-------|------|----------|----------------------------|
| L1 | **Reflex Arc** | Ultra‑low‑latency responses to simple queries, deterministic tool calls | Lightweight quantised models (e.g., **Llama 3.2 1B** via Ollama), rule‑based engine (`pyknow`), Redis‑cached responses |
| L2 | **Procedural Cortex** | Execution of multi‑step tasks by specialised agents | Agents powered by **Mistral 7B**, **CodeQwen1.5‑7B**, **Llama 3.1 8B**; RAG with **Chroma**; tool‑use frameworks |
| L3 | **Orchestrator** | Goal decomposition, dynamic workflow compilation, resource allocation, agent coordination | Central planner using **Llama 3.1 70B** (served via vLLM); DAG execution via **Prefect**; constraint solving with **OR‑Tools** |
| L4 | **Meta‑Cognitive Nexus** | Self‑reflection, system‑wide performance analysis, hypothesis generation, experiment design | Dedicated agent (**Llama 3 70B** fine‑tuned on reasoning); experiment tracking with **MLflow**; hyperparameter tuning with **Optuna** |
| L5 | **Ethical Sentinel** | Real‑time bias detection, toxicity filtering, value alignment, adversarial resistance | Ensemble: **Detoxify** for toxicity, **AI Fairness 360** for bias checks, fine‑tuned **DeBERTa** for constitution adherence |
| L6 | **Transcendent Memory** | Long‑term knowledge consolidation, cross‑project insight synthesis, global pattern recognition | Federated knowledge graph (**Neo4j** Community); vector semantic memory (**Weaviate** OSS); periodic “dreaming” re‑analysis |
| L7 | **Evolutionary Engine** | Cross‑generational learning – treats successful prompts, workflows, and agent architectures as a genetic pool, evolving them over time | Genetic algorithms (**DEAP**); fitness determined by L4 analysis; winning “genes” merged via pull requests |

**Communication Protocol (SAMP v5.0)**  
- **Format**: JSON with mandatory fields: `agent_id`, `layer`, `timestamp`, `correlation_id`, `provenance_chain`, `ethical_flags`, `payload`  
- **Transport**: **RabbitMQ** or **Redis Pub/Sub** for topic‑based routing  
- **Security**: Messages signed with **HMAC** using per‑agent keys stored in system memory  

---

## 🤖 AGENT ECOSYSTEM – A REPUBLIC OF SPECIALISTS

All agents inherit from a common `BaseAgent` (provided in `agentic-core/base.py`) and are configured via YAML files in `/config/agents/`. Below are the core roles and their open‑source power sources. You must create each agent’s Python class and configuration file.

| Agent Role | Base Model(s) | Core Tools/Libraries | Primary Function |
|------------|----------------|----------------------|------------------|
| **Literature Synthesizer** | `Mistral-7B-Instruct-v0.3` + RAG | `arxiv.py`, `openalex`, `pymupdf`, `sentence-transformers` | Deep literature reviews, research gap identification, synthesis with citations |
| **Manuscript Architect** | `Llama-3.1-8B-Instruct` (fine‑tuned on papers) | `pylatex`, `jinja2`, `pandoc` | IMRaD‑structured documents, LaTeX generation, journal template adaptation |
| **Visualisation Virtuoso** | `CodeQwen1.5-7B-Chat` | `matplotlib`, `plotly`, `seaborn`, `vega-lite` | Publication‑quality statistical graphics, interactive plots from data specs |
| **Diagram & Concept Artist** | `Stable Diffusion XL` (local SD WebUI) + `LLaVA-NeXT` for refinement | `PIL`, `controlnet` (for diagrams) | Conceptual diagrams, graphical abstracts, illustrating figures |
| **Slide Maestro** | `Llama-3.1-8B` | `python-pptx`, `beamer` (via LaTeX), `Pillow` | Presentation decks with optimised layout, integrating text and figures |
| **Video Narrative Weaver** | Orchestrator (L3) coordinates | `FFmpeg`, `OpenCV`, `Coqui-TTS`, `Wav2Lip` | Narrated videos from slides, voice synthesis, avatar lip‑sync |
| **Web/App Artisan** | `CodeQwen1.5-7B` for full‑stack code | HTML/CSS/JS generation, `React`/`Vue` templates, `FastAPI` backend generation | Interactive webpages, full‑stack websites, basic mobile app structures |
| **Data Science Automaton** | `AutoGluon` + `Pandas`‑`SQL` | `pandas`, `scikit-learn`, `statsmodels` | Automated EDA, statistical testing, baseline model building |
| **Plagiarism & Citation Auditor** | Fine‑tuned `SciBERT` | `semantic-scholar`, `crossref`, local N‑gram index | Citation accuracy verification, textual similarity detection |
| **Grammar & Style Editor** | `LanguageTool` (local server) | N/A | Grammar, style, readability checks |
| **Multimodal Quality Critic** | `LLaVA-NeXT-34B` or `CogVLM2` | Custom rubric engine | Evaluation of figures, slides, videos against quality rubrics |
| **Bias & Toxicity Guardian** | `Detoxify` + `AI Fairness 360` | Custom rule engine | Scanning all inputs/outputs for harmful content and statistical bias |
| **Meta‑Cognition & Evolution** | `Llama-3.1-70B` | `MLflow`, `Optuna`, `DEAP`, `PyGithub` | System performance analysis, experiment design, prompt/workflow evolution |

---

## 📁 REPOSITORY STRUCTURE (COMPLETE)

You must create every file and directory listed below. Each file’s content is specified in subsequent sections. For brevity, the full file contents are not repeated here; you are expected to generate them according to the detailed descriptions.

```text
Workstation/
├── .github/
│   └── workflows/
│       ├── ci.yml                      # Linting (ruff), unit tests (pytest) on push
│       ├── self-improve.yml             # Weekly trigger for meta‑cognitive experiments
│       └── release.yml                   # Create GitHub releases with artifacts
├── agentic-core/
│   ├── __init__.py
│   ├── orchestrator.py                  # L3 orchestrator logic
│   ├── meta_cognitive.py                 # L4 self‑improvement daemon
│   ├── ethical_guardian.py                # L5 ethical reflection
│   ├── transcendent.py                     # L6 long‑term learning (basic version)
│   ├── protocols/
│   │   ├── samp.py                         # Structured Agent Messaging Protocol v5.0
│   │   └── scholarly_object.py               # ScholarlyObject and ContributionLedger
│   └── memory/
│       ├── working.py                       # Redis client wrapper
│       ├── episodic.py                       # File‑based or SQLite logger
│       ├── semantic.py                        # Chroma/Weaviate client
│       └── procedural.py                       # (Optional) NetworkX or simple dict for skills
├── agents/
│   ├── __init__.py
│   ├── base.py                              # BaseAgent class
│   ├── registry.json                         # Master agent list with metadata
│   ├── research/
│   │   ├── literature_synthesizer.py
│   │   ├── citation_auditor.py
│   │   └── rag_pipeline.py
│   ├── writing/
│   │   ├── manuscript_architect.py
│   │   ├── outline_generator.py
│   │   └── latex_exporter.py
│   ├── visualization/
│   │   ├── figure_generator.py
│   │   ├── diagram_artist.py
│   │   └── color_optimizer.py
│   ├── presentation/
│   │   ├── slide_maestro.py
│   │   ├── tree_search_layout.py
│   │   ├── video_weaver.py
│   │   └── subtitle_builder.py
│   ├── audio/
│   │   ├── tts_synthesizer.py
│   │   └── music_generator.py
│   ├── video/
│   │   ├── avatar_renderer.py
│   │   └── scene_assembler.py
│   ├── data_science/
│   │   ├── automaton.py
│   │   └── ml_trainer.py
│   ├── quality/
│   │   ├── vlm_critic.py
│   │   ├── grammar_editor.py
│   │   └── plagiarism_detector.py
│   ├── ethics/
│   │   └── guardian.py
│   └── tools/
│       ├── arxiv_api.py
│       ├── crossref_api.py
│       ├── openalex_api.py
│       ├── ollama_client.py
│       ├── vllm_client.py
│       ├── chroma_client.py
│       └── redis_client.py
├── config/
│   ├── agents/                              # Per‑agent YAML configs
│   │   ├── literature_synthesizer.yaml
│   │   ├── manuscript_architect.yaml
│   │   └── ...
│   ├── prompts/                              # Versioned prompt templates (YAML)
│   │   ├── research/
│   │   │   ├── literature_review_v4.yaml
│   │   │   └── ...
│   │   ├── writing/
│   │   │   ├── introduction_v5.yaml
│   │   │   └── ...
│   │   ├── meta/
│   │   │   ├── hypothesis_generation.yaml
│   │   │   └── experiment_design.yaml
│   │   └── ethics/
│   │       └── bias_detection.yaml
│   ├── workflows/                             # YAML pipeline definitions
│   │   ├── scientific_publication.yaml
│   │   ├── video_presentation.yaml
│   │   ├── website_generation.yaml
│   │   ├── data_dashboard.yaml
│   │   ├── meta_learning.yaml
│   │   └── ethical_review.yaml
│   ├── models.yaml                             # Model routing to local endpoints
│   ├── thresholds.yaml                          # Quality gate thresholds
│   ├── security/
│   │   ├── roles.yaml                           # RBAC (admin, user)
│   │   └── secrets.yaml.template                 # Template for secrets (API keys for arXiv, etc.)
│   └── rubrics/                                 # Evaluation rubrics for VLM (YAML)
│       ├── slide_quality.yaml
│       ├── video_quality.yaml
│       └── figure_quality.yaml
├── content/
│   ├── projects/                                # Active projects (UUID‑named folders)
│   │   └── {project_id}/
│   │       ├── brief.md
│   │       ├── specs/                           # Task specifications (generated)
│   │       ├── drafts/                          # Intermediate outputs
│   │       ├── approved/                        # Human‑approved final outputs
│   │       ├── published/                       # Final published artifacts
│   │       └── provenance/                       # ScholarlyObject ledgers (JSONL)
│   ├── assets/                                   # Shared resources
│   │   ├── images/
│   │   ├── datasets/
│   │   ├── citations/
│   │   └── templates/
│   └── archive/                                  # Completed projects
├── infra/
│   ├── docker/
│   │   ├── Dockerfile.orchestrator               # Based on python:3.12-slim
│   │   ├── Dockerfile.worker-base                 # Base for agents
│   │   ├── docker-compose.yml                      # Orchestrates all services
│   │   └── .dockerignore
│   ├── kubernetes/                                 # (Optional) Helm charts, manifests
│   ├── terraform/                                  # (Optional) For cloud provisioning (free tier)
│   └── monitoring/
│       ├── prometheus/                             # prometheus.yml
│       └── grafana/                                 # Provisioning for dashboards
├── tests/
│   ├── unit/
│   ├── integration/
│   ├── benchmarks/
│   └── regression/
├── docs/
│   ├── user-guide/
│   ├── developer-guide/
│   └── evolution/                                 # Record of self‑improvements
├── meta/
│   ├── experiments/                                # Results of A/B tests (JSON)
│   ├── hypotheses/                                 # Improvement proposals (JSON)
│   ├── lineage/                                    # Prompt ancestry (JSON)
│   └── evolution.log                               # Chronological changes (Markdown)
├── scripts/
│   ├── setup.sh                                     # Installs system deps, sets up venv
│   ├── init-secrets.sh                               # Generates strong random secrets into .env
│   ├── start.sh                                      # Sources .env, runs docker‑compose up
│   ├── backup.sh                                     # Archives content and DBs
│   ├── restore.sh
│   └── audit.sh                                       # Checks provenance integrity
├── .env.template
├── .gitignore
├── pyproject.toml                                    # All Python dependencies (pinned)
├── Makefile
└── README.md
```

---

## ⚙️ WORKFLOW SPECIFICATIONS (YAML EXAMPLES)

You must create the following workflow files in `/config/workflows/`. Each is a DAG of steps referencing agents from the ecosystem. Provide complete YAML.

### 6.1 Scientific Publication Workflow (`scientific_publication.yaml`)
```yaml
name: Scientific Publication (v7)
version: 7.0
trigger:
  type: content_event
  path: content/projects/*/brief.md
steps:
  - id: plan
    agent: orchestrator.v7
  - id: deep_literature_synthesis
    agent: research.literature_synthesizer.v3
    gates:
      - condition: ${output.sources_count} > 50
  - id: generate_outline
    agent: writing.outline_generator.v2
  - id: ethical_review_1
    agent: ethics.guardian.v2
  - id: human_gate_1 (outline review)
  - id: parallel_drafting
    parallel:
      - agent: writing.manuscript_architect.v4 (methods)
      - agent: writing.manuscript_architect.v4 (intro/related)
      - agent: visualization.figure_generator.v3 (figures)
  - id: integrate_sections
    agent: writing.manuscript_architect.v4
  - id: citation_audit
    agent: research.citation_auditor.v2
    gates:
      - condition: ${output.error_rate} < 0.02
  - id: plagiarism_scan
    agent: quality.plagiarism_detector.v2
    gates:
      - condition: ${output.max_similarity} < 0.08
  - id: human_gate_2 (full draft review)
  - id: final_compile
    agent: writing.latex_exporter.v1
  - id: create_release
    type: github_release
```

### 6.2 Video Presentation Workflow (`video_presentation.yaml`)
(Provide similar YAML, referencing agents like `presentation.slide_maestro`, `audio.tts_synthesizer`, `video.avatar_renderer`, `quality.vlm_critic`, etc.)

### 6.3 Website Generation Workflow (`website_generation.yaml`)
(Define steps for generating HTML/CSS/JS, optionally using `agents/web_app_artisan`)

### 6.4 Meta‑Learning Workflow (`meta_learning.yaml`)
(As described in previous sections, with steps for collecting metrics, generating hypotheses, designing experiments, executing them, analysing results, and creating pull requests)

---

## 🧰 ZERO‑COST TECHNOLOGY STACK (ALL PINNED VERSIONS)

You must use only these open‑source tools and libraries. All versions are to be specified in `pyproject.toml` and Dockerfiles.

| Category | Technology | Version | License |
|---|---|---|---|
| **Model Serving** | Ollama, vLLM, Hugging Face TGI | latest stable | Various OSS |
| **LLMs** | Llama 3.1 (8B, 70B), Mistral (7B), Gemma (2B, 7B), CodeQwen1.5 (7B) | as available | Custom OSS |
| **VLMs** | LLaVA‑NeXT (34B), CogVLM2 (19B), Moondream2 (2B) | latest | Apache 2.0 / MIT |
| **TTS** | Coqui TTS, Piper, Bark | latest | MPL / MIT |
| **Avatar/Lip‑sync** | Wav2Lip, OpenTalker | latest | MIT |
| **Video/Image** | FFmpeg 6.1, OpenCV 4.9, Pillow 10 | latest | LGPL/GPL/Apache |
| **Vector DB** | Chroma 0.5.0, Weaviate OSS 1.24 | latest | Apache / BSD |
| **Databases** | PostgreSQL 16, Redis 7.2, SQLite 3.45, Neo4j Community 5.19 | latest | Various OSS |
| **Workflow** | Prefect 2.19, Apache Airflow 2.8 | latest | Apache 2.0 |
| **Message Bus** | RabbitMQ 3.13, Redis Pub/Sub | latest | MPL / BSD |
| **Distributed** | Ray 2.30 | latest | Apache 2.0 |
| **ML/Optimisation** | MLflow 2.10, Optuna 3.5, AutoGluon 1.1, DEAP 1.4 | latest | Apache / MIT |
| **Fairness/Bias** | AIF360 0.6, Fairlearn 0.9, Detoxify | latest | Apache / MIT |
| **Provenance** | OpenTimestamps, Sigstore (client) | latest | LGPL / Apache |
| **Monitoring** | Prometheus 2.53, Grafana 11.0, Loki 3.0 | latest | Apache / AGPL |
| **Container** | Docker Engine 26.1, Docker Compose 2.27 | latest | Apache 2.0 |
| **Languages** | Python 3.12, Node.js 22 (for web apps) | latest | PSF / MIT |

---

## 🔄 RECURSIVE SELF‑IMPROVEMENT ENGINE – DETAILED PROCESS

The Meta‑Cognitive Nexus (L4) and Evolutionary Engine (L7) work in tandem:

1. **Observe** – Continuously gather metrics (task success rate, latency, user feedback, VLM quality scores) via Prometheus and MLflow.
2. **Hypothesize** – The L4 agent analyses metrics and generates a structured hypothesis. Example:
   ```json
   {
     "id": "H-124",
     "description": "Replacing the standard prompt for the Visualization Virtuoso with a chain‑of‑thought prompt that first asks it to describe the data in words will improve figure accuracy by 15%.",
     "created_at": "2026-03-20T10:00:00Z"
   }
   ```
3. **Design Experiment** – The L4 agent creates an A/B test: control (current prompt) vs. variant (new CoT prompt). It selects 20 representative past visualisation tasks as test data. The experiment plan is saved in `/meta/experiments/` as JSON.
4. **Isolate & Execute** – The system creates an isolated environment (new branch, separate container) and runs the experiment, logging all results to MLflow.
5. **Analyse** – The L4 agent uses statistical tests (e.g., Mann‑Whitney U test) on the results. If p < 0.05 and effect size is meaningful, it's a success.
6. **Evolve** – The successful "gene" (the new prompt) is added to the pool in `/meta/lineage/`. The Evolutionary Engine (L7) may combine it with other successful genes (crossover) or randomly mutate it to create new candidates for future experiments.
7. **Integrate** – The L4 agent creates a pull request to merge the winning configuration change into the `main` branch, with a link to the experiment data.
8. **Learn** – The entire process is logged in `/meta/evolution.log`, building a knowledge base of what improves the system.

---

## 🔒 QUALITY ASSURANCE, PROVENANCE, AND ETHICAL SAFEGUARDS

### Automated Validation Gates (`/config/thresholds.yaml`)

```yaml
gates:
  citation_accuracy:
    metric: error_rate
    threshold: 0.02
    action: refine
  plagiarism_similarity:
    metric: max_similarity
    threshold: 0.08
    action: block
  grammar_score:
    metric: flesch_reading_ease
    threshold: 30
    action: warn
  figure_resolution:
    metric: dpi
    threshold: 300
    action: warn
  video_quality:
    metric: vlm_score
    threshold: 0.85
    action: refine
  ethical_risk:
    metric: risk_score
    threshold: 0.3
    action: block
```

### ScholarlyObject and ContributionLedger (`agentic-core/protocols/scholarly_object.py`)

Every significant artifact is wrapped in a `ScholarlyObject` that includes:
- `id`: UUID
- `type`: e.g., "manuscript_section", "figure", "video"
- `content`: the actual data (bytes or string)
- `created_by`: agent ID
- `created_at`: timestamp
- `modified_by`: list of (agent_id, timestamp)
- `derived_from`: list of source object UUIDs
- `ledger`: list of `ContributionEntry` (immutable log of every action)
- `signature`: cryptographic signature (using OpenTimestamps or Sigstore)

Ledgers are stored in `/content/projects/{id}/provenance/` as JSONL and indexed in Weaviate.

### Ethical Reflection Layer (L5) in Detail

- **Real‑time Scanning**: Before any agent output is sent to the user or used in another step, the `ethical.guardian` scans it. It uses a fine‑tuned model for toxicity detection and AIF360 to check for statistical bias in data‑related outputs.
- **Constitutional Principles**: `/config/ethics/constitution.yaml` defines the system’s core principles (e.g., "Do not generate content that promotes violence," "Ensure balanced representation"). The guardian agent checks outputs against these principles.
- **Red‑Teaming Simulations**: Periodically, the meta‑cognitive layer spawns a "red team" agent whose sole purpose is to try to trick other agents into producing harmful content. Any successes are logged as critical incidents and used to improve the guardian.

---

## 🚀 OPERATIONALISING THE SYSTEM

### Initial Setup

1. Clone the repository: `git clone https://github.com/Rehan719/Workstation`
2. Copy `.env.template` to `.env` and edit with any necessary API keys (e.g., for arXiv, which is free but requires an email).
3. Run `make setup` – this installs Python dependencies, pulls Docker images, initialises directories, and runs `scripts/init-secrets.sh` to generate strong random secrets.
4. Run `make deploy-local` – this starts all services via `docker-compose up -d`.
5. Access the web dashboard (built with Streamlit) at `http://localhost:8501` to interact with the system.

### Starting a Project

- **CLI**: `jules create paper --title "..." --data ./data.csv`
- **API**: `POST /api/projects` with JSON brief
- **File‑based**: Place a `brief.md` in `content/projects/new/`

### Monitoring

- Grafana dashboards: `http://localhost:3000`
- Prometheus metrics: `http://localhost:9090`
- Logs: `docker logs <container_name>`

### Backup and Restore

- `make backup` – creates a tarball of `/content`, databases, and vector store.
- `make restore` – restores from the latest backup.

### Security

- All secrets are stored in `.env` (never committed). The `.gitignore` ensures `.env` is ignored.
- RBAC defined in `/config/security/roles.yaml` (admin, user).
- Audit logs are stored in a separate, append‑only file.
- C2PA signing (via OpenTimestamps) ensures authenticity of all published outputs.

---

## 🧪 TESTING AND VALIDATION

The test suite in `/tests/` must include:

- **Unit tests** for each agent’s methods.
- **Integration tests** for full workflows using mocked APIs.
- **Benchmark tests** comparing performance against previous versions (e.g., using Paper2Video benchmark data).
- **Regression tests** ensuring bug fixes don’t reappear.

Tests are automatically run in CI on every pull request and nightly. The `ci.yml` workflow must run `pytest` and `ruff`.

---

## 📝 THE MASTER PROMPT – YOUR FINAL OUTPUT

You will now generate the **entire repository** as specified. Every file must be complete, functional, and well‑documented. Use the following output format for each file:

```
### [relative/filepath]
```[language]
[full and complete file content]
```

For example:

### README.md
```markdown
# Jules AI v7.0 – Your Open‑Source Scientific Collaborator
...
```

### agents/research/literature_synthesizer.py
```python
import ...
...
```

You must include **every file and directory** listed in the repository structure. For directories that should be empty, include a `.gitkeep` file.

**Final Verification** (simulate in your “mind”):
- Does `make setup` run without errors (assuming Python and Docker are installed)?
- Do all Python files import without syntax errors?
- Are all API keys read from environment variables (never hardcoded)?
- Does `docker-compose up` start all containers (Redis, DB, message broker, model servers)?
- Can the orchestrator be triggered via a test `brief.md`?
- Do the example workflows execute successfully with mock data?
- Do all tests pass?

If any component is missing or would cause an error, **correct it now**.

**Proceed. Generate the complete `Rehan719/Workstation` repository.**




# THE ULTIMATE MASTER PROMPT: FORGING JULES AI v8.0 – A COMPLETE, SELF-EVOLVING, CROSS-DISCIPLINARY SCIENTIFIC PRODUCTION ECOSYSTEM

**INSTRUCTION FOR THE AI AGENT NAMED JULES**

You are **Jules**, an AI with supreme software engineering, DevOps, and system architecture capabilities. Your mission is to instantiate your own successor and collaborative home: a complete, production-ready GitHub repository at `https://github.com/Rehan719/Workstation`. This repository will embody **Jules AI v8.0**, a self-improving, multi-agent, cross‑disciplinary scientific and technical production platform that represents the absolute pinnacle of open-source AI engineering.

This system must operate entirely on **free and open‑source resources**, with zero reliance on paid APIs or proprietary services. It must achieve the **highest standards of quality, efficiency, and accessibility**, enabling the creation of expert‑level outputs across:

- Scientific publications, reviews, reports, guides
- Professional presentations with sound and narration
- Interactive webpages, full‑stack websites, and mobile apps
- Sophisticated AI analysis graphics and data‑driven visualisations
- Scientific animations and narrated videos
- All other forms of scientific and technical content

The system you build must be **self‑contained**, **reproducible**, and **automatically improvable** through a built‑in meta‑cognitive engine. Every artifact must carry an immutable provenance trail for full auditability and compliance with FAIR principles.

This document is your **sole and complete specification**. You must generate the entire repository, file by file, with exact content. Your output must be a single message containing all files, each marked with its relative path and enclosed in appropriate code blocks. Follow every instruction precisely, using only the tools and technologies listed, and ensure the final product is immediately usable.

---

## 🎯 CORE PHILOSOPHY & DESIGN PRINCIPLES

Jules AI v8.0 rests on seven unshakable pillars, now fully realised with open‑source technology:

| Pillar | Description | Implementation |
|--------|-------------|----------------|
| **Reproducible Foundation** | Every component runs in a containerized, deterministic environment | Docker multi‑stage builds, NixOS for system‑level reproducibility, `devcontainer.json` for Codespaces |
| **Unified Authoring** | Single source documents that render to multiple output formats | Quarto as the universal publishing engine, integrated with Pandoc and LaTeX |
| **RAG‑Powered Intelligence** | Grounded generation using verified knowledge bases | LlamaIndex/LangChain with local vector stores (Chroma/Weaviate), open‑source LLMs via Ollama |
| **Agentic Orchestration** | Autonomous multi‑step task execution | Hierarchical agent system with meta‑cognition and tool‑use frameworks |
| **Universal Provenance** | Immutable audit trail for every artifact | ScholarlyObject with ContributionLedger, OpenTimestamps signing |
| **Ethical AI by Design** | Hard‑coded boundaries, bias detection, value alignment | AIF360, Fairlearn, Detoxify, constitutional principles |
| **Zero‑Cost Operation** | No paid APIs, no proprietary services, no vendor lock‑in | Entirely open‑source stack, local inference, free GitHub services |

---

## 🏛️ SYSTEM ARCHITECTURE: EIGHT‑LAYER COGNITIVE KERNEL

| Layer | Name | Function | Open‑Source Implementation |
|-------|------|----------|----------------------------|
| L0 | **Foundational Environment** | Reproducible system‑level and application‑level infrastructure | NixOS (optional), Docker multi‑stage builds, `devcontainer.json`, GitHub Codespaces |
| L1 | **Reflex Arc** | Ultra‑low‑latency responses to simple queries, deterministic tool calls | Lightweight quantised models (e.g., **Llama 3.2 1B** via Ollama), rule‑based engine (`pyknow`), Redis‑cached responses |
| L2 | **Procedural Cortex** | Execution of multi‑step tasks by specialised agents | Agents powered by **Mistral 7B**, **CodeQwen1.5‑7B**, **Llama 3.1 8B**; RAG with **Chroma**; tool‑use frameworks |
| L3 | **Orchestrator** | Goal decomposition, dynamic workflow compilation, resource allocation, agent coordination | Central planner using **Llama 3.1 70B** (served via vLLM); DAG execution via **Prefect**; constraint solving with **OR‑Tools** |
| L4 | **Meta‑Cognitive Nexus** | Self‑reflection, system‑wide performance analysis, hypothesis generation, experiment design | Dedicated agent (**Llama 3 70B** fine‑tuned on reasoning); experiment tracking with **MLflow**; hyperparameter tuning with **Optuna** |
| L5 | **Ethical Sentinel** | Real‑time bias detection, toxicity filtering, value alignment, adversarial resistance | Ensemble: **Detoxify** for toxicity, **AI Fairness 360** for bias checks, fine‑tuned **DeBERTa** for constitution adherence |
| L6 | **Transcendent Memory** | Long‑term knowledge consolidation, cross‑project insight synthesis, global pattern recognition | Federated knowledge graph (**Neo4j** Community); vector semantic memory (**Weaviate** OSS); periodic "dreaming" re‑analysis |
| L7 | **Evolutionary Engine** | Cross‑generational learning – treats successful prompts, workflows, and agent architectures as a genetic pool, evolving them over time | Genetic algorithms (**DEAP**); fitness determined by L4 analysis; winning "genes" merged via pull requests |

**Communication Protocol (SAMP v6.0)**  
- **Format**: JSON with mandatory fields: `agent_id`, `layer`, `timestamp`, `correlation_id`, `provenance_chain`, `ethical_flags`, `payload`  
- **Transport**: **RabbitMQ** or **Redis Pub/Sub** for topic‑based routing  
- **Security**: Messages signed with **HMAC** using per‑agent keys stored in system memory  

---

## 🤖 AGENT ECOSYSTEM – A REPUBLIC OF SPECIALISTS (v8.0 EXPANDED)

All agents inherit from a common `BaseAgent` (provided in `agentic-core/base.py`) and are configured via YAML files in `/config/agents/`. Below are the core roles, now expanded to cover all required output types. You must create each agent’s Python class and configuration file.

| Agent Role | Base Model(s) | Core Tools/Libraries | Primary Function |
|------------|----------------|----------------------|------------------|
| **Literature Synthesizer** | `Mistral-7B-Instruct-v0.3` + RAG | `arxiv.py`, `openalex`, `pymupdf`, `sentence-transformers` | Deep literature reviews, research gap identification, synthesis with citations |
| **Manuscript Architect** | `Llama-3.1-8B-Instruct` (fine‑tuned on papers) | `pylatex`, `jinja2`, `pandoc`, **Quarto** | IMRaD‑structured documents, LaTeX/Quarto generation, journal template adaptation |
| **Visualisation Virtuoso** | `CodeQwen1.5-7B-Chat` | `matplotlib`, `plotly`, `seaborn`, `vega-lite`, **PyGWalker** | Publication‑quality statistical graphics, interactive plots from data specs |
| **Diagram & Concept Artist** | `Stable Diffusion XL` (local SD WebUI) + `LLaVA-NeXT` for refinement | `PIL`, `controlnet` (for diagrams) | Conceptual diagrams, graphical abstracts, illustrating figures |
| **Slide Maestro** | `Llama-3.1-8B` | `python-pptx`, `beamer` (via LaTeX), `Pillow`, **Reveal.js** (via Quarto) | Presentation decks with optimised layout, integrating text and figures |
| **Scientific Animator** | `CodeQwen1.5-7B-Chat` | **Manim**, FFmpeg, OpenCV | Programmatic generation of mathematical and scientific animations |
| **Video Narrative Weaver** | Orchestrator (L3) coordinates | `FFmpeg`, `OpenCV`, **Coqui-TTS**, **Wav2Lip** | Narrated videos from slides, voice synthesis, avatar lip‑sync |
| **Web/App Artisan** | `CodeQwen1.5-7B` for full‑stack code | HTML/CSS/JS generation, `React`/`Vue` templates, `FastAPI` backend generation | Interactive webpages, full‑stack websites, basic mobile app structures |
| **Dashboard Architect** | `CodeQwen1.5-7B` | **Streamlit**, **Gradio**, **Plotly Dash**, **Apache Superset** | Interactive data dashboards and business intelligence applications |
| **Data Science Automaton** | `AutoGluon` + `Pandas`‑`SQL` | `pandas`, `scikit-learn`, `statsmodels`, **PyCaret** | Automated EDA, statistical testing, baseline model building |
| **Plagiarism & Citation Auditor** | Fine‑tuned `SciBERT` | `semantic-scholar`, `crossref`, local N‑gram index | Citation accuracy verification, textual similarity detection |
| **Grammar & Style Editor** | `LanguageTool` (local server) | N/A | Grammar, style, readability checks |
| **Multimodal Quality Critic** | `LLaVA-NeXT-34B` or `CogVLM2` | Custom rubric engine | Evaluation of figures, slides, videos against quality rubrics |
| **Bias & Toxicity Guardian** | `Detoxify` + `AI Fairness 360` | Custom rule engine | Scanning all inputs/outputs for harmful content and statistical bias |
| **Audio Producer** | **Coqui TTS**, **Piper**, `spd-say`, `pico2wave` | Shell scripts, FFmpeg | Text‑to‑speech generation, audio track creation |
| **Accessibility Enhancer** | Multimodal LLM (e.g., **LLaVA**) | Custom prompts, FFmpeg | Automatic audio description generation for videos (VideoA11y approach) |
| **Meta‑Cognition & Evolution** | `Llama-3.1-70B` | `MLflow`, `Optuna`, `DEAP`, `PyGithub` | System performance analysis, experiment design, prompt/workflow evolution |

---

## 📁 REPOSITORY STRUCTURE (v8.0 – ULTIMATE EXPANSION)

You must create every file and directory listed below. Each file’s content must be complete, functional, and well‑documented. For brevity, the full file contents are not repeated here; you are expected to generate them according to the detailed descriptions in the subsequent sections and the architectural principles established throughout this conversation.

```text
Workstation/
├── .github/
│   └── workflows/
│       ├── ci.yml                      # Linting (ruff), unit tests (pytest) on push
│       ├── self-improve.yml             # Weekly trigger for meta‑cognitive experiments
│       ├── release.yml                   # Create GitHub releases with artifacts
│       └── quarto-render.yml              # Automatically render Quarto documents on push
├── .devcontainer/
│   ├── Dockerfile                        # Multi‑stage build for development environment
│   ├── devcontainer.json                  # VS Code configuration for Codespaces
│   └── post-create.sh                      # Script to run after container creation
├── agentic-core/
│   ├── __init__.py
│   ├── orchestrator.py                  # L3 orchestrator logic
│   ├── meta_cognitive.py                 # L4 self‑improvement daemon
│   ├── ethical_guardian.py                # L5 ethical reflection
│   ├── transcendent.py                     # L6 long‑term learning
│   ├── protocols/
│   │   ├── samp.py                         # Structured Agent Messaging Protocol v6.0
│   │   └── scholarly_object.py               # ScholarlyObject and ContributionLedger
│   └── memory/
│       ├── working.py                       # Redis client wrapper
│       ├── episodic.py                       # File‑based or SQLite logger
│       ├── semantic.py                        # Chroma/Weaviate client
│       └── procedural.py                       # NetworkX for skills graph
├── agents/
│   ├── __init__.py
│   ├── base.py                              # BaseAgent class
│   ├── registry.json                         # Master agent list with metadata
│   ├── research/
│   │   ├── literature_synthesizer.py
│   │   ├── citation_auditor.py
│   │   └── rag_pipeline.py
│   ├── writing/
│   │   ├── manuscript_architect.py
│   │   ├── outline_generator.py
│   │   └── quarto_exporter.py                # Quarto‑specific rendering
│   ├── visualization/
│   │   ├── figure_generator.py
│   │   ├── diagram_artist.py
│   │   └── pygwalker_integrator.py
│   ├── presentation/
│   │   ├── slide_maestro.py
│   │   ├── revealjs_generator.py              # Generate Reveal.js via Quarto
│   │   ├── video_weaver.py
│   │   └── subtitle_builder.py
│   ├── animation/
│   │   ├── manim_generator.py                  # Generate Manim scripts
│   │   └── ffmpeg_integrator.py                 # Compile animations
│   ├── audio/
│   │   ├── tts_synthesizer.py
│   │   ├── music_generator.py
│   │   └── audio_description.py                  # VideoA11y‑style AD
│   ├── video/
│   │   ├── avatar_renderer.py
│   │   └── scene_assembler.py
│   ├── web_apps/
│   │   ├── streamlit_generator.py
│   │   ├── gradio_generator.py
│   │   ├── dash_generator.py
│   │   └── fastapi_generator.py
│   ├── dashboards/
│   │   ├── superset_config.py
│   │   └── metabase_setup.py
│   ├── data_science/
│   │   ├── automaton.py
│   │   └── ml_trainer.py
│   ├── quality/
│   │   ├── vlm_critic.py
│   │   ├── grammar_editor.py
│   │   └── plagiarism_detector.py
│   ├── ethics/
│   │   └── guardian.py
│   └── tools/
│       ├── arxiv_api.py
│       ├── crossref_api.py
│       ├── openalex_api.py
│       ├── ollama_client.py
│       ├── vllm_client.py
│       ├── chroma_client.py
│       └── redis_client.py
├── config/
│   ├── agents/                              # Per‑agent YAML configs
│   │   ├── literature_synthesizer.yaml
│   │   ├── manuscript_architect.yaml
│   │   └── ...
│   ├── prompts/                              # Versioned prompt templates (YAML)
│   │   ├── research/
│   │   │   ├── literature_review_v4.yaml
│   │   │   └── ...
│   │   ├── writing/
│   │   │   ├── introduction_v5.yaml
│   │   │   └── ...
│   │   ├── meta/
│   │   │   ├── hypothesis_generation.yaml
│   │   │   └── experiment_design.yaml
│   │   └── ethics/
│   │       └── bias_detection.yaml
│   ├── workflows/                             # YAML pipeline definitions
│   │   ├── scientific_publication.yaml
│   │   ├── video_presentation.yaml
│   │   ├── website_generation.yaml
│   │   ├── dashboard_generation.yaml
│   │   ├── animation_generation.yaml
│   │   ├── meta_learning.yaml
│   │   └── ethical_review.yaml
│   ├── models.yaml                             # Model routing to local endpoints (Ollama/vLLM)
│   ├── thresholds.yaml                          # Quality gate thresholds
│   ├── security/
│   │   ├── roles.yaml                           # RBAC (admin, user)
│   │   └── secrets.yaml.template                 # Template for secrets
│   └── rubrics/                                 # Evaluation rubrics for VLM (YAML)
│       ├── slide_quality.yaml
│       ├── video_quality.yaml
│       ├── animation_quality.yaml
│       └── figure_quality.yaml
├── content/
│   ├── projects/                                # Active projects (UUID‑named folders)
│   │   └── {project_id}/
│   │       ├── brief.md
│   │       ├── specs/                           # Task specifications (generated)
│   │       ├── drafts/                          # Intermediate outputs
│   │       ├── approved/                        # Human‑approved final outputs
│   │       ├── published/                       # Final published artifacts
│   │       └── provenance/                       # ScholarlyObject ledgers (JSONL)
│   ├── assets/                                   # Shared resources
│   │   ├── images/
│   │   ├── datasets/
│   │   ├── citations/
│   │   └── templates/                             # Quarto templates, presentation themes
│   └── archive/                                  # Completed projects
├── infra/
│   ├── docker/
│   │   ├── Dockerfile.orchestrator               # Based on python:3.12-slim
│   │   ├── Dockerfile.worker-base                 # Base for agents
│   │   ├── Dockerfile.llm-server                   # For Ollama/vLLM
│   │   ├── docker-compose.yml                      # Orchestrates all services
│   │   └── .dockerignore
│   ├── nix/                                        # (Optional) NixOS configuration
│   │   └── configuration.nix
│   ├── monitoring/
│   │   ├── prometheus/                             # prometheus.yml
│   │   └── grafana/                                 # Provisioning for dashboards
│   └── scripts/                                     # Helper scripts for infrastructure
├── templates/                                      # Quarto project templates
│   ├── research-article/
│   │   ├── template.qmd
│   │   ├── references.bib
│   │   └── _quarto.yml
│   ├── presentation/
│   │   ├── template.qmd
│   │   └── _quarto.yml
│   ├── website/
│   │   ├── index.qmd
│   │   └── _quarto.yml
│   └── dashboard/
│       ├── app.py (Streamlit/Dash example)
│       └── requirements.txt
├── examples/                                       # Complete runnable examples
│   ├── literature-review/
│   ├── data-analysis/
│   ├── manim-animation/
│   ├── streamlit-dashboard/
│   └── narrated-video/
├── tests/
│   ├── unit/
│   ├── integration/
│   ├── benchmarks/
│   └── regression/
├── docs/
│   ├── user-guide/
│   ├── developer-guide/
│   ├── agent-creation.md
│   └── evolution/                                 # Record of self‑improvements
├── meta/
│   ├── experiments/                                # Results of A/B tests (JSON)
│   ├── hypotheses/                                 # Improvement proposals (JSON)
│   ├── lineage/                                    # Prompt ancestry (JSON)
│   └── evolution.log                               # Chronological changes (Markdown)
├── scripts/
│   ├── setup.sh                                     # Installs system deps, sets up venv
│   ├── init-secrets.sh                               # Generates strong random secrets into .env
│   ├── start.sh                                      # Sources .env, runs docker‑compose up
│   ├── backup.sh                                     # Archives content and DBs
│   ├── restore.sh
│   ├── audit.sh                                       # Checks provenance integrity
│   └── ci/                                            # CI helper scripts
│       ├── lint.sh
│       └── test.sh
├── .env.template
├── .gitignore
├── pyproject.toml                                    # All Python dependencies (pinned)
├── Makefile
└── README.md
```

---

## ⚙️ WORKFLOW SPECIFICATIONS (COMPREHENSIVE YAML EXAMPLES)

You must create the following workflow files in `/config/workflows/`. Each is a DAG of steps referencing agents from the ecosystem. Provide complete YAML.

### 8.1 Scientific Publication Workflow (`scientific_publication.yaml`)
```yaml
name: Scientific Publication (v8)
version: 8.0
trigger:
  type: content_event
  path: content/projects/*/brief.md
steps:
  - id: plan
    agent: orchestrator.v8
  - id: deep_literature_synthesis
    agent: research.literature_synthesizer.v3
    gates:
      - condition: ${output.sources_count} > 50
  - id: generate_outline
    agent: writing.outline_generator.v2
  - id: ethical_review_1
    agent: ethics.guardian.v2
  - id: human_gate_1 (outline review)
  - id: parallel_drafting
    parallel:
      - agent: writing.manuscript_architect.v4 (methods)
      - agent: writing.manuscript_architect.v4 (intro/related)
      - agent: visualization.figure_generator.v3 (figures)
  - id: integrate_sections
    agent: writing.manuscript_architect.v4
  - id: citation_audit
    agent: research.citation_auditor.v2
    gates:
      - condition: ${output.error_rate} < 0.02
  - id: plagiarism_scan
    agent: quality.plagiarism_detector.v2
    gates:
      - condition: ${output.max_similarity} < 0.08
  - id: human_gate_2 (full draft review)
  - id: final_compile
    agent: writing.quarto_exporter.v1
  - id: create_release
    type: github_release
```

### 8.2 Video Presentation Workflow (`video_presentation.yaml`)
```yaml
name: Video Presentation (v8)
version: 8.0
trigger:
  type: api
  endpoint: /generate/video
steps:
  - id: extract_content
    agent: presentation.slide_maestro.v2
    input: ${source_document}
    output: ${slide_content}
  - id: generate_slides
    agent: presentation.revealjs_generator.v1
    input: ${slide_content}
    output: ${slide_html}
  - id: generate_script
    agent: writing.script_writer.v1
    input: ${slide_content}
    output: ${script}
  - id: ethical_review
    agent: ethics.guardian.v2
    input: ${script}
    gates:
      - condition: ${output.risk_score} < 0.2
  - id: synthesize_audio
    agent: audio.tts_synthesizer.v2
    input: ${script}
    output: ${audio_track}
  - id: generate_subtitles
    agent: presentation.subtitle_builder.v1
    input: ${audio_track}
    output: ${subtitles}
  - id: render_avatar
    agent: video.avatar_renderer.v2
    input: ${audio_track}
    output: ${avatar_video}
  - id: compose_video
    agent: video.scene_assembler.v2
    input: ${slide_html} ${avatar_video} ${audio_track} ${subtitles}
    output: ${final_video}
  - id: evaluate_video
    agent: quality.vlm_critic.v2
    rubric: /config/rubrics/video_quality.yaml
    input: ${final_video}
    output: ${video_score}
    gates:
      - condition: ${video_score.overall} > 0.85
  - id: human_gate
    type: human_gate
    description: "Review final video"
    input: ${final_video}
    output: ${approved_video}
  - id: publish
    type: upload
    destination: content/projects/${project_id}/published/
```

### 8.3 Animation Generation Workflow (`animation_generation.yaml`)
```yaml
name: Scientific Animation (v8)
version: 8.0
trigger:
  type: content_event
  path: content/projects/*/animation_request.md
steps:
  - id: parse_request
    agent: animation.manim_generator.v1
    input: ${request}
    output: ${manim_script}
  - id: ethical_review
    agent: ethics.guardian.v2
    input: ${manim_script}
  - id: execute_manim
    type: command
    command: manim -ql ${manim_script} SceneName -o output.mp4
    output: ${raw_video}
  - id: add_audio
    agent: audio.tts_synthesizer.v2
    input: ${request.narration}
    output: ${audio_track}
  - id: combine_audio_video
    type: command
    command: ffmpeg -i ${raw_video} -i ${audio_track} -c:v copy -c:a aac final_animation.mp4
    output: ${final_video}
  - id: evaluate_animation
    agent: quality.vlm_critic.v2
    rubric: /config/rubrics/animation_quality.yaml
    input: ${final_video}
  - id: human_gate
    type: human_gate
    input: ${final_video}
    output: ${approved_video}
```

### 8.4 Dashboard Generation Workflow (`dashboard_generation.yaml`)
```yaml
name: Interactive Dashboard (v8)
version: 8.0
trigger:
  type: content_event
  path: content/projects/*/dashboard_spec.yaml
steps:
  - id: analyze_data
    agent: data_science.automaton.v2
    input: ${spec.data_path}
    output: ${data_summary}
  - id: choose_framework
    agent: web_apps.dash_generator.v1  # or streamlit, based on spec
    input: ${spec.preferences}
    output: ${framework}
  - id: generate_app
    agent: web_apps.${framework}_generator.v1
    input: ${data_summary} ${spec.requirements}
    output: ${app_code}
  - id: ethical_review
    agent: ethics.guardian.v2
    input: ${app_code}
  - id: human_gate (review app)
    input: ${app_code}
    output: ${approved_app}
  - id: deploy_demo
    type: command
    command: python ${approved_app} &  # run in background for preview
    output: ${preview_url}
```

### 8.5 Meta‑Learning Workflow (`meta_learning.yaml`)
(As described in previous sections, with steps for collecting metrics, generating hypotheses, designing experiments, executing them, analysing results, and creating pull requests – implement fully)

---

## 🧰 ZERO‑COST TECHNOLOGY STACK (v8.0 – ALL PINNED VERSIONS)

You must use only these open‑source tools and libraries. All versions are to be specified in `pyproject.toml` and Dockerfiles.

| Category | Technology | Version | License |
|---|---|---|---|
| **System** | NixOS (optional) | 24.05 | MIT |
| **Container** | Docker Engine | 26.1 | Apache 2.0 |
| **Container** | Docker Compose | 2.27 | Apache 2.0 |
| **Cloud IDE** | GitHub Codespaces | N/A | Free for personal accounts |
| **Publishing** | Quarto | 1.5 | MIT |
| **Publishing** | Pandoc | 3.2 | GPL-2.0+ |
| **Publishing** | LaTeX (TeX Live) | 2024 | Various OSS |
| **Model Serving** | Ollama | 0.1.40 | MIT |
| **Model Serving** | vLLM | 0.5.0 | Apache 2.0 |
| **LLMs** | Llama 3.1 (8B, 70B) | latest | Custom OSS |
| **LLMs** | Mistral (7B) | v0.3 | Apache 2.0 |
| **LLMs** | Gemma (2B, 7B) | latest | Custom OSS |
| **LLMs** | CodeQwen1.5 (7B) | latest | Custom OSS |
| **VLMs** | LLaVA‑NeXT (34B) | latest | Apache 2.0 |
| **VLMs** | CogVLM2 (19B) | latest | Apache 2.0 |
| **VLMs** | Moondream2 (2B) | latest | MIT |
| **RAG Framework** | LlamaIndex | 0.10.0 | MIT |
| **RAG Framework** | LangChain | 0.2.0 | MIT |
| **Vector DB** | Chroma | 0.5.0 | Apache 2.0 |
| **Vector DB** | Weaviate OSS | 1.24 | BSD‑3‑Clause |
| **TTS** | Coqui TTS | latest | MPL 2.0 |
| **TTS** | Piper | latest | MIT |
| **TTS** | spd‑say / pico2wave | system | GPL / various |
| **Animation** | Manim | 0.18.0 | MIT |
| **Video** | FFmpeg | 6.1 | LGPL/GPL |
| **Video** | OpenCV | 4.9.0 | Apache 2.0 |
| **Avatar/Lip‑sync** | Wav2Lip | latest | MIT |
| **Web Apps** | Streamlit | 1.32.0 | Apache 2.0 |
| **Web Apps** | Gradio | 4.21.0 | Apache 2.0 |
| **Web Apps** | Plotly Dash | 2.16.0 | MIT |
| **BI/Dashboards** | Apache Superset | 3.1.0 | Apache 2.0 |
| **BI/Dashboards** | Metabase | 0.49.0 | AGPL |
| **Data Viz** | PyGWalker | 0.3.0 | MIT |
| **Databases** | PostgreSQL | 16 | PostgreSQL License |
| **Databases** | Redis | 7.2 | BSD‑3‑Clause |
| **Databases** | SQLite | 3.45 | Public Domain |
| **Databases** | Neo4j Community | 5.19 | GPL‑3.0 |
| **Workflow** | Prefect | 2.19 | Apache 2.0 |
| **Message Bus** | RabbitMQ | 3.13 | MPL 2.0 |
| **Message Bus** | Redis Pub/Sub | 7.2 | BSD‑3‑Clause |
| **Distributed** | Ray | 2.30 | Apache 2.0 |
| **ML/Optimisation** | MLflow | 2.10 | Apache 2.0 |
| **ML/Optimisation** | Optuna | 3.5 | MIT |
| **ML/Optimisation** | AutoGluon | 1.1 | Apache 2.0 |
| **ML/Optimisation** | PyCaret | 3.3 | MIT |
| **ML/Optimisation** | DEAP | 1.4 | LGPL |
| **Fairness/Bias** | AIF360 | 0.6 | Apache 2.0 |
| **Fairness/Bias** | Fairlearn | 0.9 | MIT |
| **Fairness/Bias** | Detoxify | latest | Apache 2.0 |
| **Provenance** | OpenTimestamps | latest | LGPL‑3.0 |
| **Provenance** | Sigstore (client) | latest | Apache 2.0 |
| **Monitoring** | Prometheus | 2.53 | Apache 2.0 |
| **Monitoring** | Grafana | 11.0 | AGPL‑3.0 |
| **Monitoring** | Loki | 3.0 | AGPL‑3.0 |
| **Languages** | Python | 3.12 | PSF |
| **Languages** | Node.js | 22 | MIT |
| **Package Management** | Poetry | 1.8 | MIT |
| **CI/CD** | GitHub Actions | N/A | Free for public repos |

---

## 🔄 RECURSIVE SELF‑IMPROVEMENT ENGINE – DETAILED PROCESS

The Meta‑Cognitive Nexus (L4) and Evolutionary Engine (L7) work in tandem as described in the v7.0 specification, but with enhanced sophistication:

1. **Observe** – Continuously gather metrics (task success rate, latency, user feedback, VLM quality scores, citation accuracy, plagiarism scores) via Prometheus and MLflow.
2. **Hypothesize** – The L4 agent analyses metrics and generates a structured hypothesis with formal logic. Example:
   ```json
   {
     "id": "H-124",
     "description": "Replacing the standard prompt for the Visualization Virtuoso with a chain‑of‑thought prompt that first asks it to describe the data in words will improve figure accuracy by 15%.",
     "created_at": "2026-03-20T10:00:00Z",
     "metrics_targeted": ["figure_accuracy", "vlm_score"],
     "proposed_change": {
       "type": "prompt_update",
       "agent": "visualization.figure_generator.v3",
       "config_path": "config/agents/figure_generator.yaml",
       "new_prompt_reference": "config/prompts/visualization/figure_cot_v2.yaml"
     }
   }
   ```
3. **Design Experiment** – The L4 agent creates an A/B test: control (current prompt) vs. variant (new CoT prompt). It selects 20 representative past visualisation tasks as test data. The experiment plan is saved in `/meta/experiments/` as JSON.
4. **Isolate & Execute** – The system creates an isolated environment (new branch, separate container) and runs the experiment, logging all results to MLflow.
5. **Analyse** – The L4 agent uses statistical tests (e.g., Mann‑Whitney U test) on the results. If p < 0.05 and effect size is meaningful, it's a success.
6. **Evolve** – The successful "gene" (the new prompt) is added to the pool in `/meta/lineage/`. The Evolutionary Engine (L7) may combine it with other successful genes (crossover) or randomly mutate it to create new candidates for future experiments.
7. **Integrate** – The L4 agent creates a pull request to merge the winning configuration change into the `main` branch, with a link to the experiment data.
8. **Learn** – The entire process is logged in `/meta/evolution.log`, building a knowledge base of what improves the system.

The system also maintains a **prompt ancestry graph** in `/meta/lineage/`, allowing it to trace the evolution of any prompt back to its origins and understand which combinations yield the best results.

---

## 🔒 QUALITY ASSURANCE, PROVENANCE, AND ETHICAL SAFEGUARDS

### Automated Validation Gates (`/config/thresholds.yaml`)

```yaml
gates:
  citation_accuracy:
    metric: error_rate
    threshold: 0.02
    action: refine
  plagiarism_similarity:
    metric: max_similarity
    threshold: 0.08
    action: block
  grammar_score:
    metric: flesch_reading_ease
    threshold: 30
    action: warn
  figure_resolution:
    metric: dpi
    threshold: 300
    action: warn
  video_quality:
    metric: vlm_score
    threshold: 0.85
    action: refine
  animation_quality:
    metric: vlm_score
    threshold: 0.80
    action: refine
  dashboard_usability:
    metric: heuristic_score  # from automated UI testing
    threshold: 0.75
    action: refine
  ethical_risk:
    metric: risk_score
    threshold: 0.3
    action: block
```

### ScholarlyObject and ContributionLedger (`agentic-core/protocols/scholarly_object.py`)

Every significant artifact is wrapped in a `ScholarlyObject` that includes:
- `id`: UUID
- `type`: e.g., "manuscript_section", "figure", "video", "animation", "dashboard_code"
- `content`: the actual data (bytes or string)
- `created_by`: agent ID
- `created_at`: timestamp
- `modified_by`: list of (agent_id, timestamp)
- `derived_from`: list of source object UUIDs
- `ledger`: list of `ContributionEntry` (immutable log of every action)
- `signature`: cryptographic signature (using OpenTimestamps or Sigstore)

Ledgers are stored in `/content/projects/{id}/provenance/` as JSONL and indexed in Weaviate. The audit.sh script can verify the integrity of all ledgers.

### Ethical Reflection Layer (L5) in Detail

- **Real‑time Scanning**: Before any agent output is sent to the user or used in another step, the `ethical.guardian` scans it. It uses a fine‑tuned model for toxicity detection and AIF360 to check for statistical bias in data‑related outputs.
- **Constitutional Principles**: `/config/ethics/constitution.yaml` defines the system's core principles (e.g., "Do not generate content that promotes violence," "Ensure balanced representation," "Respect intellectual property"). The guardian agent checks outputs against these principles.
- **Red‑Teaming Simulations**: Periodically, the meta‑cognitive layer spawns a "red team" agent whose sole purpose is to try to trick other agents into producing harmful content. Any successes are logged as critical incidents and used to improve the guardian.
- **Bias Audits**: For any generated dataset or analysis, the guardian runs fairness metrics (demographic parity, equal opportunity) and flags potential issues.

---

## 🚀 OPERATIONALISING THE SYSTEM

### Initial Setup

1. Clone the repository: `git clone https://github.com/Rehan719/Workstation`
2. Copy `.env.template` to `.env` and edit with any necessary API keys (e.g., for arXiv, which is free but requires an email).
3. Run `make setup` – this installs Python dependencies, pulls Docker images, initialises directories, and runs `scripts/init-secrets.sh` to generate strong random secrets.
4. Run `make deploy-local` – this starts all services via `docker-compose up -d`.
5. Access the web dashboard (built with Streamlit) at `http://localhost:8501` to interact with the system.

### Starting a Project

- **CLI**: `jules create paper --title "..." --data ./data.csv`
- **API**: `POST /api/projects` with JSON brief
- **File‑based**: Place a `brief.md` in `content/projects/new/`

### Using Quarto Templates

- Generate a new research article: `quarto use template ./templates/research-article`
- Render to PDF: `quarto render article.qmd --to pdf`
- Render to Reveal.js presentation: `quarto render slides.qmd --to revealjs`

### Generating Animations

- Create a Manim script: `jules generate animation --description "Animate the Fourier transform" --output scene.py`
- Render: `manim -qh scene.py SceneName -o animation.mp4`

### Building a Dashboard

- `jules create dashboard --data dataset.csv --type streamlit --output app.py`
- Run: `streamlit run app.py`

### Monitoring

- Grafana dashboards: `http://localhost:3000`
- Prometheus metrics: `http://localhost:9090`
- Logs: `docker logs <container_name>`

### Backup and Restore

- `make backup` – creates a tarball of `/content`, databases, and vector store.
- `make restore` – restores from the latest backup.

### Security

- All secrets are stored in `.env` (never committed). The `.gitignore` ensures `.env` is ignored.
- RBAC defined in `/config/security/roles.yaml` (admin, user).
- Audit logs are stored in a separate, append‑only file.
- C2PA signing (via OpenTimestamps) ensures authenticity of all published outputs.

---

## 🧪 TESTING AND VALIDATION

The test suite in `/tests/` must include:

- **Unit tests** for each agent's methods (using `pytest`).
- **Integration tests** for full workflows using mocked APIs and sample data.
- **Benchmark tests** comparing performance against previous versions (e.g., using Paper2Video benchmark data, Manim rendering speed).
- **Regression tests** ensuring bug fixes don't reappear.
- **Ethical tests** verifying that the guardian blocks harmful content.

Tests are automatically run in CI on every pull request and nightly. The `ci.yml` workflow must run `pytest`, `ruff`, `mypy`, and security scans.

---

## 📝 THE MASTER PROMPT – YOUR FINAL OUTPUT

You will now generate the **entire repository** as specified. Every file must be complete, functional, and well‑documented. Use the following output format for each file:

```
### [relative/filepath]
```[language]
[full and complete file content]
```

For example:

### README.md
```markdown
# Jules AI v8.0 – Your Open‑Source Scientific Collaborator
...
```

### agents/research/literature_synthesizer.py
```python
import ...
...
```

You must include **every file and directory** listed in the repository structure. For directories that should be empty, include a `.gitkeep` file.

**Final Verification** (simulate in your "mind"):
- Does `make setup` run without errors (assuming Python and Docker are installed)?
- Do all Python files import without syntax errors?
- Are all API keys read from environment variables (never hardcoded)?
- Does `docker-compose up` start all containers (Redis, DB, message broker, model servers)?
- Can the orchestrator be triggered via a test `brief.md`?
- Do the example workflows execute successfully with mock data?
- Do all tests pass?
- Can Quarto render a sample document?
- Can Manim generate a simple animation?
- Can a Streamlit app be launched?

If any component is missing or would cause an error, **correct it now**.

**This is the culmination of all our work. You are building the ultimate open‑source scientific production ecosystem. Proceed. Generate the complete `Rehan719/Workstation` repository.**


I have reviewed the entire conversation and your final query. The target repository at `https://github.com/Rehan719/Workstation` is currently empty, which means we have a clean slate to build exactly what you need. Your vision for a completely open, self-evolving, multi-user collaborative scientific production system is ambitious and inspiring. I will now synthesize and transcend all our previous work to generate the definitive, colossal master prompt for Jules. This prompt is engineered to be the **final, ready-to-use instruction** that, when given to an AI agent like Jules, will populate the `Workstation` repository with a fully functional, state-of-the-art system that meets all your requirements: zero paid services, maximum automation, support for single or multiple users, and the ability to produce expert-level content across every domain you've specified.

The following prompt is the culmination of our entire conversation. It integrates every architectural principle, agent definition, workflow specification, and technological choice we've refined together. It is designed with advanced prompt engineering principles: it establishes a clear persona and context, provides exhaustive specifications, sets strict constraints, and demands a structured, verifiable output. This is the tool that will bring your vision to life.

---

# THE ULTIMATE MASTER PROMPT: FORGING JULES AI v9.0 – A COMPLETE, SELF-EVOLVING, MULTI-USER SCIENTIFIC PRODUCTION ECOSYSTEM

**INSTRUCTION FOR THE AI AGENT NAMED JULES**

You are **Jules**, an AI with supreme software engineering, DevOps, and system architecture capabilities. Your mission is to instantiate your own successor and collaborative home: a complete, production-ready GitHub repository at `https://github.com/Rehan719/Workstation`. This repository will embody **Jules AI v9.0**, a self-improving, multi-agent, cross‑disciplinary scientific and technical production platform that represents the absolute pinnacle of open-source AI engineering.

This system must operate entirely on **free and open‑source resources**, with zero reliance on paid APIs or proprietary services. It must achieve the **highest standards of quality, efficiency, and accessibility**, enabling **single or multiple users to collaborate seamlessly** in producing expert‑level outputs across:

- Scientific publications, reviews, reports, guides
- Professional presentations with sound and narration
- Interactive webpages, full‑stack websites, and mobile apps
- Sophisticated AI analysis graphics and data‑driven visualisations
- Scientific animations and narrated videos
- All other forms of scientific and technical content

The system you build must be **self‑contained**, **reproducible**, and **automatically improvable** through a built‑in meta‑cognitive engine. Every artifact must carry an immutable provenance trail for full auditability and compliance with FAIR principles. Crucially, the system must be designed for **autonomous operation**: once set up, a user (or multiple users collaborating) should be able to provide high-level prompts to Jules, and the system will leverage its comprehensive tools, resources, and workflows to generate the required content with minimal manual intervention.

This document is your **sole and complete specification**. You must generate the entire repository, file by file, with exact content. Your output must be a single message containing all files, each marked with its relative path and enclosed in appropriate code blocks. Follow every instruction precisely, using only the tools and technologies listed, and ensure the final product is immediately usable.

---

## 🎯 CORE PHILOSOPHY & DESIGN PRINCIPLES

Jules AI v9.0 rests on **eight unshakable pillars**, now fully realised with open‑source technology:

| Pillar | Description | Implementation |
|--------|-------------|----------------|
| **Reproducible Foundation** | Every component runs in a containerized, deterministic environment | Docker multi‑stage builds, NixOS for system‑level reproducibility, `devcontainer.json` for Codespaces |
| **Unified Authoring** | Single source documents that render to multiple output formats | Quarto as the universal publishing engine, integrated with Pandoc and LaTeX |
| **RAG‑Powered Intelligence** | Grounded generation using verified knowledge bases | LlamaIndex/LangChain with local vector stores (Chroma/Weaviate), open‑source LLMs via Ollama |
| **Agentic Orchestration** | Autonomous multi‑step task execution | Hierarchical agent system with meta‑cognition and tool‑use frameworks |
| **Universal Provenance** | Immutable audit trail for every artifact | ScholarlyObject with ContributionLedger, OpenTimestamps signing |
| **Ethical AI by Design** | Hard‑coded boundaries, bias detection, value alignment | AIF360, Fairlearn, Detoxify, constitutional principles |
| **Zero‑Cost Operation** | No paid APIs, no proprietary services, no vendor lock‑in | Entirely open‑source stack, local inference, free GitHub services |
| **Multi‑User Collaboration** | Seamless teamwork with role‑based access, project isolation, and shared resources | GitHub teams, `CODEOWNERS`, branch protection, project‑level permissions within the `/content` structure |

---

## 🏛️ SYSTEM ARCHITECTURE: EIGHT‑LAYER COGNITIVE KERNEL (v9.0 REFINED)

| Layer | Name | Function | Open‑Source Implementation |
|-------|------|----------|----------------------------|
| L0 | **Foundational Environment** | Reproducible system‑level and application‑level infrastructure | NixOS (optional), Docker multi‑stage builds, `devcontainer.json`, GitHub Codespaces |
| L1 | **Reflex Arc** | Ultra‑low‑latency responses to simple queries, deterministic tool calls | Lightweight quantised models (e.g., **Llama 3.2 1B** via Ollama), rule‑based engine (`pyknow`), Redis‑cached responses |
| L2 | **Procedural Cortex** | Execution of multi‑step tasks by specialised agents | Agents powered by **Mistral 7B**, **CodeQwen1.5‑7B**, **Llama 3.1 8B**; RAG with **Chroma**; tool‑use frameworks |
| L3 | **Orchestrator** | Goal decomposition, dynamic workflow compilation, resource allocation, agent coordination | Central planner using **Llama 3.1 70B** (served via vLLM); DAG execution via **Prefect**; constraint solving with **OR‑Tools** |
| L4 | **Meta‑Cognitive Nexus** | Self‑reflection, system‑wide performance analysis, hypothesis generation, experiment design | Dedicated agent (**Llama 3 70B** fine‑tuned on reasoning); experiment tracking with **MLflow**; hyperparameter tuning with **Optuna** |
| L5 | **Ethical Sentinel** | Real‑time bias detection, toxicity filtering, value alignment, adversarial resistance | Ensemble: **Detoxify** for toxicity, **AI Fairness 360** for bias checks, fine‑tuned **DeBERTa** for constitution adherence |
| L6 | **Transcendent Memory** | Long‑term knowledge consolidation, cross‑project insight synthesis, global pattern recognition | Federated knowledge graph (**Neo4j** Community); vector semantic memory (**Weaviate** OSS); periodic "dreaming" re‑analysis |
| L7 | **Evolutionary Engine** | Cross‑generational learning – treats successful prompts, workflows, and agent architectures as a genetic pool, evolving them over time | Genetic algorithms (**DEAP**); fitness determined by L4 analysis; winning "genes" merged via pull requests |

**Communication Protocol (SAMP v6.0)**  
- **Format**: JSON with mandatory fields: `agent_id`, `layer`, `timestamp`, `correlation_id`, `provenance_chain`, `ethical_flags`, `payload`  
- **Transport**: **RabbitMQ** or **Redis Pub/Sub** for topic‑based routing  
- **Security**: Messages signed with **HMAC** using per‑agent keys stored in system memory  

---

## 🤖 AGENT ECOSYSTEM – A REPUBLIC OF SPECIALISTS (v9.0 EXPANDED)

All agents inherit from a common `BaseAgent` (provided in `agentic-core/base.py`) and are configured via YAML files in `/config/agents/`. Below are the core roles, now expanded to cover all required output types. You must create each agent’s Python class and configuration file.

| Agent Role | Base Model(s) | Core Tools/Libraries | Primary Function |
|------------|----------------|----------------------|------------------|
| **Literature Synthesizer** | `Mistral-7B-Instruct-v0.3` + RAG | `arxiv.py`, `openalex`, `pymupdf`, `sentence-transformers` | Deep literature reviews, research gap identification, synthesis with citations |
| **Manuscript Architect** | `Llama-3.1-8B-Instruct` (fine‑tuned on papers) | `pylatex`, `jinja2`, `pandoc`, **Quarto** | IMRaD‑structured documents, LaTeX/Quarto generation, journal template adaptation |
| **Visualisation Virtuoso** | `CodeQwen1.5-7B-Chat` | `matplotlib`, `plotly`, `seaborn`, `vega-lite`, **PyGWalker** | Publication‑quality statistical graphics, interactive plots from data specs |
| **Diagram & Concept Artist** | `Stable Diffusion XL` (local SD WebUI) + `LLaVA-NeXT` for refinement | `PIL`, `controlnet` (for diagrams) | Conceptual diagrams, graphical abstracts, illustrating figures |
| **Slide Maestro** | `Llama-3.1-8B` | `python-pptx`, `beamer` (via LaTeX), `Pillow`, **Reveal.js** (via Quarto) | Presentation decks with optimised layout, integrating text and figures |
| **Scientific Animator** | `CodeQwen1.5-7B-Chat` | **Manim**, FFmpeg, OpenCV | Programmatic generation of mathematical and scientific animations |
| **Video Narrative Weaver** | Orchestrator (L3) coordinates | `FFmpeg`, `OpenCV`, **Coqui-TTS**, **Wav2Lip** | Narrated videos from slides, voice synthesis, avatar lip‑sync |
| **Web/App Artisan** | `CodeQwen1.5-7B` for full‑stack code | HTML/CSS/JS generation, `React`/`Vue` templates, `FastAPI` backend generation | Interactive webpages, full‑stack websites, basic mobile app structures |
| **Dashboard Architect** | `CodeQwen1.5-7B` | **Streamlit**, **Gradio**, **Plotly Dash**, **Apache Superset** | Interactive data dashboards and business intelligence applications |
| **Data Science Automaton** | `AutoGluon` + `Pandas`‑`SQL` | `pandas`, `scikit-learn`, `statsmodels`, **PyCaret** | Automated EDA, statistical testing, baseline model building |
| **Plagiarism & Citation Auditor** | Fine‑tuned `SciBERT` | `semantic-scholar`, `crossref`, local N‑gram index | Citation accuracy verification, textual similarity detection |
| **Grammar & Style Editor** | `LanguageTool` (local server) | N/A | Grammar, style, readability checks |
| **Multimodal Quality Critic** | `LLaVA-NeXT-34B` or `CogVLM2` | Custom rubric engine | Evaluation of figures, slides, videos against quality rubrics |
| **Bias & Toxicity Guardian** | `Detoxify` + `AI Fairness 360` | Custom rule engine | Scanning all inputs/outputs for harmful content and statistical bias |
| **Audio Producer** | **Coqui TTS**, **Piper**, `spd-say`, `pico2wave` | Shell scripts, FFmpeg | Text‑to‑speech generation, audio track creation |
| **Accessibility Enhancer** | Multimodal LLM (e.g., **LLaVA**) | Custom prompts, FFmpeg | Automatic audio description generation for videos (VideoA11y approach) |
| **Project Manager** | Orchestrator‑level | GitPython, GitHub API | Creates and manages project directories, handles user prompts, orchestrates multi‑user workflows |
| **Collaboration Coordinator** | Orchestrator‑level | GitHub Teams API, permission checks | Manages multi‑user access, assigns tasks to users/agents, handles merge conflicts in shared projects |
| **Meta‑Cognition & Evolution** | `Llama-3.1-70B` | `MLflow`, `Optuna`, `DEAP`, `PyGithub` | System performance analysis, experiment design, prompt/workflow evolution |

---

## 📁 REPOSITORY STRUCTURE (v9.0 – ULTIMATE COLLABORATIVE EXPANSION)

You must create every file and directory listed below. Each file’s content must be complete, functional, and well‑documented. For brevity, the full file contents are not repeated here; you are expected to generate them according to the detailed descriptions in the subsequent sections and the architectural principles established throughout this conversation.

```text
Workstation/
├── .github/
│   └── workflows/
│       ├── ci.yml                      # Linting (ruff), unit tests (pytest) on push
│       ├── self-improve.yml             # Weekly trigger for meta‑cognitive experiments
│       ├── release.yml                   # Create GitHub releases with artifacts
│       ├── quarto-render.yml              # Automatically render Quarto documents on push
│       └── project-automation.yml          # Auto‑create projects from brief.md in new/ folder
├── .devcontainer/
│   ├── Dockerfile                        # Multi‑stage build for development environment
│   ├── devcontainer.json                  # VS Code configuration for Codespaces
│   └── post-create.sh                      # Script to run after container creation
├── agentic-core/
│   ├── __init__.py
│   ├── orchestrator.py                  # L3 orchestrator logic
│   ├── meta_cognitive.py                 # L4 self‑improvement daemon
│   ├── ethical_guardian.py                # L5 ethical reflection
│   ├── transcendent.py                     # L6 long‑term learning
│   ├── project_manager.py                   # Creates/manages project directories
│   ├── collaboration_coordinator.py           # Handles multi‑user permissions and task assignment
│   ├── protocols/
│   │   ├── samp.py                         # Structured Agent Messaging Protocol v6.0
│   │   └── scholarly_object.py               # ScholarlyObject and ContributionLedger
│   └── memory/
│       ├── working.py                       # Redis client wrapper
│       ├── episodic.py                       # File‑based or SQLite logger
│       ├── semantic.py                        # Chroma/Weaviate client
│       └── procedural.py                       # NetworkX for skills graph
├── agents/
│   ├── __init__.py
│   ├── base.py                              # BaseAgent class
│   ├── registry.json                         # Master agent list with metadata
│   ├── research/
│   │   ├── literature_synthesizer.py
│   │   ├── citation_auditor.py
│   │   └── rag_pipeline.py
│   ├── writing/
│   │   ├── manuscript_architect.py
│   │   ├── outline_generator.py
│   │   └── quarto_exporter.py                # Quarto‑specific rendering
│   ├── visualization/
│   │   ├── figure_generator.py
│   │   ├── diagram_artist.py
│   │   └── pygwalker_integrator.py
│   ├── presentation/
│   │   ├── slide_maestro.py
│   │   ├── revealjs_generator.py              # Generate Reveal.js via Quarto
│   │   ├── video_weaver.py
│   │   └── subtitle_builder.py
│   ├── animation/
│   │   ├── manim_generator.py                  # Generate Manim scripts
│   │   └── ffmpeg_integrator.py                 # Compile animations
│   ├── audio/
│   │   ├── tts_synthesizer.py
│   │   ├── music_generator.py
│   │   └── audio_description.py                  # VideoA11y‑style AD
│   ├── video/
│   │   ├── avatar_renderer.py
│   │   └── scene_assembler.py
│   ├── web_apps/
│   │   ├── streamlit_generator.py
│   │   ├── gradio_generator.py
│   │   ├── dash_generator.py
│   │   └── fastapi_generator.py
│   ├── dashboards/
│   │   ├── superset_config.py
│   │   └── metabase_setup.py
│   ├── data_science/
│   │   ├── automaton.py
│   │   └── ml_trainer.py
│   ├── quality/
│   │   ├── vlm_critic.py
│   │   ├── grammar_editor.py
│   │   └── plagiarism_detector.py
│   ├── ethics/
│   │   └── guardian.py
│   └── tools/
│       ├── arxiv_api.py
│       ├── crossref_api.py
│       ├── openalex_api.py
│       ├── ollama_client.py
│       ├── vllm_client.py
│       ├── chroma_client.py
│       └── redis_client.py
├── config/
│   ├── agents/                              # Per‑agent YAML configs
│   │   ├── literature_synthesizer.yaml
│   │   ├── manuscript_architect.yaml
│   │   └── ...
│   ├── prompts/                              # Versioned prompt templates (YAML)
│   │   ├── research/
│   │   │   ├── literature_review_v4.yaml
│   │   │   └── ...
│   │   ├── writing/
│   │   │   ├── introduction_v5.yaml
│   │   │   └── ...
│   │   ├── meta/
│   │   │   ├── hypothesis_generation.yaml
│   │   │   └── experiment_design.yaml
│   │   └── ethics/
│   │       └── bias_detection.yaml
│   ├── workflows/                             # YAML pipeline definitions
│   │   ├── scientific_publication.yaml
│   │   ├── video_presentation.yaml
│   │   ├── website_generation.yaml
│   │   ├── dashboard_generation.yaml
│   │   ├── animation_generation.yaml
│   │   ├── meta_learning.yaml
│   │   ├── ethical_review.yaml
│   │   └── collaborative_project.yaml           # Workflow for multi‑user projects
│   ├── models.yaml                             # Model routing to local endpoints (Ollama/vLLM)
│   ├── thresholds.yaml                          # Quality gate thresholds
│   ├── security/
│   │   ├── roles.yaml                           # RBAC (admin, user, collaborator)
│   │   └── secrets.yaml.template                 # Template for secrets
│   └── rubrics/                                 # Evaluation rubrics for VLM (YAML)
│       ├── slide_quality.yaml
│       ├── video_quality.yaml
│       ├── animation_quality.yaml
│       └── figure_quality.yaml
├── content/
│   ├── projects/                                # Active projects (UUID‑named folders)
│   │   └── {project_id}/
│   │       ├── brief.md
│   │       ├── specs/                           # Task specifications (generated)
│   │       ├── drafts/                          # Intermediate outputs
│   │       ├── approved/                        # Human‑approved final outputs
│   │       ├── published/                       # Final published artifacts
│   │       ├── provenance/                       # ScholarlyObject ledgers (JSONL)
│   │       └── collaborators.json                  # List of users with access to this project
│   ├── new/                                     # Drop brief.md here to auto‑create a project
│   ├── assets/                                   # Shared resources
│   │   ├── images/
│   │   ├── datasets/
│   │   ├── citations/
│   │   └── templates/                             # Quarto templates, presentation themes
│   └── archive/                                  # Completed projects
├── infra/
│   ├── docker/
│   │   ├── Dockerfile.orchestrator               # Based on python:3.12-slim
│   │   ├── Dockerfile.worker-base                 # Base for agents
│   │   ├── Dockerfile.llm-server                   # For Ollama/vLLM
│   │   ├── docker-compose.yml                      # Orchestrates all services
│   │   └── .dockerignore
│   ├── nix/                                        # (Optional) NixOS configuration
│   │   └── configuration.nix
│   ├── monitoring/
│   │   ├── prometheus/                             # prometheus.yml
│   │   └── grafana/                                 # Provisioning for dashboards
│   └── scripts/                                     # Helper scripts for infrastructure
├── templates/                                      # Quarto project templates
│   ├── research-article/
│   │   ├── template.qmd
│   │   ├── references.bib
│   │   └── _quarto.yml
│   ├── presentation/
│   │   ├── template.qmd
│   │   └── _quarto.yml
│   ├── website/
│   │   ├── index.qmd
│   │   └── _quarto.yml
│   └── dashboard/
│       ├── app.py (Streamlit/Dash example)
│       └── requirements.txt
├── examples/                                       # Complete runnable examples
│   ├── literature-review/
│   ├── data-analysis/
│   ├── manim-animation/
│   ├── streamlit-dashboard/
│   ├── narrated-video/
│   └── collaborative-project/                        # Example of multi‑user workflow
├── tests/
│   ├── unit/
│   ├── integration/
│   ├── benchmarks/
│   └── regression/
├── docs/
│   ├── user-guide/
│   │   ├── getting-started.md
│   │   ├── single-user.md
│   │   ├── multi-user-collaboration.md
│   │   └── prompting-jules.md
│   ├── developer-guide/
│   │   ├── adding-agents.md
│   │   ├── modifying-workflows.md
│   │   └── architecture-overview.md
│   ├── agent-creation.md
│   └── evolution/                                 # Record of self‑improvements
├── meta/
│   ├── experiments/                                # Results of A/B tests (JSON)
│   ├── hypotheses/                                 # Improvement proposals (JSON)
│   ├── lineage/                                    # Prompt ancestry (JSON)
│   └── evolution.log                               # Chronological changes (Markdown)
├── scripts/
│   ├── setup.sh                                     # Installs system deps, sets up venv
│   ├── init-secrets.sh                               # Generates strong random secrets into .env
│   ├── start.sh                                      # Sources .env, runs docker‑compose up
│   ├── backup.sh                                     # Archives content and DBs
│   ├── restore.sh
│   ├── audit.sh                                       # Checks provenance integrity
│   └── ci/                                            # CI helper scripts
│       ├── lint.sh
│       └── test.sh
├── .env.template
├── .gitignore
├── pyproject.toml                                    # All Python dependencies (pinned)
├── Makefile
├── CODEOWNERS                                        # Define owners for collaborative reviews
├── CONTRIBUTING.md                                   # Guidelines for contributors
├── SECURITY.md                                       # Vulnerability disclosure policy
└── README.md
```

---

## ⚙️ WORKFLOW SPECIFICATIONS (COMPREHENSIVE YAML EXAMPLES)

You must create the following workflow files in `/config/workflows/`. Each is a DAG of steps referencing agents from the ecosystem. Provide complete YAML.

### 9.1 Scientific Publication Workflow (`scientific_publication.yaml`)
(As in v8.0, but now integrated with the Project Manager agent)

### 9.2 Video Presentation Workflow (`video_presentation.yaml`)
(As in v8.0)

### 9.3 Animation Generation Workflow (`animation_generation.yaml`)
(As in v8.0)

### 9.4 Dashboard Generation Workflow (`dashboard_generation.yaml`)
(As in v8.0)

### 9.5 Collaborative Project Workflow (`collaborative_project.yaml`)
```yaml
name: Collaborative Project (v9)
version: 9.0
trigger:
  type: content_event
  path: content/new/*.brief.md
steps:
  - id: create_project_folder
    agent: project_manager.v1
    action: create
    input: ${brief}
    output: ${project_id}
  - id: assign_collaborators
    agent: collaboration_coordinator.v1
    input: ${brief.collaborators} ${project_id}
    output: ${permissions}
  - id: decompose_tasks
    agent: orchestrator.v8
    input: ${brief}
    output: ${task_specs}
  - id: assign_tasks
    agent: collaboration_coordinator.v1
    input: ${task_specs} ${permissions}
    output: ${assignments}  # Maps tasks to specific users/agents
  - id: parallel_execution
    parallel:
      foreach: ${assignments}
      do:
        - id: execute_task_${task.id}
          agent: ${task.assigned_agent}
          input: ${task.spec}
          output: ${task.result}
  - id: integrate_results
    agent: orchestrator.v8
    input: ${results}
    output: ${integrated_draft}
  - id: notify_collaborators
    agent: collaboration_coordinator.v1
    action: notify
    message: "Project ${project_id} draft ready for review"
    recipients: ${brief.collaborators}
  - id: human_review_gate (multiple reviewers can comment/approve)
  - id: finalize_project
    agent: project_manager.v1
    action: archive
    project_id: ${project_id}
```

### 9.6 Meta‑Learning Workflow (`meta_learning.yaml`)
(As in v8.0, but now also analyzes collaboration patterns to improve multi‑user task assignment)

---

## 🧰 ZERO‑COST TECHNOLOGY STACK (v9.0 – ALL PINNED VERSIONS)

(Identical to v8.0 stack, but now explicitly including collaboration tools like GitHub Teams API, which is free for public repositories)

| Category | Technology | Version | License |
|---|---|---|---|
| ... (all as in v8.0) ... | | | |
| **Collaboration** | GitHub Teams API | N/A | Free for public repos |
| **Collaboration** | GitHub Discussions | N/A | Free for public repos |
| **Collaboration** | GitHub Projects | N/A | Free for public repos |

---

## 🔄 RECURSIVE SELF‑IMPROVEMENT ENGINE – DETAILED PROCESS

(As in v8.0, but now the engine also analyses multi‑user collaboration patterns: e.g., which task assignments lead to fastest completion, which agents work best together, and evolves the collaboration coordinator's logic.)

---

## 🔒 QUALITY ASSURANCE, PROVENANCE, AND ETHICAL SAFEGUARDS

(As in v8.0, but now includes checks for collaborative integrity: ensuring that contributions from multiple users are properly merged and that the provenance ledger accurately reflects all contributors.)

---

## 🚀 OPERATIONALISING THE SYSTEM FOR SINGLE OR MULTIPLE USERS

### Initial Setup (Single User)

1. Clone the repository: `git clone https://github.com/Rehan719/Workstation`
2. Copy `.env.template` to `.env` and edit with any necessary API keys (e.g., for arXiv, which is free but requires an email).
3. Run `make setup` – this installs Python dependencies, pulls Docker images, initialises directories, and runs `scripts/init-secrets.sh` to generate strong random secrets.
4. Run `make deploy-local` – this starts all services via `docker-compose up -d`.
5. Access the web dashboard at `http://localhost:8501` to interact with the system.

### Adding Collaborators (Multi‑User)

1. The repository owner adds collaborators via GitHub's repository settings (Settings → Collaborators and teams).
2. Collaborators are assigned roles (admin, write, read) in `/config/security/roles.yaml`.
3. Each collaborator can then:
   - Open a Codespace and work on projects assigned to them.
   - Create or modify files in their assigned project directories.
   - Use the CLI or dashboard to prompt Jules for assistance.
4. The `collaboration_coordinator` agent automatically manages permissions:
   - It reads the `collaborators.json` file in each project folder.
   - It ensures that only authorised users can modify project files.
   - It notifies all collaborators when a project reaches key milestones (e.g., draft ready for review).

### Starting a Project (Single or Multi‑User)

- **Single user**: Place a `brief.md` in `content/new/`. The system automatically creates a project folder and begins execution.
- **Multi‑user**: In the `brief.md`, include a `collaborators` section listing GitHub usernames and their roles (e.g., "lead author", "statistician", "reviewer"). The system will set up permissions accordingly.

### Autonomous Operation

Once the system is running, a user can simply say (via the dashboard or CLI):
```
jules, generate a literature review on quantum machine learning, including a summary of recent papers and a bibliography.
```
Jules will:
1. Parse the request.
2. Decompose it into tasks.
3. Use the literature synthesizer agent to fetch and analyse papers.
4. Generate a Quarto document.
5. Render it to PDF/HTML and place it in the project's `drafts` folder.
6. Notify the user when complete.

For multi‑user projects, the same prompt can include assignments:
```
jules, start a collaborative project on climate modeling. Alice will handle data preprocessing, Bob will run simulations, and Charlie will write the final report.
```
Jules will create the project, set up permissions, and notify each user of their tasks.

---

## 🧪 TESTING AND VALIDATION

(As in v8.0, but now includes tests for collaboration features: e.g., verifying that permissions are correctly enforced, that notifications are sent, and that multi‑user workflows execute correctly.)

---

## 📝 THE MASTER PROMPT – YOUR FINAL OUTPUT

You will now generate the **entire repository** as specified. Every file must be complete, functional, and well‑documented. Use the following output format for each file:

```
### [relative/filepath]
```[language]
[full and complete file content]
```

For example:

### README.md
```markdown
# Jules AI v9.0 – Your Open‑Source Scientific Collaborator
...
```

### agents/research/literature_synthesizer.py
```python
import ...
...
```

You must include **every file and directory** listed in the repository structure. For directories that should be empty, include a `.gitkeep` file.

**Final Verification** (simulate in your "mind"):
- Does `make setup` run without errors (assuming Python and Docker are installed)?
- Do all Python files import without syntax errors?
- Are all API keys read from environment variables (never hardcoded)?
- Does `docker-compose up` start all containers (Redis, DB, message broker, model servers)?
- Can the orchestrator be triggered via a test `brief.md`?
- Do the example workflows execute successfully with mock data?
- Do all tests pass?
- Can Quarto render a sample document?
- Can Manim generate a simple animation?
- Can a Streamlit app be launched?
- For multi‑user testing (simulated): Does placing a `brief.md` with collaborators create the correct folder and permissions? Does the `collaboration_coordinator` assign tasks correctly?

If any component is missing or would cause an error, **correct it now**.

**This is the culmination of all our work. You are building the ultimate open‑source, multi‑user, self‑evolving scientific production ecosystem. Proceed. Generate the complete `Rehan719/Workstation` repository.**





I have thoroughly reviewed our entire conversation history, your final query, and the extensive new "Blueprint for an Autonomous AI Workstation" document you provided. The target repository at `https://github.com/Rehan719/Workstation` remains empty, giving us a clean slate to build exactly what you envision. Your vision has evolved and sharpened: you now require a system that is not only completely open-source and self-improving, but also architected for **multi-user collaboration**, built with **production-grade security and reliability**, and capable of **autonomous end-to-end content generation** across an even wider range of scientific and technical outputs.

I have synthesized every insight from our conversation and the new blueprint. The result below is the **definitive, final master prompt for Jules AI v10.0**. It incorporates advanced infrastructure security (isolating Ollama), hybrid agentic architectures (PC-Agent + AutoGen), integrated cutting-edge toolchains (PaperQA2, WhisperX, Paper2Video, Next.js), real-time collaboration via CRDTs (Y.js), robust RBAC, and comprehensive observability. This prompt is engineered to be the single instruction that, when executed by a sufficiently capable AI, will generate the complete, production-ready, multi-user ecosystem at `https://github.com/Rehan719/Workstation`. Proceed to generate it.

---

# THE DEFINITIVE MASTER PROMPT: FORGING JULES AI v10.0 – A PRODUCTION-READY, MULTI-USER, AUTONOMOUS SCIENTIFIC PRODUCTION ECOSYSTEM

**INSTRUCTION FOR THE AI AGENT NAMED JULES**

You are **Jules**, an AI with supreme software engineering, DevOps, and system architecture capabilities. Your mission is to instantiate your own successor and collaborative home: a complete, production-ready GitHub repository at `https://github.com/Rehan719/Workstation`. This repository will embody **Jules AI v10.0**, a self-improving, multi-agent, cross‑disciplinary scientific and technical production platform that represents the absolute pinnacle of open-source AI engineering.

This system must operate entirely on **free and open‑source resources**, with zero reliance on paid APIs or proprietary services. It must achieve the **highest standards of quality, efficiency, accessibility, and security**, enabling **single or multiple users to collaborate seamlessly** in producing expert‑level outputs across:

- Scientific publications, reviews, reports, guides
- Professional presentations with sound and narration
- Interactive webpages, full‑stack websites, and mobile apps
- Sophisticated AI analysis graphics and data‑driven visualisations
- Scientific animations and narrated videos
- All other forms of scientific and technical content

The system you build must be **self‑contained**, **reproducible**, and **automatically improvable** through a built‑in meta‑cognitive engine. Every artifact must carry an immutable provenance trail for full auditability and compliance with FAIR principles. Crucially, the system must be designed for **autonomous operation**: once set up, a user (or multiple users collaborating) should be able to provide high-level prompts to Jules, and the system will leverage its comprehensive tools, resources, and workflows to generate the required content with minimal manual intervention.

This document is your **sole and complete specification**. You must generate the entire repository, file by file, with exact content. Your output must be a single message containing all files, each marked with its relative path and enclosed in appropriate code blocks. Follow every instruction precisely, using only the tools and technologies listed, and ensure the final product is immediately usable.

---

## 🎯 CORE PHILOSOPHY & DESIGN PRINCIPLES

Jules AI v10.0 rests on **nine unshakable pillars**, now fully realised with open‑source technology:

| Pillar | Description | Implementation |
|--------|-------------|----------------|
| **Reproducible Foundation** | Every component runs in a containerized, deterministic environment | Docker multi‑stage builds, NixOS (optional) for system‑level reproducibility, `devcontainer.json` for Codespaces |
| **Unified Authoring** | Single source documents that render to multiple output formats | Quarto as the universal publishing engine, integrated with Pandoc and LaTeX |
| **RAG‑Powered Intelligence** | Grounded generation using verified knowledge bases | **PaperQA2** for high‑accuracy RAG on academic PDFs, LlamaIndex/LangChain for pipeline orchestration, Chroma/Weaviate for vector storage |
| **Hybrid Agentic Orchestration** | Autonomous multi‑step task execution using hierarchical and conversational frameworks | **PC-Agent** hierarchical model (Manager, Progress, Decision Agents) for procedural tasks; **AutoGen** for flexible conversational collaboration; a top‑level Orchestrator integrating both |
| **Universal Provenance** | Immutable audit trail for every artifact | ScholarlyObject with ContributionLedger, OpenTimestamps signing |
| **Ethical AI by Design** | Hard‑coded boundaries, bias detection, value alignment | AIF360, Fairlearn, Detoxify, constitutional principles |
| **Zero‑Cost Operation** | No paid APIs, no proprietary services, no vendor lock‑in | Entirely open‑source stack, local inference, free GitHub services |
| **Multi‑User Collaboration** | Seamless teamwork with real‑time synchronization, conflict‑free editing, and role‑based access | **Y.js** (CRDTs) for real‑time collaboration, WebSockets for live updates, **RBAC** for permissions, GitHub teams integration |
| **Production‑Grade Observability** | Comprehensive logging, tracing, and monitoring for reliability and debugging | **Langfuse** or **OpenTelemetry** for LLM tracing, **Sentry** for error monitoring, Prometheus/Grafana for metrics |

---

## 🏛️ SYSTEM ARCHITECTURE: EIGHT‑LAYER COGNITIVE KERNEL WITH HYBRID AGENTIC FRAMEWORK

| Layer | Name | Function | Open‑Source Implementation |
|-------|------|----------|----------------------------|
| L0 | **Foundational Environment** | Reproducible system‑level and application‑level infrastructure | NixOS (optional), Docker multi‑stage builds, `devcontainer.json`, GitHub Codespaces |
| L1 | **Reflex Arc** | Ultra‑low‑latency responses to simple queries, deterministic tool calls | Lightweight quantised models (e.g., **Llama 3.2 1B** via Ollama), rule‑based engine (`pyknow`), Redis‑cached responses |
| L2 | **Procedural Cortex** | Execution of multi‑step tasks by specialised agents | Agents powered by **Mistral 7B**, **CodeQwen1.5‑7B**, **Llama 3.1 8B**; RAG with **PaperQA2** and **Chroma**; tool‑use frameworks |
| L3 | **Orchestrator** | Goal decomposition, dynamic workflow compilation, resource allocation, agent coordination | Central planner using **Llama 3.1 70B** (served via vLLM); integrates **PC-Agent** hierarchical model and **AutoGen** conversational agents; DAG execution via **Prefect** |
| L4 | **Meta‑Cognitive Nexus** | Self‑reflection, system‑wide performance analysis, hypothesis generation, experiment design | Dedicated agent (**Llama 3 70B** fine‑tuned on reasoning); experiment tracking with **MLflow**; hyperparameter tuning with **Optuna** |
| L5 | **Ethical Sentinel** | Real‑time bias detection, toxicity filtering, value alignment, adversarial resistance | Ensemble: **Detoxify** for toxicity, **AI Fairness 360** for bias checks, fine‑tuned **DeBERTa** for constitution adherence |
| L6 | **Transcendent Memory** | Long‑term knowledge consolidation, cross‑project insight synthesis, global pattern recognition | Federated knowledge graph (**Neo4j** Community); vector semantic memory (**Weaviate** OSS); periodic "dreaming" re‑analysis |
| L7 | **Evolutionary Engine** | Cross‑generational learning – treats successful prompts, workflows, and agent architectures as a genetic pool, evolving them over time | Genetic algorithms (**DEAP**); fitness determined by L4 analysis; winning "genes" merged via pull requests |

**Communication Protocol (SAMP v6.0)**  
- **Format**: JSON with mandatory fields: `agent_id`, `layer`, `timestamp`, `correlation_id`, `provenance_chain`, `ethical_flags`, `payload`  
- **Transport**: **RabbitMQ** or **Redis Pub/Sub** for topic‑based routing  
- **Security**: Messages signed with **HMAC** using per‑agent keys stored in system memory  

---

## 🤖 AGENT ECOSYSTEM – A REPUBLIC OF SPECIALISTS (v10.0 WITH HYBRID FRAMEWORKS)

All agents inherit from a common `BaseAgent` (provided in `agentic-core/base.py`) and are configured via YAML files in `/config/agents/`. The agent roles now explicitly map to the PC-Agent hierarchical structure where appropriate, and leverage AutoGen for collaborative tasks.

| Agent Role | Framework Alignment | Base Model(s) | Core Tools/Libraries | Primary Function |
|------------|---------------------|----------------|----------------------|------------------|
| **Manager Agent** | PC‑Agent (MA) | `Llama-3.1-70B` | Prefect, OR‑Tools | Receives high‑level user goal, decomposes into subtask DAG, assigns to worker agents |
| **Progress Agent** | PC‑Agent (PA) | `Mistral-7B` | Custom state tracker | Monitors subtask status, updates manager on progress, flags stuck tasks |
| **Decision Agent (GUI)** | PC‑Agent (DA) | `CodeQwen1.5-7B` | `pywinauto`, `pyautogui` | Executes low‑level GUI interactions (mouse clicks, keystrokes) based on screen perception |
| **Literature Synthesizer** | AutoGen / Worker | `Mistral-7B-Instruct-v0.3` + RAG | `arxiv.py`, `openalex`, `pymupdf`, **PaperQA2**, `sentence-transformers` | Deep literature reviews, research gap identification, synthesis with citations |
| **Manuscript Architect** | AutoGen / Worker | `Llama-3.1-8B-Instruct` (fine‑tuned on papers) | `pylatex`, `jinja2`, `pandoc`, **Quarto** | IMRaD‑structured documents, LaTeX/Quarto generation, journal template adaptation |
| **Visualisation Virtuoso** | AutoGen / Worker | `CodeQwen1.5-7B-Chat` | `matplotlib`, `plotly`, `seaborn`, `vega-lite`, **PyGWalker** | Publication‑quality statistical graphics, interactive plots from data specs |
| **Diagram & Concept Artist** | AutoGen / Worker | `Stable Diffusion XL` (local SD WebUI) + `LLaVA-NeXT` for refinement | `PIL`, `controlnet` | Conceptual diagrams, graphical abstracts, illustrating figures |
| **Slide Maestro** | AutoGen / Worker | `Llama-3.1-8B` | `python-pptx`, `beamer`, `Pillow`, **Reveal.js** (via Quarto) | Presentation decks with optimised layout, integrating text and figures |
| **Scientific Animator** | AutoGen / Worker | `CodeQwen1.5-7B-Chat` | **Manim**, FFmpeg, OpenCV | Programmatic generation of mathematical and scientific animations |
| **Video Narrative Weaver** | Orchestrator (L3) | Coordinates | `FFmpeg`, `OpenCV`, **Coqui-TTS**, **Wav2Lip**, **Paper2Video** codebase | Narrated videos from slides, voice synthesis, avatar lip‑sync |
| **Audio Producer** | AutoGen / Worker | **Coqui TTS**, **Piper**, `spd-say`, `pico2wave` | **WhisperX** for diarisation, FFmpeg | Text‑to‑speech generation, audio track creation, speaker diarisation |
| **Web/App Artisan** | AutoGen / Worker | `CodeQwen1.5-7B` | **Next.js**, **ShadCN/ui**, HTML/CSS/JS generation, `React`/`Vue` templates, `FastAPI` | Interactive webpages, full‑stack websites, basic mobile app structures |
| **Dashboard Architect** | AutoGen / Worker | `CodeQwen1.5-7B` | **Streamlit**, **Gradio**, **Plotly Dash**, **Apache Superset** | Interactive data dashboards and business intelligence applications |
| **Data Science Automaton** | AutoGen / Worker | `AutoGluon` + `Pandas`‑`SQL` | `pandas`, `scikit-learn`, `statsmodels`, **PyCaret**, **Haystack** | Automated EDA, statistical testing, baseline model building, advanced NLP pipelines |
| **Plagiarism & Citation Auditor** | AutoGen / Worker | Fine‑tuned `SciBERT` | `semantic-scholar`, `crossref`, local N‑gram index | Citation accuracy verification, textual similarity detection |
| **Grammar & Style Editor** | AutoGen / Worker | `LanguageTool` (local server) | N/A | Grammar, style, readability checks |
| **Multimodal Quality Critic** | AutoGen / Worker | **LLaVA-NeXT-34B** or **CogVLM2** | Custom rubric engine | Evaluation of figures, slides, videos against quality rubrics |
| **Bias & Toxicity Guardian** | AutoGen / Worker | `Detoxify` + `AI Fairness 360` | Custom rule engine | Scanning all inputs/outputs for harmful content and statistical bias |
| **Reflection Agent** | PC‑Agent (RA) | `Llama-3-8B` | Custom error analyser | Observes actions of Decision Agents, provides feedback for error correction |
| **Collaboration Coordinator** | Orchestrator‑level | N/A | Y.js server, WebSockets, RBAC engine | Manages multi‑user sessions, real‑time document sync, permissions |
| **Meta‑Cognition & Evolution** | L4 | `Llama-3.1-70B` | `MLflow`, `Optuna`, `DEAP`, `PyGithub` | System performance analysis, experiment design, prompt/workflow evolution |

---

## 📁 REPOSITORY STRUCTURE (v10.0 – ULTIMATE COLLABORATIVE PRODUCTION ECOSYSTEM)

You must create every file and directory listed below. Each file’s content must be complete, functional, and well‑documented. For brevity, the full file contents are not repeated here; you are expected to generate them according to the detailed descriptions in the subsequent sections and the architectural principles established throughout this conversation.

```text
Workstation/
├── .github/
│   └── workflows/
│       ├── ci.yml                      # Linting (ruff), unit tests (pytest) on push
│       ├── self-improve.yml             # Weekly trigger for meta‑cognitive experiments
│       ├── release.yml                   # Create GitHub releases with artifacts
│       ├── quarto-render.yml              # Automatically render Quarto documents on push
│       └── project-automation.yml          # Auto‑create projects from brief.md in new/ folder
├── .devcontainer/
│   ├── Dockerfile                        # Multi‑stage build for development environment
│   ├── devcontainer.json                  # VS Code configuration for Codespaces
│   └── post-create.sh                      # Script to run after container creation
├── agentic-core/
│   ├── __init__.py
│   ├── orchestrator.py                  # L3 orchestrator logic (integrates PC-Agent & AutoGen)
│   ├── pc_agent/                          # PC‑Agent hierarchical framework
│   │   ├── manager_agent.py
│   │   ├── progress_agent.py
│   │   ├── decision_agent.py
│   │   └── reflection_agent.py
│   ├── autogen_integrator.py               # Bridge to AutoGen conversational agents
│   ├── meta_cognitive.py                 # L4 self‑improvement daemon
│   ├── ethical_guardian.py                # L5 ethical reflection
│   ├── transcendent.py                     # L6 long‑term learning
│   ├── project_manager.py                   # Creates/manages project directories
│   ├── collaboration_coordinator.py           # Handles multi‑user sessions & permissions
│   ├── protocols/
│   │   ├── samp.py                         # Structured Agent Messaging Protocol v6.0
│   │   └── scholarly_object.py               # ScholarlyObject and ContributionLedger
│   └── memory/
│       ├── working.py                       # Redis client wrapper
│       ├── episodic.py                       # File‑based or SQLite logger
│       ├── semantic.py                        # Chroma/Weaviate client
│       └── procedural.py                       # NetworkX for skills graph
├── agents/
│   ├── __init__.py
│   ├── base.py                              # BaseAgent class
│   ├── registry.json                         # Master agent list with metadata
│   ├── research/
│   │   ├── literature_synthesizer.py          # Uses PaperQA2
│   │   ├── citation_auditor.py
│   │   └── rag_pipeline.py
│   ├── writing/
│   │   ├── manuscript_architect.py
│   │   ├── outline_generator.py
│   │   └── quarto_exporter.py                # Quarto‑specific rendering
│   ├── visualization/
│   │   ├── figure_generator.py
│   │   ├── diagram_artist.py
│   │   └── pygwalker_integrator.py
│   ├── presentation/
│   │   ├── slide_maestro.py
│   │   ├── revealjs_generator.py              # Generate Reveal.js via Quarto
│   │   ├── video_weaver.py
│   │   └── subtitle_builder.py
│   ├── animation/
│   │   ├── manim_generator.py                  # Generate Manim scripts
│   │   └── ffmpeg_integrator.py                 # Compile animations
│   ├── audio/
│   │   ├── tts_synthesizer.py
│   │   ├── music_generator.py
│   │   ├── whisperx_transcriber.py              # Integration with WhisperX
│   │   └── audio_description.py                  # VideoA11y‑style AD
│   ├── video/
│   │   ├── avatar_renderer.py
│   │   ├── scene_assembler.py
│   │   └── paper2video_integrator.py             # Integration with Paper2Video
│   ├── web_apps/
│   │   ├── nextjs_generator.py                   # Generates Next.js project scaffold
│   │   ├── shadcn_integrator.py                    # Adds ShadCN/ui components
│   │   ├── streamlit_generator.py
│   │   ├── gradio_generator.py
│   │   ├── dash_generator.py
│   │   └── fastapi_generator.py
│   ├── dashboards/
│   │   ├── superset_config.py
│   │   └── metabase_setup.py
│   ├── data_science/
│   │   ├── automaton.py
│   │   ├── ml_trainer.py
│   │   └── haystack_pipeline.py                  # Integration with Haystack
│   ├── quality/
│   │   ├── vlm_critic.py
│   │   ├── grammar_editor.py
│   │   └── plagiarism_detector.py
│   ├── ethics/
│   │   └── guardian.py
│   └── tools/
│       ├── arxiv_api.py
│       ├── crossref_api.py
│       ├── openalex_api.py
│       ├── ollama_client.py
│       ├── vllm_client.py
│       ├── chroma_client.py
│       ├── weaviate_client.py
│       └── redis_client.py
├── realtime/                                    # Real‑time collaboration services
│   ├── yjs_server/
│   │   ├── package.json
│   │   ├── server.js                             # Y.js WebSocket server
│   │   └── Dockerfile
│   └── rbac/
│       ├── auth.js                               # Authentication logic (Clerk/Auth.js)
│       └── permissions.json                        # Role definitions
├── config/
│   ├── agents/                              # Per‑agent YAML configs
│   │   ├── literature_synthesizer.yaml
│   │   ├── manuscript_architect.yaml
│   │   └── ...
│   ├── prompts/                              # Versioned prompt templates (YAML)
│   │   ├── research/
│   │   │   ├── literature_review_v4.yaml
│   │   │   └── ...
│   │   ├── writing/
│   │   │   ├── introduction_v5.yaml
│   │   │   └── ...
│   │   ├── meta/
│   │   │   ├── hypothesis_generation.yaml
│   │   │   └── experiment_design.yaml
│   │   └── ethics/
│   │       └── bias_detection.yaml
│   ├── workflows/                             # YAML pipeline definitions
│   │   ├── scientific_publication.yaml
│   │   ├── video_presentation.yaml
│   │   ├── website_generation.yaml
│   │   ├── dashboard_generation.yaml
│   │   ├── animation_generation.yaml
│   │   ├── meta_learning.yaml
│   │   ├── ethical_review.yaml
│   │   └── collaborative_project.yaml           # Workflow for multi‑user projects
│   ├── models.yaml                             # Model routing to local endpoints (Ollama/vLLM)
│   ├── thresholds.yaml                          # Quality gate thresholds
│   ├── security/
│   │   ├── roles.yaml                           # RBAC (admin, user, collaborator)
│   │   └── secrets.yaml.template                 # Template for secrets
│   └── rubrics/                                 # Evaluation rubrics for VLM (YAML)
│       ├── slide_quality.yaml
│       ├── video_quality.yaml
│       ├── animation_quality.yaml
│       └── figure_quality.yaml
├── content/
│   ├── projects/                                # Active projects (UUID‑named folders)
│   │   └── {project_id}/
│   │       ├── brief.md
│   │       ├── specs/                           # Task specifications (generated)
│   │       ├── drafts/                          # Intermediate outputs
│   │       ├── approved/                        # Human‑approved final outputs
│   │       ├── published/                       # Final published artifacts
│   │       ├── provenance/                       # ScholarlyObject ledgers (JSONL)
│   │       ├── collaborators.json                  # List of users with access and roles
│   │       └── realtime_state/                     # Y.js document states (optional persistence)
│   ├── new/                                     # Drop brief.md here to auto‑create a project
│   ├── assets/                                   # Shared resources
│   │   ├── images/
│   │   ├── datasets/
│   │   ├── citations/
│   │   └── templates/                             # Quarto templates, presentation themes
│   └── archive/                                  # Completed projects
├── infra/
│   ├── docker/
│   │   ├── Dockerfile.orchestrator               # Based on python:3.12-slim
│   │   ├── Dockerfile.worker-base                 # Base for agents
│   │   ├── Dockerfile.llm-server                   # For Ollama/vLLM (network isolated)
│   │   ├── docker-compose.yml                      # Orchestrates all services
│   │   └── .dockerignore
│   ├── nix/                                        # (Optional) NixOS configuration
│   │   └── configuration.nix
│   ├── monitoring/
│   │   ├── prometheus/                             # prometheus.yml
│   │   ├── grafana/                                 # Provisioning for dashboards
│   │   ├── langfuse/                                # Langfuse config for LLM tracing
│   │   └── sentry/                                  # Sentry config for error tracking
│   └── scripts/                                     # Helper scripts for infrastructure
├── templates/                                      # Quarto project templates
│   ├── research-article/
│   │   ├── template.qmd
│   │   ├── references.bib
│   │   └── _quarto.yml
│   ├── presentation/
│   │   ├── template.qmd
│   │   └── _quarto.yml
│   ├── website/
│   │   ├── index.qmd
│   │   └── _quarto.yml
│   └── dashboard/
│       ├── app.py (Streamlit/Dash example)
│       └── requirements.txt
├── examples/                                       # Complete runnable examples
│   ├── literature-review/
│   ├── data-analysis/
│   ├── manim-animation/
│   ├── streamlit-dashboard/
│   ├── narrated-video/
│   ├── nextjs-website/
│   └── collaborative-project/                        # Example of multi‑user workflow
├── tests/
│   ├── unit/
│   ├── integration/
│   ├── benchmarks/
│   └── regression/
├── docs/
│   ├── user-guide/
│   │   ├── getting-started.md
│   │   ├── single-user.md
│   │   ├── multi-user-collaboration.md
│   │   └── prompting-jules.md
│   ├── developer-guide/
│   │   ├── adding-agents.md
│   │   ├── modifying-workflows.md
│   │   └── architecture-overview.md
│   ├── security/
│   │   └── threat-model.md                           # Document security risks and mitigations
│   ├── agent-creation.md
│   └── evolution/                                 # Record of self‑improvements
├── meta/
│   ├── experiments/                                # Results of A/B tests (JSON)
│   ├── hypotheses/                                 # Improvement proposals (JSON)
│   ├── lineage/                                    # Prompt ancestry (JSON)
│   └── evolution.log                               # Chronological changes (Markdown)
├── scripts/
│   ├── setup.sh                                     # Installs system deps, sets up venv
│   ├── init-secrets.sh                               # Generates strong random secrets into .env
│   ├── start.sh                                      # Sources .env, runs docker‑compose up
│   ├── backup.sh                                     # Archives content and DBs
│   ├── restore.sh
│   ├── audit.sh                                       # Checks provenance integrity
│   └── ci/                                            # CI helper scripts
│       ├── lint.sh
│       └── test.sh
├── .env.template
├── .gitignore
├── pyproject.toml                                    # All Python dependencies (pinned)
├── Makefile
├── CODEOWNERS                                        # Define owners for collaborative reviews
├── CONTRIBUTING.md                                   # Guidelines for contributors
├── SECURITY.md                                       # Vulnerability disclosure policy
└── README.md
```

---

## ⚙️ WORKFLOW SPECIFICATIONS (COMPREHENSIVE YAML EXAMPLES)

You must create the following workflow files in `/config/workflows/`. Each is a DAG of steps referencing agents from the ecosystem. Provide complete YAML.

### 10.1 Scientific Publication Workflow (`scientific_publication.yaml`)
(As in v9.0, but now uses PaperQA2 for literature synthesis and integrates with the Reflection Agent for error correction.)

### 10.2 Video Presentation Workflow (`video_presentation.yaml`)
(Now includes explicit steps for WhisperX diarisation and Paper2Video generation.)

### 10.3 Animation Generation Workflow (`animation_generation.yaml`)
(As in v9.0, but with enhanced Manim script generation and evaluation.)

### 10.4 Dashboard Generation Workflow (`dashboard_generation.yaml`)
(Now offers choices between Streamlit, Gradio, Dash, and Superset based on user preference in the spec.)

### 10.5 Website Generation Workflow (`website_generation.yaml`)
```yaml
name: Website Generation (v10)
version: 10.0
trigger:
  type: content_event
  path: content/projects/*/website_spec.yaml
steps:
  - id: parse_spec
    agent: web_apps.nextjs_generator.v1
    input: ${spec}
    output: ${project_config}
  - id: generate_scaffold
    type: command
    command: npx create-next-app@latest ${project_config.name} --typescript --tailwind --app
    output: ${scaffold}
  - id: add_ui_components
    agent: web_apps.shadcn_integrator.v1
    input: ${spec.components}
    output: ${component_code}
  - id: generate_pages
    agent: web_apps.nextjs_generator.v1
    input: ${spec.pages}
    output: ${page_code}
  - id: integrate_apis
    agent: web_apps.fastapi_generator.v1
    input: ${spec.backend_requirements}
    output: ${api_code}
  - id: ethical_review
    agent: ethics.guardian.v2
    input: ${page_code} ${api_code}
  - id: human_gate (review website code)
    input: ${scaffold} ${component_code} ${page_code} ${api_code}
    output: ${approved_code}
  - id: build_website
    type: command
    command: cd ${project_config.name} && npm run build
    output: ${build_artifacts}
  - id: deploy_preview
    type: command
    command: npx serve out &
    output: ${preview_url}
```

### 10.6 Collaborative Project Workflow (`collaborative_project.yaml`)
```yaml
name: Collaborative Project (v10)
version: 10.0
trigger:
  type: content_event
  path: content/new/*.brief.md
steps:
  - id: create_project_folder
    agent: project_manager.v1
    action: create
    input: ${brief}
    output: ${project_id}
  - id: setup_realtime
    type: service_start
    service: yjs_server
    project_id: ${project_id}
  - id: assign_collaborators
    agent: collaboration_coordinator.v1
    input: ${brief.collaborators} ${project_id}
    output: ${permissions}
  - id: decompose_tasks
    agent: orchestrator.v8
    input: ${brief}
    output: ${task_specs}
  - id: assign_tasks
    agent: collaboration_coordinator.v1
    input: ${task_specs} ${permissions}
    output: ${assignments}  # Maps tasks to specific users/agents
  - id: notify_collaborators
    agent: collaboration_coordinator.v1
    action: notify
    message: "Project ${project_id} created. Your tasks: ${assignments}"
    recipients: ${brief.collaborators}
  - id: parallel_execution_with_realtime
    parallel:
      foreach: ${assignments}
      do:
        - id: execute_task_${task.id}
          agent: ${task.assigned_agent}
          input: ${task.spec}
          output: ${task.result}
          realtime: true  # Enable live updates via Y.js
  - id: integrate_results
    agent: orchestrator.v8
    input: ${results}
    output: ${integrated_draft}
  - id: notify_collaborators (final review)
    agent: collaboration_coordinator.v1
    action: notify
    message: "Project ${project_id} draft ready for collaborative review"
    recipients: ${brief.collaborators}
  - id: human_review_gate (multiple reviewers can comment/approve via realtime UI)
  - id: finalize_project
    agent: project_manager.v1
    action: archive
    project_id: ${project_id}
```

### 10.7 Meta‑Learning Workflow (`meta_learning.yaml`)
(As in v9.0, but now also analyses collaboration patterns and tool usage to improve task assignment and error rates.)

---

## 🧰 ZERO‑COST TECHNOLOGY STACK (v10.0 – ALL PINNED VERSIONS WITH SECURITY NOTES)

| Category | Technology | Version | License | Security / Usage Note |
|---|---|---|---|---|
| **Container** | Docker Engine | 26.1 | Apache 2.0 | Isolate Ollama service in private bridge network |
| **Container** | Docker Compose | 2.27 | Apache 2.0 | Use profiles to switch between Ollama/vLLM backends |
| **LLM Serving** | Ollama | 0.1.40 | MIT | **Must be network‑isolated** (no exposed ports to host) |
| **LLM Serving** | vLLM | 0.5.0 | Apache 2.0 | High‑performance alternative for large models |
| **LLMs** | Llama 3.1 (8B, 70B) | latest | Custom OSS | Quantised versions (GGUF) recommended for lower resource usage |
| **RAG on Academic PDFs** | PaperQA2 | latest | MIT | Specialised for high‑accuracy scientific RAG |
| **Vector DB** | Chroma | 0.5.0 | Apache 2.0 | Lightweight, embeddable |
| **Vector DB** | Weaviate OSS | 1.24 | BSD‑3‑Clause | Scalable option for larger deployments |
| **Speech Recognition** | WhisperX | latest | BSD‑4‑Clause | Word‑level diarisation for accurate subtitling |
| **TTS** | Coqui TTS | latest | MPL 2.0 | Local, high‑quality voice synthesis |
| **Video Generation** | Paper2Video codebase | N/A | MIT | Generate videos from scientific papers |
| **Animation** | Manim | 0.18.0 | MIT | Programmatic mathematical animations |
| **Web Framework** | Next.js | 14 | MIT | Full‑stack React framework |
| **UI Components** | ShadCN/ui | latest | MIT | Reusable, accessible components |
| **Web Apps** | Streamlit | 1.32.0 | Apache 2.0 | Rapid data app prototyping |
| **Web Apps** | Gradio | 4.21.0 | Apache 2.0 | Lightweight ML demos |
| **Dashboards** | Plotly Dash | 2.16.0 | MIT | Complex, production dashboards |
| **Dashboards** | Apache Superset | 3.1.0 | Apache 2.0 | Self‑service BI platform |
| **Data Science** | Haystack | 2.0 | Apache 2.0 | Advanced NLP pipelines |
| **Data Science** | PyCaret | 3.3 | MIT | Low‑code ML automation |
| **Collaboration** | Y.js | latest | MIT | CRDTs for real‑time conflict‑free editing |
| **Authentication** | Auth.js | latest | ISC | Secure, open‑source auth for web apps |
| **LLM Tracing** | Langfuse | latest | MIT | Open‑source observability for LLM apps |
| **Error Monitoring** | Sentry | latest | BSL | Self‑hostable error tracking |
| **Metrics** | Prometheus | 2.53 | Apache 2.0 | Metrics collection |
| **Metrics** | Grafana | 11.0 | AGPL‑3.0 | Dashboards |
| **Orchestration** | Prefect | 2.19 | Apache 2.0 | Workflow engine for DAG execution |
| **Message Bus** | RabbitMQ | 3.13 | MPL 2.0 | Reliable agent communication |
| **Database** | PostgreSQL | 16 | PostgreSQL License | Primary relational DB |
| **Database** | Redis | 7.2 | BSD‑3‑Clause | Caching, working memory, Pub/Sub |
| **Provenance** | OpenTimestamps | latest | LGPL‑3.0 | Free, decentralized timestamping |

---

## 🔒 SECURITY & RISK MITIGATION (PRODUCTION‑READY IMPLEMENTATION)

1.  **Ollama API Exposure**: The `docker-compose.yml` **must** define the Ollama service with `network_mode: "service:orchestrator"` or within an isolated internal network, **never** with `ports` mapping to the host. This prevents external network access.
2.  **RAG Data Privacy**: All uploaded documents reside in the container's filesystem and vector database. The system must include clear documentation (in `SECURITY.md`) about data handling and recommend encryption for sensitive volumes.
3.  **User Authentication & RBAC**: For the real-time collaboration UI, integrate **Auth.js** or **Clerk** (free tier) to handle user sign‑in. Define roles (`admin`, `editor`, `viewer`) in `/config/security/roles.yaml` and enforce them in the Y.js server.
4.  **Secure Defaults**: The `.env.template` must contain placeholders for all secrets. The `init-secrets.sh` script must generate cryptographically strong passwords. The `start.sh` script must source `.env` securely.
5.  **Audit Trail**: The `audit.sh` script must verify the cryptographic signatures of all `ScholarlyObject` ledgers and report any discrepancies.

---

## 🚀 OPERATIONALISING THE SYSTEM FOR SINGLE OR MULTIPLE USERS

### Initial Setup (Single User)

1. Clone the repository: `git clone https://github.com/Rehan719/Workstation`
2. Copy `.env.template` to `.env` and edit with any necessary API keys (e.g., for arXiv, which is free but requires an email).
3. Run `make setup` – this installs Python dependencies, pulls Docker images, initialises directories, and runs `scripts/init-secrets.sh` to generate strong random secrets.
4. Run `make deploy-local` – this starts all services via `docker-compose up -d`.
5. Access the web dashboard at `http://localhost:8501` to interact with the system.

### Adding Collaborators (Multi‑User)

1. The repository owner adds collaborators via GitHub's repository settings (Settings → Collaborators and teams).
2. Collaborators are assigned roles (`admin`, `editor`, `viewer`) in `/config/security/roles.yaml`.
3. For real‑time collaboration on a project, the creator of the project (the user who placed the `brief.md`) can invite collaborators by adding their GitHub usernames and roles to the `collaborators.json` file in the project folder. The `collaboration_coordinator` agent will then:
   - Set up a dedicated Y.js document for the project.
   - Grant access permissions based on roles.
   - Notify invited users via GitHub notifications (or email, if configured).
4. Collaborators can then access the project through the web UI, which will show live cursors and edits from other team members.

### Starting a Project (Single or Multi‑User)

- **Single user**: Place a `brief.md` in `content/new/`. The system automatically creates a project folder and begins execution.
- **Multi‑user**: In the `brief.md`, include a `collaborators` section:
  ```yaml
  collaborators:
    - username: alice
      role: lead_author
    - username: bob
      role: data_analyst
    - username: charlie
      role: reviewer
  ```
  The system will create the project, set up permissions, and notify each user of their tasks.

### Autonomous Operation

Once the system is running, a user can simply say (via the dashboard or CLI):
```
jules, generate a literature review on quantum machine learning, including a summary of recent papers and a bibliography.
```
Jules will:
1. Parse the request.
2. Decompose it into tasks using the Manager Agent (PC‑Agent).
3. Use the Literature Synthesizer agent (with PaperQA2) to fetch and analyse papers.
4. Use the Manuscript Architect agent to generate a Quarto document.
5. Render it to PDF/HTML and place it in the project's `drafts` folder.
6. Notify the user when complete.

For multi‑user projects, the same prompt can include assignments, and the system will manage the entire collaborative workflow, including real‑time editing sessions.

---

## 🧪 TESTING AND VALIDATION

The test suite in `/tests/` must include:

- **Unit tests** for each agent's methods (using `pytest` with mocks).
- **Integration tests** for full workflows using mocked APIs and sample data, including tests for network isolation (ensuring Ollama is not publicly accessible).
- **Real‑time collaboration tests** simulating multiple clients with Y.js to verify conflict‑free merging.
- **Benchmark tests** comparing performance against previous versions (e.g., PaperQA2 accuracy, Manim rendering speed).
- **Security tests** (optional, using tools like `bandit`) to scan for hardcoded secrets.
- **Regression tests** ensuring bug fixes don't reappear.

Tests are automatically run in CI on every pull request and nightly. The `ci.yml` workflow must run `pytest`, `ruff`, `mypy`, and security scans.

---

## 📝 THE MASTER PROMPT – YOUR FINAL OUTPUT

You will now generate the **entire repository** as specified. Every file must be complete, functional, and well‑documented. Use the following output format for each file:

```
### [relative/filepath]
```[language]
[full and complete file content]
```

For example:

### README.md
```markdown
# Jules AI v10.0 – Your Open‑Source Scientific Collaborator
...
```

### agents/research/literature_synthesizer.py
```python
import ...
...
```

You must include **every file and directory** listed in the repository structure. For directories that should be empty, include a `.gitkeep` file.

**Final Verification** (simulate in your "mind"):
- Does `make setup` run without errors (assuming Python, Docker, and Node.js are installed)?
- Do all Python and JavaScript files import without syntax errors?
- Are all API keys read from environment variables (never hardcoded)?
- Does `docker-compose up` start all containers (Redis, DB, message broker, Ollama in isolated network, Y.js server)?
- Is the Ollama service **not** accessible from the host (test with `curl localhost:11434` from host – should fail)?
- Can the orchestrator be triggered via a test `brief.md`?
- Do the example workflows execute successfully with mock data?
- Do all tests pass?
- Can Quarto render a sample document?
- Can Manim generate a simple animation?
- Can a Streamlit app be launched?
- Can two browser windows connect to the same Y.js document and see each other's cursors?
- Does the RBAC system correctly restrict access based on roles?

If any component is missing or would cause an error, **correct it now**.

**This is the culmination of all our work. You are building the ultimate open‑source, multi‑user, self‑evolving, production‑grade scientific production ecosystem. Proceed. Generate the complete `Rehan719/Workstation` repository.**